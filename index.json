[{"authors":["changhong-fu"],"categories":null,"content":"Dr. Changhong Fu received his Ph.D. degree in Robotics \u0026amp; Automation from Computer Vision \u0026amp; Aerial Robotics Lab, Technical University of Madrid, Spain. During his Ph.D., he held two research positions at Arizona State University, USA \u0026amp; Nanyang Technological University (NTU), Singapore. After received his Ph.D., he worked at the NTU as Post-doc Research Fellow. He has worked on 2 international, 2 national \u0026amp; 4 industrial projects related to the vision for UAV. Currently, he is an Assistant Professor at School of Mechanical Engineering, Tongji University, China, and leading 4 projects related to the vision for multi-UAV. His research areas are Intelligent Vision \u0026amp; Control for Unmanned Systems in Complex Environments.\n","date":1602028800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1602028800,"objectID":"4d44576c929bb566eeb3c1d75eab7732","permalink":"https://vision4robotics.github.io/authors/changhong-fu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/changhong-fu/","section":"authors","summary":"Dr. Changhong Fu received his Ph.D. degree in Robotics \u0026amp; Automation from Computer Vision \u0026amp; Aerial Robotics Lab, Technical University of Madrid, Spain. During his Ph.D., he held two research positions at Arizona State University, USA \u0026amp; Nanyang Technological University (NTU), Singapore. After received his Ph.D., he worked at the NTU as Post-doc Research Fellow. He has worked on 2 international, 2 national \u0026amp; 4 industrial projects related to the vision for UAV.","tags":null,"title":"Changhong Fu","type":"authors"},{"authors":["fuling-lin"],"categories":null,"content":"Fuling Lin received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing M.Sc. degree in mechanical engineering in Tongji University, Shanghai, China. His research interests include robotics, visual object tracking and computer vision.\n","date":1602028800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1602028800,"objectID":"3bcddded919311d425d4fbd3ccb1c10d","permalink":"https://vision4robotics.github.io/authors/fuling-lin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/fuling-lin/","section":"authors","summary":"Fuling Lin received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing M.Sc. degree in mechanical engineering in Tongji University, Shanghai, China. His research interests include robotics, visual object tracking and computer vision.","tags":null,"title":"Fuling Lin","type":"authors"},{"authors":["junjie-ye"],"categories":null,"content":"Junjie Ye received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing M.Sc. degree in mechanical engineering at Tongji University, Shanghai, China. His research interests include visual object tracking, deep learning, and robotics.\n","date":1602028800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1602028800,"objectID":"fe2fccb538ad42b64242c0535fb42663","permalink":"https://vision4robotics.github.io/authors/junjie-ye/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/junjie-ye/","section":"authors","summary":"Junjie Ye received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing M.Sc. degree in mechanical engineering at Tongji University, Shanghai, China. His research interests include visual object tracking, deep learning, and robotics.","tags":null,"title":"Junjie Ye","type":"authors"},{"authors":["juntao-xu"],"categories":null,"content":"Juntao Xu received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing PhD in Mechanical Engineering, Hong Kong University, China. His research interests involve visual object tracking and computer vision.\n","date":1602028800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1602028800,"objectID":"284a7f7acfea514aee71c7fe9afc9da8","permalink":"https://vision4robotics.github.io/authors/juntao-xu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/juntao-xu/","section":"authors","summary":"Juntao Xu received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing PhD in Mechanical Engineering, Hong Kong University, China. His research interests involve visual object tracking and computer vision.","tags":null,"title":"Juntao Xu","type":"authors"},{"authors":["yujie-he"],"categories":null,"content":"Yujie He received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing M.Sc. degree in Robotics, École Polytechnique Fédérale de Lausanne (EPFL), Switzerland. His research interests include robotics, visual object tracking, and place recognition.\n","date":1602028800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1602028800,"objectID":"388dc41701aaac09a116f4f6c6c06830","permalink":"https://vision4robotics.github.io/authors/yujie-he/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yujie-he/","section":"authors","summary":"Yujie He received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing M.Sc. degree in Robotics, École Polytechnique Fédérale de Lausanne (EPFL), Switzerland. His research interests include robotics, visual object tracking, and place recognition.","tags":null,"title":"Yujie He","type":"authors"},{"authors":["changjing-liu"],"categories":null,"content":"Changjing Liu is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include robotics and computer Vision.\n","date":1593561600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1593561600,"objectID":"296ffb07ffade55bcb1583aff8e5990c","permalink":"https://vision4robotics.github.io/authors/changjing-liu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/changjing-liu/","section":"authors","summary":"Changjing Liu is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include robotics and computer Vision.","tags":null,"title":"Changjing Liu","type":"authors"},{"authors":["fangqiang-ding"],"categories":null,"content":"Fangqiang Ding is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include unmanned aerial vehicle and computer Vision.\n","date":1593561600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1593561600,"objectID":"e5afbfabb290cc0d7ec63af348a5ecf6","permalink":"https://vision4robotics.github.io/authors/fangqiang-ding/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/fangqiang-ding/","section":"authors","summary":"Fangqiang Ding is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include unmanned aerial vehicle and computer Vision.","tags":null,"title":"Fangqiang Ding","type":"authors"},{"authors":["jin-jin"],"categories":null,"content":"Jin Jin is currently a senior student at Tongji University and is achieving a bachelor\u0026rsquo;s degree as a stage goal. He majors in Mechanical Engineering and is now specializing in visual object tracking as well.\n","date":1593561600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1593561600,"objectID":"5a89def764852e0117af9268b9bc82b1","permalink":"https://vision4robotics.github.io/authors/jin-jin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jin-jin/","section":"authors","summary":"Jin Jin is currently a senior student at Tongji University and is achieving a bachelor\u0026rsquo;s degree as a stage goal. He majors in Mechanical Engineering and is now specializing in visual object tracking as well.","tags":null,"title":"Jin Jin","type":"authors"},{"authors":["xiaoxiao-yang"],"categories":null,"content":"Xiaoxiao Yang is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in control science and engineering. His research interests include robotics and visual object tracking.\n","date":1593561600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1593561600,"objectID":"683d6e977f4a6d54ea09b4b67d85888f","permalink":"https://vision4robotics.github.io/authors/xiaoxiao-yang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xiaoxiao-yang/","section":"authors","summary":"Xiaoxiao Yang is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in control science and engineering. His research interests include robotics and visual object tracking.","tags":null,"title":"Xiaoxiao Yang","type":"authors"},{"authors":["yiming-li"],"categories":null,"content":"Yiming Li received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing PhD in Robotics, New York University, USA. His research interests include robotics and visual object tracking.\n","date":1593561600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1593561600,"objectID":"74ab2b740364fa8299cfb4d6742cccdd","permalink":"https://vision4robotics.github.io/authors/yiming-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yiming-li/","section":"authors","summary":"Yiming Li received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing PhD in Robotics, New York University, USA. His research interests include robotics and visual object tracking.","tags":null,"title":"Yiming Li","type":"authors"},{"authors":["ziyuan-huang"],"categories":null,"content":"Ziyuan Huang received his B.Eng. degree in vehicle engineering from Tongji University, Shanghai, China. He is currently pursuing PhD in Mechanical Engineering, National University of Singapore (NUS), Singapore. His research interests involve visual tracking for unmanned aerial vehicles and computer vision.\n","date":1593561600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1593561600,"objectID":"1d5ccc4aa1c5de52ed53972a45961a66","permalink":"https://vision4robotics.github.io/authors/ziyuan-huang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ziyuan-huang/","section":"authors","summary":"Ziyuan Huang received his B.Eng. degree in vehicle engineering from Tongji University, Shanghai, China. He is currently pursuing PhD in Mechanical Engineering, National University of Singapore (NUS), Singapore. His research interests involve visual tracking for unmanned aerial vehicles and computer vision.","tags":null,"title":"Ziyuan Huang","type":"authors"},{"authors":["yinqiang-zhang"],"categories":null,"content":"Yinqiang Zhang received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing M.Sc. degree in mechatronics and information technology, Technical University of Munich (TUM), Munich, Germany. His research interests include visual tracking and computer vision.\n","date":1586217600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1586217600,"objectID":"b2c3db1f8b9ba4f5d084ca76dfab0758","permalink":"https://vision4robotics.github.io/authors/yinqiang-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yinqiang-zhang/","section":"authors","summary":"Yinqiang Zhang received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing M.Sc. degree in mechatronics and information technology, Technical University of Munich (TUM), Munich, Germany. His research interests include visual tracking and computer vision.","tags":null,"title":"Yinqiang Zhang","type":"authors"},{"authors":["weijiang-xiong"],"categories":null,"content":"Weijiang Xiong received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing M.Sc. degree in Robotics, Aalto University, Finland. His research interests include visual object tracking and artificial intelligence.\n","date":1575504000,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1575504000,"objectID":"a60def9cac4a1c8f7aad2558c4a06e69","permalink":"https://vision4robotics.github.io/authors/weijiang-xiong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/weijiang-xiong/","section":"authors","summary":"Weijiang Xiong received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing M.Sc. degree in Robotics, Aalto University, Finland. His research interests include visual object tracking and artificial intelligence.","tags":null,"title":"Weijiang Xiong","type":"authors"},{"authors":["admin"],"categories":null,"content":"The vision4robotics group is a multidisciplinary research group at Tongji University. Our research interests include the following aspects that focus on intelligent vision and control technologies for robotics.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://vision4robotics.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"The vision4robotics group is a multidisciplinary research group at Tongji University. Our research interests include the following aspects that focus on intelligent vision and control technologies for robotics.","tags":null,"title":"Vision4robotics Group","type":"authors"},{"authors":["bowen-li"],"categories":null,"content":"Bowen Li is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include robotics, artificial intelligence, and computer vision.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"50b3204098dafa7a3d9415e91dcb3d02","permalink":"https://vision4robotics.github.io/authors/bowen-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/bowen-li/","section":"authors","summary":"Bowen Li is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include robotics, artificial intelligence, and computer vision.","tags":null,"title":"Bowen Li","type":"authors"},{"authors":["guangze-zheng"],"categories":null,"content":"Guangze Zheng is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include visual object tracking and machine learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"1ca1e48351fd704681e10048d6811556","permalink":"https://vision4robotics.github.io/authors/guangze-zheng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/guangze-zheng/","section":"authors","summary":"Guangze Zheng is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include visual object tracking and machine learning.","tags":null,"title":"Guangze Zheng","type":"authors"},{"authors":["hao-chen"],"categories":null,"content":"Hao Chen received his B.Eng. degree in mechanical engineering from Chongqing University, Chongqing, China. He is now pursuing M.Sc. degree in mechanical engineering at Tongji University. His research interests include robotics and visual/lidar SLAM.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"7123babb776e1a2719ea30a1de6294ce","permalink":"https://vision4robotics.github.io/authors/hao-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/hao-chen/","section":"authors","summary":"Hao Chen received his B.Eng. degree in mechanical engineering from Chongqing University, Chongqing, China. He is now pursuing M.Sc. degree in mechanical engineering at Tongji University. His research interests include robotics and visual/lidar SLAM.","tags":null,"title":"Hao Chen","type":"authors"},{"authors":["jianqiao-lu"],"categories":null,"content":"Jianqiao Lu received his B.Eng. degree in vehicle engineering from Tongji University, Shanghai, China. He is currently pursuing PhD in Computer Science, HongKong University (HKU), China. His research interests include robotics motion planning, biorobotics control, and place recognition.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"816035b05c4c141078b704fe19a1c71e","permalink":"https://vision4robotics.github.io/authors/jianqiao-lu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jianqiao-lu/","section":"authors","summary":"Jianqiao Lu received his B.Eng. degree in vehicle engineering from Tongji University, Shanghai, China. He is currently pursuing PhD in Computer Science, HongKong University (HKU), China. His research interests include robotics motion planning, biorobotics control, and place recognition.","tags":null,"title":"Jianqiao Lu","type":"authors"},{"authors":["jilin-zhao"],"categories":null,"content":"Jilin Zhao is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. He has been recommended to pursue M.Sc. degree in mechanical engineering at Tongji University. His research interests include visual object tracking, deep learning, and computer vision.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"702af82e1991df761b4f5893ce247ec8","permalink":"https://vision4robotics.github.io/authors/jilin-zhao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jilin-zhao/","section":"authors","summary":"Jilin Zhao is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. He has been recommended to pursue M.Sc. degree in mechanical engineering at Tongji University. His research interests include visual object tracking, deep learning, and computer vision.","tags":null,"title":"Jilin Zhao","type":"authors"},{"authors":["shaoqiu-xu"],"categories":null,"content":"Shaoqiu Xu received a bachelor\u0026rsquo;s degree in mechanical engineering at Tongji University and is currently pursuing a Master\u0026rsquo;s degree at Shanghai Jiaotong University. His research interests include object tracking and pose estimation.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"278d4354562b1001d3ddd31b27aaafc0","permalink":"https://vision4robotics.github.io/authors/shaoqiu-xu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/shaoqiu-xu/","section":"authors","summary":"Shaoqiu Xu received a bachelor\u0026rsquo;s degree in mechanical engineering at Tongji University and is currently pursuing a Master\u0026rsquo;s degree at Shanghai Jiaotong University. His research interests include object tracking and pose estimation.","tags":null,"title":"Shaoqiu Xu","type":"authors"},{"authors":["teng-li"],"categories":null,"content":"Li Teng is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include unmanned aerial vehicle and computer Vision.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"17be8edc583687f02f1b94788fa21e9e","permalink":"https://vision4robotics.github.io/authors/teng-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/teng-li/","section":"authors","summary":"Li Teng is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include unmanned aerial vehicle and computer Vision.","tags":null,"title":"Teng Li","type":"authors"},{"authors":["weiyu-peng"],"categories":null,"content":"Weiyu Peng received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing M.Sc. degree in mechanical engineering at Tongji University, Shanghai, China. His research interests include deep learning, robotics, and visual object tracking.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"da145e55e892c20569410284ed79d85f","permalink":"https://vision4robotics.github.io/authors/weiyu-peng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/weiyu-peng/","section":"authors","summary":"Weiyu Peng received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing M.Sc. degree in mechanical engineering at Tongji University, Shanghai, China. His research interests include deep learning, robotics, and visual object tracking.","tags":null,"title":"Weiyu Peng","type":"authors"},{"authors":["xiangpeng-zeng"],"categories":null,"content":"Xiangpeng Zeng received his B.Eng. degree in material forming and control engineering from East China University of Science and Technology, Shanghai, China. He is currently pursuing M.Sc. degree in mechanical engineering at Tongji University, Shanghai, China.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"1cb22594f1d06998e93397a29f031698","permalink":"https://vision4robotics.github.io/authors/xiangpeng-zeng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xiangpeng-zeng/","section":"authors","summary":"Xiangpeng Zeng received his B.Eng. degree in material forming and control engineering from East China University of Science and Technology, Shanghai, China. He is currently pursuing M.Sc. degree in mechanical engineering at Tongji University, Shanghai, China.","tags":null,"title":"Xiangpeng Zeng","type":"authors"},{"authors":["yiyong-sun"],"categories":null,"content":"Yiyong Sun is an incoming master student in Mechanical Engineering at the National University of Singapore (NUS) and currently pursuing his BEng degree in Mechanical Engineering at Tongji University. His research interests include robotics, visual object tracking for unmanned aerial vehicle (UAV), and machine learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"289a010c77c85364a2de7389ba94f586","permalink":"https://vision4robotics.github.io/authors/yiyong-sun/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yiyong-sun/","section":"authors","summary":"Yiyong Sun is an incoming master student in Mechanical Engineering at the National University of Singapore (NUS) and currently pursuing his BEng degree in Mechanical Engineering at Tongji University. His research interests include robotics, visual object tracking for unmanned aerial vehicle (UAV), and machine learning.","tags":null,"title":"Yiyong Sun","type":"authors"},{"authors":["yulin-li"],"categories":null,"content":"Yulin Li is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include robotics motion planning, biorobotics control, and place recognition.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"b1258af42bb4dfb5508c09da7b48273f","permalink":"https://vision4robotics.github.io/authors/yulin-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yulin-li/","section":"authors","summary":"Yulin Li is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include robotics motion planning, biorobotics control, and place recognition.","tags":null,"title":"Yulin Li","type":"authors"},{"authors":["zheng-shen"],"categories":null,"content":"Zheng Shen is currently a senior student at Tongji University, in pursuit of a B.Eng. degree in mechanical engineering with a specialization in mechatronics. His research interests involve control theory for unmanned aerial vehicles.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"96d70063e0299291f33c32adbd420931","permalink":"https://vision4robotics.github.io/authors/zheng-shen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zheng-shen/","section":"authors","summary":"Zheng Shen is currently a senior student at Tongji University, in pursuit of a B.Eng. degree in mechanical engineering with a specialization in mechatronics. His research interests involve control theory for unmanned aerial vehicles.","tags":null,"title":"Zheng Shen","type":"authors"},{"authors":["ziang-cao"],"categories":null,"content":"Ziang Cao is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in Vehicle Engineering. His research interests include unmanned aerial vehicle and visual object tracking.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"4fd79dd332235ee46766fc8986a07748","permalink":"https://vision4robotics.github.io/authors/ziang-cao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ziang-cao/","section":"authors","summary":"Ziang Cao is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in Vehicle Engineering. His research interests include unmanned aerial vehicle and visual object tracking.","tags":null,"title":"Ziang Cao","type":"authors"},{"authors":["Changhong Fu","Junjie Ye","Juntao Xu","Yujie He","Fuling Lin"],"categories":null,"content":" Tracking procedure of the proposed IBRI tracker in the k-th frame. Historical interval responses are incorporated into the filter training phase after denoising by a novel disruptor-aware scheme based on response bucketing. \n","date":1602028800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602028800,"objectID":"06a2c27d3e614c9491702be79f0c1bcf","permalink":"https://vision4robotics.github.io/publication/2020_tgrs_ibri-tracker/","publishdate":"2020-10-07T00:00:00Z","relpermalink":"/publication/2020_tgrs_ibri-tracker/","section":"publication","summary":"Aerial object tracking approaches based on discriminative correlation filter (DCF) have attracted wide attention in the tracking community due to their impressive progress recently. Many studies introduce temporal regularization into the DCF-based framework to achieve a more robust appearance model and further enhance the tracking performance. However, existing temporal regularization approaches usually utilize the information of two consecutive frames, which are not robust enough due to limited information. Although some methods attempt to incorporate abundant training samples and generally improve the tracking performance, these improvements are at the expense of significantly increased computing consumption. Besides, most existing methods introduce historical information directly without denoising, which means background noises are also introduced into the filter training and may degrade the tracking accuracy. To tackle the drawbacks mentioned above, this work proposes a novel aerial object tracking approach to exploit disruptor-aware interval-based response inconsistency, i.e., IBRI tracker. The proposed method is able to incorporate historical interval information by utilizing responses in the filter training process, thereby obtaining a robust tracking performance while maintaining the real-time speed. Moreover, to reduce the disruptions caused by similar object, partial occlusion, and other challenging scenes, a novel disruptor-aware scheme based on response bucketing is introduced to detect the disruptor and enforce a spatial penalty for the disruptive area around the tracked object. Exhausted experiments on multiple well-known challenging aerial tracking benchmarks demonstrate the accuracy and robustness of the proposed IBRI tracker against other 35 state-of-the-art trackers. With a real-time speed of ~32 frames per second on a single CPU, the proposed approach can be applied for typical aerial platforms to achieve aerial visual object tracking efficiently.","tags":["Aerial object tracking","Discriminative correlation filter (DCF)","Temporal regularization","Historical frame information","Interval-based response inconsistency","Disruptor-aware bucketing"],"title":"Disruptor-Aware Interval-Based Response Inconsistency for Correlation Filters in Real-Time Aerial Tracking","type":"publication"},{"authors":["Fuling Lin","Changhong Fu","Yujie He","Fuyu Guo","Qian Tang"],"categories":null,"content":" A flowchart of the proposed TB-BiCF tracker. \n","date":1599436800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599436800,"objectID":"63a546ae784c6d754ee9ab3741c2dc20","permalink":"https://vision4robotics.github.io/publication/2020_tcsvt_tb-bicf-tracker/","publishdate":"2020-09-07T00:00:00Z","relpermalink":"/publication/2020_tcsvt_tb-bicf-tracker/","section":"publication","summary":"In the field of UAV object tracking, correlation filter based approaches have received lots of attention due to their computational efficiency. The methods learn filters by the ridge regression and generate response maps to distinguish the specified target from the background. An ideal filter can predict the object's position in a new frame, and in turn, can backtrack the object in the past frames. However, the neglect of tracking reversibility in most methods limits the potential of using inter-frame information to improve performance. In this work, a novel bidirectional incongruity-aware correlation filter is presented based on the nature of tracking reversibility. The proposed method incorporates the response-based bidirectional incongruity, which represents the gap between the filters' discriminative difference in the forward and backward tracking perspective caused by object appearance changes. It enables the filter not only to inherit the discriminability from previous filters but also to enhance the generalization capability to unpredictable appearance variations in upcoming frames. Moreover, a temporary block-based strategy is introduced to empower the filter accommodate more drastic object appearance changes and make more effective use of inter-frame information. Comprehensive experiments are conducted on three challenging UAV tracking benchmarks, including UAV123@10fps, DTB70, and UAVDT. Experimental results indicate that the proposed method has superior performance compared with the other 34 state-of-the-art trackers. Our approach permits real-time performance at ∼46.8 FPS on a single CPU and is suitable for UAV online tracking applications.","tags":["Aerial video analysis","Unmanned aerial vehicle","Visual object tracking","Discriminative correlation filter","Temporary block-based bidirectional incongruity"],"title":"Learning Temporary Block-Based Bidirectional Incongruity-Aware Correlation Filters for Efficient UAV Object Tracking","type":"publication"},{"authors":["Yiming Li","Changhong Fu","Fangqiang Ding","Ziyuan Huang","Jia Pan"],"categories":null,"content":" Overall structure of AMCF. \n","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"465de267b6953cf02f6e1d667dfc86a1","permalink":"https://vision4robotics.github.io/publication/2020_iros_amcf-tracker/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/2020_iros_amcf-tracker/","section":"publication","summary":"The outstanding computational efficiency of discriminative correlation filter (DCF) fades away with various complicated improvements. Previous appearances are also gradually forgotten due to the exponential decay of historical views in traditional appearance updating scheme of DCF framework, reducing the model's robustness. In this work, a novel tracker based on DCF framework is proposed to augment memory of previously appeared views while running at a real-time speed. Several historical views and the current view are simultaneously introduced in training to allow tracker to adapt to new appearances as well as memorize previous ones. A novel rapid compressive context learning is proposed to increase the discriminative ability of the filter efficiently. Substantial experiments on UAVDT and UAV123 datasets have validated that the proposed tracker performs competitively against other 26 top DCF and deep-based trackers with over 40 FPS on CPU.","tags":["Visual tracking","Unmanned aerial vehicles","Augmented memory"],"title":"Augmented Memory for Correlation Filters in Real-Time UAV Tracking","type":"publication"},{"authors":["Fangqiang Ding","Changhong Fu","Yiming Li","Jin Jin","Chen Feng"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"4c9cb98cabea629fb7a37ba71c5310a6","permalink":"https://vision4robotics.github.io/publication/2020_iros_jsar-tracker/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/2020_iros_jsar-tracker/","section":"publication","summary":"Current unmanned aerial vehicle (UAV) visual tracking algorithms are primarily limited with respect to\":\" (i) the kind of size variation they can deal with, (ii) the implementation speed which hardly meet the real-time requirement. In this work, a real-time UAV tracking algorithm with powerful size estimation ability is proposed. Specifically, the overall tracking task is allocated to two 2D filters\":\" (i) translation filter for location prediction in the space domain, (ii) size filter for scale and aspect ratio optimization in the size domain. Besides, an efficient two-stage re-detection strategy is introduced for long-term UAV tracking tasks. Large-scale experiments on four UAV benchmarks demonstrate the superiority of the presented method which has computation feasibility on a low-cost CPU.","tags":["Visual tracking","Unmanned aerial vehicles","Joint scale and aspect ratio optimization"],"title":"Automatic Failure Recovery and Re-Initialization for Online UAV Tracking with Joint Scale and Aspect Ratio Optimization","type":"publication"},{"authors":["Changhong Fu","Fangqiang Ding","Yiming Li","Jin Jin","Chen Feng"],"categories":null,"content":" Overall work-flow of DR^2Track. \n","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"8c2ab231206a62eb37d14ffed9bf43ff","permalink":"https://vision4robotics.github.io/publication/2020_iros_dr2track-tracker/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/2020_iros_dr2track-tracker/","section":"publication","summary":"Visual tracking has yielded promising applications with unmanned aerial vehicle (UAV). In literature, the advanced discriminative correlation filter (DCF) type trackers generally distinguish the foreground from the background with a learned regressor which regresses the implicit circulated samples into a fixed target label. However, the predefined and unchanged regression target results in low robustness and adaptivity to uncertain aerial tracking scenarios. In this work, we exploit the local extreme points of the response map generated in the detection phase to automatically locate current distractors. By repressing the response of distractors in the regressor learning, we can dynamically and adaptively alter our regression target to leverage the tracking robustness as well as adaptivity. Substantial experiments conducted on three challenging UAV benchmarks demonstrate both the excellent performance and extraordinary speed (∼50fps on a cheap CPU) of our tracker.","tags":["Visual tracking","Unmanned aerial vehicles","Distractor repressed dynamic regression"],"title":"DR^2Track: Towards Real-Time Visual Tracking for UAV via Distractor Repressed Dynamic Regression","type":"publication"},{"authors":["Changhong Fu","Xiaoxiao Yang","Fan Li","Changjing Liu","Peng Lu"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"5850528e163470bcdde66d49766e4b65","permalink":"https://vision4robotics.github.io/publication/2020_iros_cpcf-tracker/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/2020_iros_cpcf-tracker/","section":"publication","summary":"Correlation filter (CF) has proven its superb efficiency in visual tracking for unmanned aerial vehicle (UAV) applications. To enhance the temporal smoothness of the filter, many CF-based approaches introduce temporal regularization terms to penalize the variation of coefficients in an element-wise manner. However, this element-wise smoothness is stiff to the filter coefficients and can lead to poor adaptiveness in case of various challenges, e.g., fast motion and viewpoint changes, which frequently occur in the UAV tracking process. To tackle this issue, this work introduces a novel tracker with consistency pursed correlation filter, i.e., CPCF tracker. It is able to achieve flexible temporal smoothness by evaluating the similarity between two consecutive response maps with a correlation operation. By correlation operations, the consistency constraint allows for flexible variations in the response map without losing temporal smoothness. Besides, a dynamic label function is introduced to further increase adaptiveness in the training process. Considerable experiments on three challenging UAV tracking benchmarks verify that the presented tracker has surpassed the other 25 state-of-the-art trackers with satisfactory speed (~25 FPS) for real-time applications on a single CPU.","tags":["Visual tracking","Unmanned aerial vehicles","Consistency pursued"],"title":"Learning Consistency Pursued Correlation Filters for Real-Time UAV Tracking","type":"publication"},{"authors":["Yujie He","Changhong Fu","Fuling Lin","Yiming Li","Peng Lu"],"categories":null,"content":" The main workflow of the TACF tracker. \n","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"577e5083b7eff43d607721263bb9babd","permalink":"https://vision4robotics.github.io/publication/2020_iros_tacf-tracker/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/2020_iros_tacf-tracker/","section":"publication","summary":"Object tracking has been broadly applied in unmanned aerial vehicle (UAV) tasks in recent years. However, existing algorithms still face difficulties such as partial occlusion, clutter background, and other challenging visual factors. Inspired by the cutting-edge attention mechanisms, a new visual tracking framework leveraging multi-level visual attention to make full use of the information during tracking. Three primary attention, i.e., contextual attention, dimensional attention, and spatiotemporal attention, are integrated into the training and detection stages of correlation filter-based tracking pipeline. Therefore, the proposed tracker is equipped with robust discriminative power against challenging factors while maintains high operational efficiency in UAV scenarios. Quantitative and qualitative experiments on two well-known benchmark with 173 challenging UAV video sequences demonstrate the effectiveness of the proposed framework. The proposed tracking algorithm compares favorably against state-of-the-art methods, yielding 4.8% relative gain in UAVDT and 8.2% relative gain in UAV123@10fps against the baseline tracker while operating at the speed of ∼28 frames per second","tags":["Visual tracking","Unmanned aerial vehicles","Multi-level visual attention"],"title":"Towards Robust Visual Tracking for Unmanned Aerial Vehicle with Tri-Attentional Correlation Filters","type":"publication"},{"authors":["Changhong Fu","Juntao Xu","Fuling Lin","Fuyu Guo","Tingcong Liu","Zhijun Zhang"],"categories":null,"content":" Comparison between the tracking pipeline of the baseline SRDCF tracker and the proposed DRCF tracker. \n","date":1587772800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587772800,"objectID":"5af1766e7fd3898b385f4b0d358898af","permalink":"https://vision4robotics.github.io/publication/2020_tgrs_drcf-tracker/","publishdate":"2020-04-25T00:00:00Z","relpermalink":"/publication/2020_tgrs_drcf-tracker/","section":"publication","summary":"Spatial regularization has proven itself to be an effective method in terms of alleviating the boundary effect and boosting the performance of a discriminative correlation filter (DCF) in aerial visual object tracking. However, existing spatial regularization methods usually treat the regularizer as a supplementary term apart from the main regression and neglect to regularize the filter involved in the correlation operation. To address the aforementioned issue, this work introduces a novel object saliency-aware dual regularized correlation filter, i.e., DRCF. Specifically, the proposed DRCF tracker suggests a dual regularization strategy to directly regularize the filter involved with the correlation operation inside the core of the filter generating ridge regression. This allows the DRCF tracker to suppress the boundary effect and consequently enhance the performance of the tracker. Furthermore, an efficient method based on a saliency detection algorithm is employed to generate the dual regularizers dynamically and provide the regularizers with online adjusting ability. This enables the generated dynamic regularizers to automatically discern the object from the background and actively regularize the filter to accentuate the object during its unpredictable appearance changes. By the merits of the dual regularization strategy and the saliency-aware dynamical regularizers, the proposed DRCF tracker performs favorably in terms of suppressing the boundary effect, penalizing the irrelevant background noise coefficients and boosting the overall performance of the tracker. Exhaustive evaluations on 193 challenging video sequences from multiple well-known challenging aerial object tracking benchmarks validate the accuracy and robustness of the proposed DRCF tracker against 27 other state-of-the-art methods. Meanwhile, the proposed tracker can perform real-time aerial tracking applications on a single CPU with a sufficient speed of 38.4 frames per second.","tags":["Visual tracking","Unmanned aerial vehicles","Keyfilter"],"title":"Object Saliency-Aware Dual Regularized Correlation Filter for Real-Time Aerial Tracking","type":"publication"},{"authors":["Yiming Li","Changhong Fu","Ziyuan Huang","Yinqiang Zhang","Jia Pan"],"categories":null,"content":" Main work-flow of the KAOT tracker. \n","date":1586217600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586217600,"objectID":"40b96a8cb3779f78b5457015094a6642","permalink":"https://vision4robotics.github.io/publication/2020_tmm_kaot-tracker/","publishdate":"2020-04-07T00:00:00Z","relpermalink":"/publication/2020_tmm_kaot-tracker/","section":"publication","summary":"Correlation filter-based tracking has been widely applied in unmanned aerial vehicle (UAV) with high efficiency. However, it has two imperfections, i.e., boundary effect and filter corruption. Several methods enlarging the search area can mitigate boundary effect, yet introducing undesired background distraction. Existing frame-by-frame context learning strategies for repressing background distraction nevertheless lower the tracking speed. Inspired by keyframe-based simultaneous localization and mapping, keyfilter is proposed in visual tracking for the first time, in order to handle the above issues efficiently and effectively. Keyfilters generated by periodically selected keyframes learn the context intermittently and are used to restrain the learning of filters, so that 1) context awareness can be transmitted to all the filters via keyfilter restriction, and 2) filter corruption can be repressed. Compared to the state-of-the-art results, our tracker performs better on two challenging benchmarks, with enough speed for UAV real-time applications.","tags":["Visual tracking","Unmanned aerial vehicles","Keyfilter"],"title":"Intermittent Contextual Learning for Keyfilter-Aware UAV Object Tracking Using Deep Convolutional Feature","type":"publication"},{"authors":["Yiming Li","Changhong Fu","Fangqiang Ding","Ziyuan Huang","Geng Lu"],"categories":null,"content":"","date":1582502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582502400,"objectID":"f96897038cb4fd1e59d7ddbd39f3388f","permalink":"https://vision4robotics.github.io/publication/2020_cvpr_autotrack/","publishdate":"2020-02-24T00:00:00Z","relpermalink":"/publication/2020_cvpr_autotrack/","section":"publication","summary":"Most existing trackers based on discriminative correlation filters (DCF) try to introduce predefined regularization term to improve the learning of target objects, e.g., by sup-pressing background learning or by restricting change rate of correlation filters. However, predefined parameters intro-duce much effort in tuning them and they still fail to adapt to new situations that the designer did not think of. In this work, a novel approach is proposed to online automatically and adaptively learn spatio-temporal regularization term. Spatially local response map variation is introduced as spatial regularization to make DCF focus on the learning of trust-worthy parts of the object, and global response map variation determines the updating rate of the filter. Extensive experiments on four UAV benchmarks have proven the superiority of our method compared to the state-of-the-art CPU- and GPU-based trackers, with a speed of ∼60 frames per second running on a single CPU. Our tracker is additionally proposed to be applied in UAV localization. Considerable tests in the indoor practical scenarios have proven the effectiveness and versatility of our localization method. The code is available at .","tags":["Correlation filter","Real-time object tracking","Unmanned aerial vehicles","Localization by tracking"],"title":"AutoTrack: Towards High-Performance Visual Tracking for UAV with Automatic Spatio-Temporal Regularization","type":"publication"},{"authors":["Fuling Lin","Changhong Fu","Yujie He","Fuyu Guo","Qian Tang"],"categories":null,"content":"\n Comparison between discriminative correlation filter (DCF) and the proposed BiCF tracker\n\n","date":1579694400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579694400,"objectID":"6bb33494dc7d24845b77b9fe07c72ab1","permalink":"https://vision4robotics.github.io/publication/2020_icra_bicf-tracker/","publishdate":"2020-01-22T12:00:00Z","relpermalink":"/publication/2020_icra_bicf-tracker/","section":"publication","summary":"Correlation filters (CFs) have shown excellent performance in unmanned aerial vehicle (UAV) tracking scenarios due to their high computational efficiency. During the UAV tracking process, viewpoint variations are usually accompanied by changes in the object and background appearance, which poses a unique challenge to CF-based trackers. Since the appearance is gradually changing over time, an ideal tracker can not only forward predict the object position but also backtrack to locate its position in the previous frame. There exist response-based errors in the reversibility of the tracking process containing the information on the changes in appearance. However, some existing methods do not consider the forward and backward errors based on while using only the current training sample to learn the filter. For other ones, the applicants of considerable historical training samples impose a computational burden on the UAV. In this work, a novel bidirectional incongruity-aware correlation filter (BiCF) is proposed. By integrating the response-based bidirectional incongruity error into the CF, BiCF can efficiently learn the changes in appearance and suppress the inconsistent error. Extensive experiments on 243 challenging sequences from three UAV datasets (UAV123, UAVDT, and DTB70) are conducted to demonstrate that BiCF favorably outperforms other 25 state-of-the-art trackers and achieves a real-time speed of 45.4 FPS on a single CPU, which can be applied in UAV efficiently.","tags":["Visual tracking","Unmanned aerial vehicles","Bidirectional incongruity learning"],"title":"BiCF: Learning Bidirectional Incongruity-Aware Correlation Filter for Efficient UAV Object Tracking","type":"publication"},{"authors":["Fan Li","Changhong Fu","Fuling Lin","Yiming Li","Peng Lu"],"categories":null,"content":"\nComparison between our TSD tracker with the baseline BACF tracker\n\n","date":1579680000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579680000,"objectID":"6bba38c858ff464c41cde65aad3840f8","permalink":"https://vision4robotics.github.io/publication/2020_icra_tsd-tracker/","publishdate":"2020-01-22T08:00:00Z","relpermalink":"/publication/2020_icra_tsd-tracker/","section":"publication","summary":"Correlation filter (CF) has recently exhibited promising performance in visual object tracking for unmanned aerial vehicle (UAV). Such online learning method heavily depends on the quality of the training-set, yet complicated aerial scenarios like occlusion or out of view can reduce its reliability. In this work, a novel time slot-based distillation approach is proposed to efficiently and effectively optimize the training-set’s quality on the fly. A cooperative energy minimization function is established to score the historical samples adaptively. To accel-erate the scoring process, frames with high confident tracking results are employed as the keyframes to divide the tracking process into multiple time slots. After the establishment of a new slot, the weighted fusion of the previous samples generates one key-sample, in order to reduce the number of samples to be scored. Besides, when the current time slot exceeds the maximum frame number, which can be scored, the sample with the lowest score will be discarded. Consequently, the training-set can be efficiently and reliably distilled. Comprehensive tests on two well-known UAV benchmarks prove the effectiveness of our method with real-time speed on a single CPU.","tags":["Visual tracking","Unmanned aerial vehicles","Training-Set distillation"],"title":"Training-Set Distillation for Real-Time UAV Object Tracking","type":"publication"},{"authors":["Yiming Li","Changhong Fu","Ziyuan Huang","Yinqiang Zhang","Jia Pan"],"categories":null,"content":" Comparison between response maps of our tracker and baseline.\n\n","date":1579651200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579651200,"objectID":"3bcd6c1a928b6f63c32bf6c64f38f25b","permalink":"https://vision4robotics.github.io/publication/2020_icra_kaot-tracker/","publishdate":"2020-01-22T00:00:00Z","relpermalink":"/publication/2020_icra_kaot-tracker/","section":"publication","summary":"Correlation filter-based tracking has been widely applied in unmanned aerial vehicle (UAV) with high efficiency. However, it has two imperfections, i.e., boundary effect and filter corruption. Several methods enlarging the search area can mitigate boundary effect, yet introducing undesired background distraction. Existing frame-by-frame context learning strategies for repressing background distraction nevertheless lower the tracking speed. Inspired by keyframe-based simultaneous localization and mapping, keyfilter is proposed in visual tracking for the first time, in order to handle the above issues efficiently and effectively. Keyfilters generated by periodically selected keyframes learn the context intermittently and are used to restrain the learning of filters, so that 1) context awareness can be transmitted to all the filters via keyfilter restriction, and 2) filter corruption can be repressed. Compared to the state-of-the-art results, our tracker performs better on two challenging benchmarks, with enough speed for UAV real-time applications.","tags":["Visual tracking","Unmanned aerial vehicles","Keyfilter"],"title":"Keyfilter-Aware Real-Time UAV Object Tracking","type":"publication"},{"authors":["Changhong Fu","Yujie He","Fuling Lin","Weijiang Xiong"],"categories":null,"content":"\nMain structure of the proposed MKCT-Tracker\n\n","date":1575504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575504000,"objectID":"aa813cef24765cfa64a8db4422313184","permalink":"https://vision4robotics.github.io/publication/2020_ncaa_mkct-tracker/","publishdate":"2020-01-06T00:00:00Z","relpermalink":"/publication/2020_ncaa_mkct-tracker/","section":"publication","summary":"In recent years, the correlation filter (CF)-based method has significantly advanced in the tracking for unmanned aerial vehicles (UAV). As the core component of most trackers, CF is a discriminative classifier to distinguish the object from the surrounding environment. However, the poor representation of the object and lack of contextual information have restricted the tracker to gain better performance. In this work, a robust framework with multi-kernelized correlators is proposed to improve robustness and accuracy simultaneously. Both convolutional features extracted from the neural network and hand-crafted features are employed to enhance expressions for object appearances. Then, the adaptive context analysis strategy helps filters to effectively learn the surrounding information by introducing context patches with the GMSD index. In the training stage, multiple dynamic filters with time-attenuated factors are introduced to avoid tracking failure caused by dramatic appearance changes. The response maps corresponding to different features are finally fused before the novel resolution enhancement operation to increase distinguishing capability. As a result, the optimization problem is reformulated, and a closed-form solution for the proposed framework can be obtained in the kernel space. Extensive experiments on 100 challenging UAV tracking sequences demonstrate the proposed tracker outperforms other 23 state-of-the-art trackers and can effectively handle unexpected appearance variations under the complex and constantly changing working conditions.","tags":["Visual tracking","Unmanned aerial vehicles","Multi-kernelized correlators","Adaptive context analysis","Dynamic weighted filters"],"title":"Robust Multi-Kernelized Correlators for UAV Tracking with Adaptive Context Analysis and Dynamic Weighted Filters","type":"publication"},{"authors":["Changhong Fu","Weijiang Xiong","Fuling Lin","Yufeng Yue"],"categories":null,"content":"\nFig. 1 Main workflow of the proposed SASR tracker.\n\n","date":1569801600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569801600,"objectID":"d2d81b87c37cfff0a97221e87a865c7a","permalink":"https://vision4robotics.github.io/publication/2019_signal_processing_sasr-tracker/","publishdate":"2019-09-30T00:00:00Z","relpermalink":"/publication/2019_signal_processing_sasr-tracker/","section":"publication","summary":"The great advance of visual object tracking has provided unmanned aerial vehicle (UAV) with intriguing capability for various practical applications. With promising performance and efficiency, discriminative correlation filter-based trackers have drawn great attention and undergone remarkable progress. However, background interference and boundary effect remain two thorny problems. In this paper, a surrounding-aware tracker with selective spatial regularization (SASR) is presented. SASR tracker extracts surrounding samples according to the size and shape of the object in order to utilize context and maintain the integrality of the object. Additionally, a selective spatial regularizer is introduced to address boundary effect. Central coefficients in the filter are evenly regularized to preserve valid information from the object. While the others are penalized according to their spatial location. Under the framework of SASR tracker, surrounding information and selective spatial regularization prove to be complementary to each other, which actually did not draw much attention before. They managed to improve not only the robustness against various distractions in the surrounding but also the flexibility to catch up with frequent appearance change of the object. Qualitative evaluation and quantitative experiments on challenging UAV tracking sequences have shown that SASR tracker has performed favorably against 23 state-of-the-art trackers.","tags":["Unmanned aerial vehicle (UAV)","Visual object tracking","Discriminative correlation filter","Surrounding information","Selective spatial regularization"],"title":"Surrounding-Aware Correlation Filter for UAV Tracking with Selective Spatial Regularization","type":"publication"},{"authors":["Ziyuan Huang","Changhong Fu","Yiming Li","Fuling Lin","Peng Lu"],"categories":null,"content":"\nFig. 1 Comparison between background-aware correlation filter (BACF) and the proposed ARCF tracker.\nFig. 2 Main structure of the proposed ARCF tracker.\n\n","date":1563753600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563753600,"objectID":"f83555e1f1f71a6d7b37896ad8adbdd5","permalink":"https://vision4robotics.github.io/publication/2019_iccv_arcf-tracker/","publishdate":"2019-07-22T00:00:00Z","relpermalink":"/publication/2019_iccv_arcf-tracker/","section":"publication","summary":"Traditional framework of discriminative correlation filters (DCF) is often subject to undesired boundary effects. Several approaches to enlarge search regions have been already proposed in the past years to make up for this shortcoming. However, with excessive background information, more background noises are also introduced and the discriminative filter is prone to learn from the ambiance rather than the object. This situation, along with appearance changes of objects caused by full/partial occlusion, illumination variation, and other reasons has made it more likely to have aberrances in the detection process, which could substantially degrade the credibility of its result. Therefore, in this work, a novel approach to repress the aberrances happening during the detection process is proposed, i.e., aberrance repressed correlation filter (ARCF). By enforcing restriction to the rate of alteration in response maps generated in the detection phase, the ARCF tracker can evidently suppress aberrances and is thus more robust and accurate to track objects. Considerable experiments are conducted on different UAV datasets to perform object tracking from an aerial view, i.e., UAV123, UAVDT, and DTB70, with 243 challenging image sequences containing over 90K frames to verify the performance of the ARCF tracker and it has proven itself to have outperformed other 20 state-of-the-art trackers based on DCF and deep-based frameworks with sufficient speed for real-time applications.","tags":["Correlation filter","Real-time object tracking","Unmanned aerial vehicles","Multi-Frame Consensus Veriﬁcation"],"title":"Learning Aberrance Repressed Correlation Filters for Real-Time UAV Tracking","type":"publication"},{"authors":["Changhong Fu","Ziyuan Huang","Yiming Li","Ran Duan","Peng Lu"],"categories":null,"content":"\nFig. 1 Main structure of the proposed tracking approach.\n\n","date":1561334400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561334400,"objectID":"261d6e3cb4942a75c4083e904f782650","permalink":"https://vision4robotics.github.io/publication/2019_iros_bevt-tracker/","publishdate":"2019-06-24T00:00:00Z","relpermalink":"/publication/2019_iros_bevt-tracker/","section":"publication","summary":"Due to implicitly introduced periodic shifting of limited searching area, visual object tracking using correlation filters often has to confront undesired boundary effect. As boundary effect severely degrade the quality of object model, it has made it a challenging task for unmanned aerial vehicles (UAV) to perform robust and accurate object following. Traditional hand-crafted features are also not precise and robust enough to describe the object in the viewing point of UAV. In this work, a novel tracker with online enhanced background learning is specifically proposed to tackle boundary effects. Real background samples are densely extracted to learn as well as update correlation filters. Spatial penalization is introduced to offset the noise introduced by exceedingly more background information so that a more accurate appearance model can be established. Meanwhile, convolutional features are extracted to provide a more comprehensive representation of the object. In order to mitigate changes of objects' appearances, multi-frame technique is applied to learn an ideal response map and verify the generated one in each frame. Exhaustive experiments were conducted on 100 challenging UAV image sequences and the proposed tracker has achieved state-of-the-art performance.","tags":["Correlation filter","Onject tracking","Unmanned aerial vehicles","Multi-Frame Consensus Veriﬁcation"],"title":"Boundary Effect-Aware Visual Tracking for UAV with Online Enhanced Background Learning and Multi-Frame Consensus Veriﬁcation","type":"publication"},{"authors":["Changhong Fu","Yinqiang Zhang","Ziyuan Huang","Ran Duan","Zongwu Xie"],"categories":null,"content":"\nFig. 1 Main structure of the proposed tracking approach.\n\n","date":1560211200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560211200,"objectID":"d52b309a8d46898dba500a69ea2a9fdd","permalink":"https://vision4robotics.github.io/publication/2019_ieee_access_pbbat-tracker/","publishdate":"2019-06-11T00:00:00Z","relpermalink":"/publication/2019_ieee_access_pbbat-tracker/","section":"publication","summary":"In recent years, visual tracking is a challenging task in UAV applications. The standard correlation filter (CF) has been extensively applied for UAV object tracking. However, the CF-based tracker severely suffers from boundary effects and cannot effectively cope with object occlusion, which results in suboptimal performance. Besides, it is still a tough task to obtain an appearance model precisely with hand-crafted features. In this paper, a novel part-based tracker is proposed for the UAV. With successive cropping operations, the tracking object is separated into several parts. More specially, background-aware correlation filters with different cropping matrices are applied. To estimate the translation and scale variation of the tracking object, a structure comparison, and a Bayesian inference approach are proposed, which jointly achieve a coarse-to-fine strategy. Moreover, an adaptive mechanism is used to update the local appearance model of each part with a Gaussian process regression method. To construct a better appearance model, features extracted from the convolutional neural network are utilized instead of hand-crafted features. Through extensive experiments, the proposed tracker reaches competitive performance on 123 challenging UAV image sequences and outperforms other 20 popular state-of-the-art visual trackers in terms of overall performance and different challenging attributes.","tags":["Visual tracking","Unmanned aerial vehicle (UAV)"],"title":"Part-Based Background-Aware Tracking for UAV with Convolutional Features","type":"publication"},{"authors":["Guang Chen","Shu Liu","Kejia Ren","Zhongnan Qu","Changhong Fu","Gereon Hinz","Alois Knoll"],"categories":null,"content":"![SSIM-WMIL_workflow](featured.jpg) Fig. 1 The closed-loop control structure for the long-term navigation of the quadrotor UAV in real-time application.  -- ","date":1560124800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560124800,"objectID":"67e10887ad282922fd5dee0e7a0e3c87","permalink":"https://vision4robotics.github.io/publication/2019_iet_intell_transp_syst/","publishdate":"2019-06-10T00:00:00Z","relpermalink":"/publication/2019_iet_intell_transp_syst/","section":"publication","summary":"Advanced communication technology of IoT era enables a heterogeneous connectivity where mobile devices broadcast information to everything. Previous short-range on-board sensor perception system attached to moblie applications such as robots and vehicles could be transferred to long-range mobilesensing perception system, which can be used as part of a more extensive intelligent system surveilling real-time state of the environment. However, the mobile sensing perception brings new challenges for how to efficiently analyze and intelligently interpret the deluge of IoT data in mission-critical services. In this article, we model the challenges as latency, packet loss and measurement noise which severely deteriorate the reliability and quality of IoT data. We integrate the artificial intelligence into IoT to tackle these challenges. We propose a novel architecture that leverages recurrent neural networks (RNN) and Kalman filtering to anticipate motions and interactions between objects. The basic idea is to learn environment dynamics by recurrent networks. To improve the robustness of IoT communication, we use the idea of Kalman filtering and deploy a prediction and correction step. In this way, the architecture learns to develop a biased belief between prediction and measurement in the different situation. We demonstrate our approach with synthetic and real-world datasets with noise that mimics the challenges of IoT communications. Our method brings a new level of IoT intelligence. It is also lightweight compared to other state-of-theart convolutional recurrent architecture and is ideally suitable for the resource-limited mobile applications","tags":["Recurrent neural network","Internet of things","Kalman filtering","Convolutional LSTM","Factor graph"],"title":"Deep Anticipation: Lightweight Intelligent Mobile Sensing for Unmanned Vehicles in IoT by Recurrent Architecture","type":"publication"},{"authors":["Andriy Sarabakha","Changhong Fu","Erdal Kayacan"],"categories":null,"content":"![SSIM-WMIL_workflow](featured.jpg) Fig. 1 The closed-loop control structure for the long-term navigation of the quadrotor UAV in real-time application.  -- ","date":1557446400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557446400,"objectID":"8ce82b86a51519a1e30d70a72724343f","permalink":"https://vision4robotics.github.io/publication/2019_appl_soft_comput/","publishdate":"2019-05-10T00:00:00Z","relpermalink":"/publication/2019_appl_soft_comput/","section":"publication","summary":"Although a considerable amount of effort has been put in to show that fuzzy logic controllers have exceptional capabilities of dealing with uncertainty, there are still noteworthy concerns, e.g., the design of fuzzy logic controllers is an arduous task due to the lack of closed-form input–output relationships which is a limitation to interpretability of these controllers. The role of design parameters in fuzzy logic controllers, such as position, shape, and height of membership functions, is not straightforward. Motivated by the fact that the availability of an interpretable relationship from input to output will simplify the design procedure of fuzzy logic controllers, the main aims in this work are derive fuzzy mappings for both type-1 and interval type-2 fuzzy logic controllers, analyse them, and eventually benefit from such a nonlinear mapping to design fuzzy logic controllers. Thereafter, simulation and real-time experimental results support the presented theoretical findings.","tags":["Type-1 fuzzy logic controllers","Interval type-2 fuzzy logic controllers","Fuzzy mapping","Aerial robotics","Unmanned aerial vehicles"],"title":"Intuit Before Tuning: Type-1 and Type-2 Fuzzy Logic Controllers","type":"publication"},{"authors":["Changhong Fu","Fuling Lin","Yiming Li","Guang Chen"],"categories":null,"content":"\nFig. 1 Main structure of the proposed tracking approach.\n\n","date":1551830400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551830400,"objectID":"9c89d8499279bdf9ef669630a3a66cf0","permalink":"https://vision4robotics.github.io/publication/2019_remote_sens_omfl-tracker/","publishdate":"2019-03-06T00:00:00Z","relpermalink":"/publication/2019_remote_sens_omfl-tracker/","section":"publication","summary":"In this paper, a novel online learning-based tracker is presented for the unmanned aerial vehicle (UAV) in different types of tracking applications.","tags":["Visual tracking","Unmanned aerial vehicle (UAV)","Background-aware correlation filter","Online multi-feature learning","Peak-to-sidelobe ratio (PSR)","Response map fusion"],"title":"Correlation Filter-Based Visual Tracking for UAV with Online Multi-Feature Learning","type":"publication"},{"authors":["Changhong Fu","Ran Duan","Erdal Kayacan"],"categories":null,"content":"\nFig. 1 Main structure of the proposed tracking approach.\n\n","date":1546041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546041600,"objectID":"cd794bb37b2cab0f1083e488a11e406a","permalink":"https://vision4robotics.github.io/publication/2019_inf_sci_ssim-wmil/","publishdate":"2018-12-29T00:00:00Z","relpermalink":"/publication/2019_inf_sci_ssim-wmil/","section":"publication","summary":"This paper presents an online adaptive tracker, which employs a novel weighted multiple instance learning (WMIL) approach.In the proposed tracker, both positive and negative sample importances are integrated into an online learning mechanism for improving tracking performance in challenging environments. The sample importance is computed based on a new measure, i.e., structural similarity (SSIM), instead of using the Euclidean distance. Moreover, a novel bag probability function, which adopts both positive and negative weighted instance probabilities, is designed. Furthermore, a novel efficient weak classifier selection solution is developed for the proposed tracker. Qualitative and quantitative experiments on 30 challenging image sequences show that the novel tracking algorithm, i.e., SSIM-WMIL tracker, performs favorably against the MIL and WMIL counterparts as well as other 13 recently-proposed state-of-the-art trackers in terms of accuracy, robustness and efficiency. In addition, the negative sample importance can be used to enhance the multiple instance learning, and the SSIM-based approach is capable of improving the multiple instance learning performance for object tracking when compared to the Euclidean distance-based method.","tags":["Visual tracking","Multiple instance learning","Structural similarity","Negative sample importance","Bag probability function","Weak classifier selection"],"title":"Visual Tracking With Online Structural Similarity-Based Weighted Multiple Instance Learning","type":"publication"},{"authors":["Changhong Fu","Yinqiang Zhang","Ran Duan","Zongwu Xie"],"categories":null,"content":"\nFig. 1 Main structure of the proposed tracking approach.\n\n","date":1539820800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539820800,"objectID":"0930f45b6aefa5864f0fe417409c6351","permalink":"https://vision4robotics.github.io/publication/2018_robio_spbacf-tracker/","publishdate":"2018-10-18T00:00:00Z","relpermalink":"/publication/2018_robio_spbacf-tracker/","section":"publication","summary":"Robust visual tracking for the unmanned aerial vehicle (UAV) is a challenging task in different types of civilian UAV applications. Although the classical correlation filter (CF) has been widely applied for UAV object tracking, the background of the object is not learned in the classical CF. In addition, the classical CF cannot estimate the object scale changes, and it is not able to cope with object occlusion effectively. Part-based tracking approach is often used for the visual tracker to solve the occlusion issue. However, its real-time performance for the UAV cannot be achieved due to the high cost of object appearance updating. In this paper, a novel robust visual tracker is presented for the UAV. The object is initially divided into multiple parts, and different background-aware correlation filters are applied for these divided object parts, respectively. An efficient coarse-to-fine strategy with structure comparison and Bayesian inference approach is proposed to locate object and estimate the object scale changes. In addition, an adaptive threshold is presented to update each local appearance model with a Gaussian process regression method. Qualitative and quantitative tests show that the presented visual tracking algorithm reaches real-time performance (i.e., more than twenty frames per second) on an i7 processor with 640×360 image resolution, and performs favorably against the most popular state-of-the-art visual trackers in terms of robustness and accuracy. To the best of our knowledge, it is the first time that this novel scalable part-based visual tracker is presented, and applied for the UAV tracking applications.","tags":["Correlation filter","Onject tracking","Unmanned aerial vehicles","Real-time systems"],"title":"Robust Scalable Part-Based Visual Tracking for UAV with Background-Aware Correlation Filter","type":"publication"},{"authors":["Changhong Fu","Andriy Sarabakha","Erdal Kayacan","Christian Wagner","Robert John","Jon Garibaldi"],"categories":null,"content":"\nFig. 1 The closed-loop control structure for the long-term navigation of the quadrotor UAV in real-time application.\n\n","date":1519776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519776000,"objectID":"d6e3c6233bd65d3fee4a94c25879af84","permalink":"https://vision4robotics.github.io/publication/2018_tmech/","publishdate":"2018-02-28T00:00:00Z","relpermalink":"/publication/2018_tmech/","section":"publication","summary":"Input uncertainty, e.g., noise on the on-board camera and inertial measurement unit, in vision-based control of unmanned aerial vehicles (UAVs) is an inevitable problem. In order to handle input uncertainties as well as further analyze the interaction between the input and the antecedent fuzzy sets (FSs) of nonsingleton fuzzy logic controllers (NSFLCs), an input uncertainty sensitivity enhanced NSFLC has been developed in robot operating system using the C++ programming language. Based on recent advances in nonsingleton inference, the centroid of the intersection of the input and antecedent FSs (Cen-NSFLC) is utilized to calculate the firing strength of each rule instead of the maximum of the intersection used in traditional NSFLC (Tra-NSFLC). An 8-shaped trajectory, consisting of straight and curved lines, is used for the real-time validation of the proposed controllers for a trajectory following problem. An accurate monocular keyframe-based visual-inertial simultaneous localization and mapping (SLAM) approach is used to estimate the position of the quadrotor UAV in GPS-denied unknown environments. The performance of the Cen-NSFLC is compared with a conventional proportional-integral derivative (PID) controller, a singleton FLC and a Tra-NSFLC. All controllers are evaluated for different flight speeds, thus introducing different levels of uncertainty into the control problem. Visual-inertial SLAM-based real-time quadrotor UAV flight tests demonstrate that not only does the Cen-NSFLC achieve the best control performance among the four controllers, but it also shows better control performance when compared to their singleton counterparts. Considering the bias in the use of model-based controllers, e.g., PID, for the control of UAVs, this paper advocates an alternative method, namely Cen-NSFLCs, in uncertain working environments.","tags":["Fuzzy logic controller (FLC)","input uncertainty sensitivity enhanced nonsingleton FLC (NSFLC)","Monocular visual-inertial simultaneous localization and mapping (SLAM)","Unmanned aerial vehicle (UAV)"],"title":"Input Uncertainty Sensitivity Enhanced Nonsingleton Fuzzy Logic Controllers for Long-Term Navigation of Quadrotor UAVs","type":"publication"}]