[{"authors":["changhong-fu"],"categories":null,"content":"Changhong Fu received the Ph.D. degree in robotics and automation from the Computer Vision and Aerial Robotics (CVAR) Laboratory, Technical University of Madrid, Madrid, Spain, in 2015. During his Ph.D., he held two research positions at Arizona State University, Tempe, AZ, USA, and Nanyang Technological University (NTU), Singapore. After receiving his Ph.D., he worked at NTU as a Post-Doctoral Research Fellow. He is currently an Associate Professor with the School of Mechanical Engineering, Tongji University, Shanghai, China, and leading more than five projects related to the vision for unmanned systems (US). In addition, he has published more than 80 journal and conference papers (including the IEEE GRS Magazine, IEEE TII, IEEE TITS, IEEE TMC, IEEE TGRS, IEEE TCSVT, IEEE TMM, IEEE TMECH, IEEE TIE, IEEE RA Letters, CVPR, ICCV, ICRA, IROS) related to the intelligent vision and control for UAV. His research areas are intelligent vision and control for US in a complex environment.\n","date":1671667200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1671667200,"objectID":"4d44576c929bb566eeb3c1d75eab7732","permalink":"https://vision4robotics.github.io/authors/changhong-fu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/changhong-fu/","section":"authors","summary":"Changhong Fu received the Ph.D. degree in robotics and automation from the Computer Vision and Aerial Robotics (CVAR) Laboratory, Technical University of Madrid, Madrid, Spain, in 2015. During his Ph.D., he held two research positions at Arizona State University, Tempe, AZ, USA, and Nanyang Technological University (NTU), Singapore. After receiving his Ph.D., he worked at NTU as a Post-Doctoral Research Fellow. He is currently an Associate Professor with the School of Mechanical Engineering, Tongji University, Shanghai, China, and leading more than five projects related to the vision for unmanned systems (US).","tags":null,"title":"Changhong Fu","type":"authors"},{"authors":["haobo-zuo"],"categories":null,"content":"Haobo Zuo is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include UAV, artificial intelligence, and computer vision.\n","date":1671667200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1671667200,"objectID":"0f0f6be7d2bd3b6e5f9b256966e4a5d3","permalink":"https://vision4robotics.github.io/authors/haobo-zuo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/haobo-zuo/","section":"authors","summary":"Haobo Zuo is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include UAV, artificial intelligence, and computer vision.","tags":null,"title":"Haobo Zuo","type":"authors"},{"authors":["kunhan-lu"],"categories":null,"content":"Kunhan Lu received his B.Eng. degree in mechanical engineering from Chongqing University, Chongqing, China. He is now pursuing the M.Sc. degree in mechanical engineering at Tongji University, Shanghai, China. His research interests include visual object tracking, deep learning, and computer vision.\n","date":1671667200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1671667200,"objectID":"b49d63425a5e1f8655b7a44f15162a02","permalink":"https://vision4robotics.github.io/authors/kunhan-lu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/kunhan-lu/","section":"authors","summary":"Kunhan Lu received his B.Eng. degree in mechanical engineering from Chongqing University, Chongqing, China. He is now pursuing the M.Sc. degree in mechanical engineering at Tongji University, Shanghai, China. His research interests include visual object tracking, deep learning, and computer vision.","tags":null,"title":"Kunhan Lu","type":"authors"},{"authors":["sihang-li"],"categories":null,"content":"Sihang Li received the B.Eng. degree in mechanical engineering in 2022 from Tongji University, Shanghai, China. His research interests include robotics, adversarial machine learning, and computer vision.\n","date":1671667200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1671667200,"objectID":"358864c9e81712ad520a18e231a9a5b9","permalink":"https://vision4robotics.github.io/authors/sihang-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/sihang-li/","section":"authors","summary":"Sihang Li received the B.Eng. degree in mechanical engineering in 2022 from Tongji University, Shanghai, China. His research interests include robotics, adversarial machine learning, and computer vision.","tags":null,"title":"Sihang Li","type":"authors"},{"authors":["yiming-li"],"categories":null,"content":"Yiming Li received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing PhD in Robotics at New York University, USA. His research interests include robotics and visual object tracking.\n","date":1671667200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1671667200,"objectID":"74ab2b740364fa8299cfb4d6742cccdd","permalink":"https://vision4robotics.github.io/authors/yiming-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yiming-li/","section":"authors","summary":"Yiming Li received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing PhD in Robotics at New York University, USA. His research interests include robotics and visual object tracking.","tags":null,"title":"Yiming Li","type":"authors"},{"authors":["bowen-li"],"categories":null,"content":"Bowen Li received his B.Eng. degree in Mechanical Engineering from Tongji University, Shanghai, China, in 2022. He is currently pursuing the PhD degree in Robotics at Carnegie Mellon University (CMU), USA. His research interests include robotics, artificial intelligence, and computer vision.\n","date":1669420800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1669420800,"objectID":"50b3204098dafa7a3d9415e91dcb3d02","permalink":"https://vision4robotics.github.io/authors/bowen-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/bowen-li/","section":"authors","summary":"Bowen Li received his B.Eng. degree in Mechanical Engineering from Tongji University, Shanghai, China, in 2022. He is currently pursuing the PhD degree in Robotics at Carnegie Mellon University (CMU), USA. His research interests include robotics, artificial intelligence, and computer vision.","tags":null,"title":"Bowen Li","type":"authors"},{"authors":["guangze-zheng"],"categories":null,"content":"Guangze Zheng received his B.Eng. degree in Mechanical Engineering from Tongji University, Shanghai, China, in 2022. He is currently pursuing the PhD degree in Computer Science at HongKong University (HKU), China. His research interests include visual object tracking and machine learning.\n","date":1669420800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1669420800,"objectID":"1ca1e48351fd704681e10048d6811556","permalink":"https://vision4robotics.github.io/authors/guangze-zheng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/guangze-zheng/","section":"authors","summary":"Guangze Zheng received his B.Eng. degree in Mechanical Engineering from Tongji University, Shanghai, China, in 2022. He is currently pursuing the PhD degree in Computer Science at HongKong University (HKU), China. His research interests include visual object tracking and machine learning.","tags":null,"title":"Guangze Zheng","type":"authors"},{"authors":["junjie-ye"],"categories":null,"content":"Junjie Ye received the B.Eng. degree in mechanical engineering in 2020 from Tongji University, Shanghai, China, where he is currently working toward the M.Sc. degree in mechanical engineering. His research interests include visual object tracking, deep learning, and robotics.\n","date":1669420800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1669420800,"objectID":"fe2fccb538ad42b64242c0535fb42663","permalink":"https://vision4robotics.github.io/authors/junjie-ye/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/junjie-ye/","section":"authors","summary":"Junjie Ye received the B.Eng. degree in mechanical engineering in 2020 from Tongji University, Shanghai, China, where he is currently working toward the M.Sc. degree in mechanical engineering. His research interests include visual object tracking, deep learning, and robotics.","tags":null,"title":"Junjie Ye","type":"authors"},{"authors":["ziyuan-huang"],"categories":null,"content":"Ziyuan Huang received his B.Eng. degree in vehicle engineering from Tongji University, Shanghai, China. He is currently pursuing PhD in Mechanical Engineering, National University of Singapore (NUS), Singapore. His research interests involve visual tracking for unmanned aerial vehicles and computer vision.\n","date":1668902400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1668902400,"objectID":"1d5ccc4aa1c5de52ed53972a45961a66","permalink":"https://vision4robotics.github.io/authors/ziyuan-huang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ziyuan-huang/","section":"authors","summary":"Ziyuan Huang received his B.Eng. degree in vehicle engineering from Tongji University, Shanghai, China. He is currently pursuing PhD in Mechanical Engineering, National University of Singapore (NUS), Singapore. His research interests involve visual tracking for unmanned aerial vehicles and computer vision.","tags":null,"title":"Ziyuan Huang","type":"authors"},{"authors":["haolin-Dong"],"categories":null,"content":"Haolin Dong received the B.Eng. degree in mechanical engineering in 2022 from Tongji University, Shanghai, China, where he is currently working toward the M.Sc. degree in mechanical engineering. His research interests include low-light image enhancement and machine learning.\n","date":1656547200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1656547200,"objectID":"393487efe7cf43c7cb6d7496b4caa136","permalink":"https://vision4robotics.github.io/authors/haolin-dong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/haolin-dong/","section":"authors","summary":"Haolin Dong received the B.Eng. degree in mechanical engineering in 2022 from Tongji University, Shanghai, China, where he is currently working toward the M.Sc. degree in mechanical engineering. His research interests include low-light image enhancement and machine learning.","tags":null,"title":"Haolin Dong","type":"authors"},{"authors":["jilin-zhao"],"categories":null,"content":"Jilin Zhao received his B.Eng. degree in mechanical engineering in 2021 from Tongji University, Shanghai, China, where he is currently pursuing the M.Sc. degree. His research interests include visual object tracking, deep learning, and computer vision.\n","date":1656547200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1656547200,"objectID":"702af82e1991df761b4f5893ce247ec8","permalink":"https://vision4robotics.github.io/authors/jilin-zhao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jilin-zhao/","section":"authors","summary":"Jilin Zhao received his B.Eng. degree in mechanical engineering in 2021 from Tongji University, Shanghai, China, where he is currently pursuing the M.Sc. degree. His research interests include visual object tracking, deep learning, and computer vision.","tags":null,"title":"Jilin Zhao","type":"authors"},{"authors":["weiyu-peng"],"categories":null,"content":"Weiyu Peng received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing M.Sc. degree in mechanical engineering at Tongji University, Shanghai, China. His research interests include deep learning, robotics, and visual object tracking.\n","date":1656547200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1656547200,"objectID":"da145e55e892c20569410284ed79d85f","permalink":"https://vision4robotics.github.io/authors/weiyu-peng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/weiyu-peng/","section":"authors","summary":"Weiyu Peng received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing M.Sc. degree in mechanical engineering at Tongji University, Shanghai, China. His research interests include deep learning, robotics, and visual object tracking.","tags":null,"title":"Weiyu Peng","type":"authors"},{"authors":["ziang-cao"],"categories":null,"content":"Ziang Cao received his B.Eng. degree in Vehicle Engineering from Tongji University, Shanghai, China, in 2022. He is currently pursuing PhD in Computer Science at Nanyang Technological University (NTU), Singapore. His research interests include unmanned aerial vehicle and visual object tracking.\n","date":1656547200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1656547200,"objectID":"4fd79dd332235ee46766fc8986a07748","permalink":"https://vision4robotics.github.io/authors/ziang-cao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ziang-cao/","section":"authors","summary":"Ziang Cao received his B.Eng. degree in Vehicle Engineering from Tongji University, Shanghai, China, in 2022. He is currently pursuing PhD in Computer Science at Nanyang Technological University (NTU), Singapore. His research interests include unmanned aerial vehicle and visual object tracking.","tags":null,"title":"Ziang Cao","type":"authors"},{"authors":["kunhui-chen"],"categories":null,"content":"Kunhui Chen received his B. Eng. degree in mechanical and electronic engineering From Guangdong University of Technology, Guangzhou, China. He is currently pursuing M. Sc. degree in mechanical engineering in Tongji University, Shanghai, China. His research interests include visual measurement, object detection, and deep learning.\n","date":1652313600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1652313600,"objectID":"86ad1f48c0fb1cfc4b202cebe4497480","permalink":"https://vision4robotics.github.io/authors/kunhui-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/kunhui-chen/","section":"authors","summary":"Kunhui Chen received his B. Eng. degree in mechanical and electronic engineering From Guangdong University of Technology, Guangzhou, China. He is currently pursuing M. Sc. degree in mechanical engineering in Tongji University, Shanghai, China. His research interests include visual measurement, object detection, and deep learning.","tags":null,"title":"Kunhui Chen","type":"authors"},{"authors":["fangqiang-ding"],"categories":null,"content":"Fangqiang Ding received his B.Eng. degree in Mechanical Engineering from Tongji University, Shanghai, China. He is currently pursuing the PhD degree in Robotics and Autonomous System at The University of Edinburgh, Edinburgh, U.K.. His research interests include unmanned aerial vehicle and computer Vision.\n","date":1647907200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1647907200,"objectID":"e5afbfabb290cc0d7ec63af348a5ecf6","permalink":"https://vision4robotics.github.io/authors/fangqiang-ding/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/fangqiang-ding/","section":"authors","summary":"Fangqiang Ding received his B.Eng. degree in Mechanical Engineering from Tongji University, Shanghai, China. He is currently pursuing the PhD degree in Robotics and Autonomous System at The University of Edinburgh, Edinburgh, U.K.. His research interests include unmanned aerial vehicle and computer Vision.","tags":null,"title":"Fangqiang Ding","type":"authors"},{"authors":["fuling-lin"],"categories":null,"content":"Fuling Lin received his B.Eng. degree and M.Sc. degree in mechanical engineering from Tongji University, Shanghai, China, in 2019 and 2022, respectively. He is currently pursuing the PhD degree in mechanical engineering at HongKong University (HKU), China.\n","date":1647907200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1647907200,"objectID":"3bcddded919311d425d4fbd3ccb1c10d","permalink":"https://vision4robotics.github.io/authors/fuling-lin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/fuling-lin/","section":"authors","summary":"Fuling Lin received his B.Eng. degree and M.Sc. degree in mechanical engineering from Tongji University, Shanghai, China, in 2019 and 2022, respectively. He is currently pursuing the PhD degree in mechanical engineering at HongKong University (HKU), China.","tags":null,"title":"Fuling Lin","type":"authors"},{"authors":["xinnan-yuan"],"categories":null,"content":"Xinnan Yuan received his B.Eng. degree in Mechanical Engineering from Tongji University, Shanghai, China in 2022. He is currently pursuing the MSc degree in Artificial Intelligence at Nanyang Technological University (NTU), Singapore. His research interests include robotics and visual object tracking.\n","date":1643702400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1643702400,"objectID":"1c061ec3fba49c8f929b6323eb1b7566","permalink":"https://vision4robotics.github.io/authors/xinnan-yuan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xinnan-yuan/","section":"authors","summary":"Xinnan Yuan received his B.Eng. degree in Mechanical Engineering from Tongji University, Shanghai, China in 2022. He is currently pursuing the MSc degree in Artificial Intelligence at Nanyang Technological University (NTU), Singapore. His research interests include robotics and visual object tracking.","tags":null,"title":"Xinnan Yuan","type":"authors"},{"authors":["ran-duan"],"categories":null,"content":"Ran Duan received the Master degree in computer vision in 2015, from the University of Bourgogne, France (European VIBOT program). From 2015 to 2017, he worked at Nanyang Technological University (NTU) as a research associate. During 2021, he has exchanged scientific researches at Tongji University, Shanghai, China. He is currently a PhD candidate at the Department of Aeronautical and Aviation Engineering (AAE), the Hong Kong Polytechnic University. His research areas include image processing, visual tracking, and UAV visual navigation.\n","date":1641081600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1641081600,"objectID":"400624d450edf15bad2dda1fe6682867","permalink":"https://vision4robotics.github.io/authors/ran-duan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ran-duan/","section":"authors","summary":"Ran Duan received the Master degree in computer vision in 2015, from the University of Bourgogne, France (European VIBOT program). From 2015 to 2017, he worked at Nanyang Technological University (NTU) as a research associate. During 2021, he has exchanged scientific researches at Tongji University, Shanghai, China. He is currently a PhD candidate at the Department of Aeronautical and Aviation Engineering (AAE), the Hong Kong Polytechnic University. His research areas include image processing, visual tracking, and UAV visual navigation.","tags":null,"title":"Ran Duan","type":"authors"},{"authors":["jin-jin"],"categories":null,"content":"Jin Jin received his B.Eng. degree in Mechanical Engineering from Tongji University, Shanghai, China. He is currently pursuing the M.Sc. degree in Management, Technology, and Economics at ETH, Zurich, Switzerland. He majors in Mechanical Engineering and is now specializing in visual object tracking as well.\n","date":1632614400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1632614400,"objectID":"5a89def764852e0117af9268b9bc82b1","permalink":"https://vision4robotics.github.io/authors/jin-jin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jin-jin/","section":"authors","summary":"Jin Jin received his B.Eng. degree in Mechanical Engineering from Tongji University, Shanghai, China. He is currently pursuing the M.Sc. degree in Management, Technology, and Economics at ETH, Zurich, Switzerland. He majors in Mechanical Engineering and is now specializing in visual object tracking as well.","tags":null,"title":"Jin Jin","type":"authors"},{"authors":["weijiang-xiong"],"categories":null,"content":"Weijiang Xiong received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing M.Sc. degree in Robotics, Aalto University, Finland. His research interests include visual object tracking and artificial intelligence.\n","date":1625011200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1625011200,"objectID":"a60def9cac4a1c8f7aad2558c4a06e69","permalink":"https://vision4robotics.github.io/authors/weijiang-xiong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/weijiang-xiong/","section":"authors","summary":"Weijiang Xiong received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing M.Sc. degree in Robotics, Aalto University, Finland. His research interests include visual object tracking and artificial intelligence.","tags":null,"title":"Weijiang Xiong","type":"authors"},{"authors":["yujie-he"],"categories":null,"content":"Yujie He received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing M.Sc. degree in Robotics, École Polytechnique Fédérale de Lausanne (EPFL), Switzerland. His research interests include robotics, visual object tracking, and place recognition.\n","date":1625011200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1625011200,"objectID":"388dc41701aaac09a116f4f6c6c06830","permalink":"https://vision4robotics.github.io/authors/yujie-he/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yujie-he/","section":"authors","summary":"Yujie He received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing M.Sc. degree in Robotics, École Polytechnique Fédérale de Lausanne (EPFL), Switzerland. His research interests include robotics, visual object tracking, and place recognition.","tags":null,"title":"Yujie He","type":"authors"},{"authors":["juntao-xu"],"categories":null,"content":"Juntao Xu received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing PhD in Mechanical Engineering, Hong Kong University, China. His research interests involve visual object tracking and computer vision.\n","date":1602028800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1602028800,"objectID":"284a7f7acfea514aee71c7fe9afc9da8","permalink":"https://vision4robotics.github.io/authors/juntao-xu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/juntao-xu/","section":"authors","summary":"Juntao Xu received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing PhD in Mechanical Engineering, Hong Kong University, China. His research interests involve visual object tracking and computer vision.","tags":null,"title":"Juntao Xu","type":"authors"},{"authors":["changjing-liu"],"categories":null,"content":"Changjing Liu received the B.E. degree in Mechanical Engineering from Tongji University, Shanghai, China. He is currently working toward the M.Sc. degree in Instrument Science and Technology at Shanghai Jiao Tong University, Shanghai, China. His research interests include robotics and computer Vision.\n","date":1593561600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1593561600,"objectID":"296ffb07ffade55bcb1583aff8e5990c","permalink":"https://vision4robotics.github.io/authors/changjing-liu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/changjing-liu/","section":"authors","summary":"Changjing Liu received the B.E. degree in Mechanical Engineering from Tongji University, Shanghai, China. He is currently working toward the M.Sc. degree in Instrument Science and Technology at Shanghai Jiao Tong University, Shanghai, China. His research interests include robotics and computer Vision.","tags":null,"title":"Changjing Liu","type":"authors"},{"authors":["xiaoxiao-yang"],"categories":null,"content":"Xiaoxiao Yang received a B.E. degree in control science and engineering from Tongji University, Shanghai, China. He is currently working toward the M.Sc. degree in Automation at Shanghai Jiao Tong University, Shanghai, China. His research interests include robotics and visual object tracking.\n","date":1593561600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1593561600,"objectID":"683d6e977f4a6d54ea09b4b67d85888f","permalink":"https://vision4robotics.github.io/authors/xiaoxiao-yang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xiaoxiao-yang/","section":"authors","summary":"Xiaoxiao Yang received a B.E. degree in control science and engineering from Tongji University, Shanghai, China. He is currently working toward the M.Sc. degree in Automation at Shanghai Jiao Tong University, Shanghai, China. His research interests include robotics and visual object tracking.","tags":null,"title":"Xiaoxiao Yang","type":"authors"},{"authors":["yinqiang-zhang"],"categories":null,"content":"Yinqiang Zhang received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing M.Sc. degree in mechatronics and information technology, Technical University of Munich (TUM), Munich, Germany. His research interests include visual tracking and computer vision.\n","date":1586217600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1586217600,"objectID":"b2c3db1f8b9ba4f5d084ca76dfab0758","permalink":"https://vision4robotics.github.io/authors/yinqiang-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yinqiang-zhang/","section":"authors","summary":"Yinqiang Zhang received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing M.Sc. degree in mechatronics and information technology, Technical University of Munich (TUM), Munich, Germany. His research interests include visual tracking and computer vision.","tags":null,"title":"Yinqiang Zhang","type":"authors"},{"authors":["admin"],"categories":null,"content":" The vision4robotics group is a multidisciplinary research group at Tongji University. Our research interests focus on intelligent vision and control technologies for robotics.\nRecruit Looking for new students/engineers to work on exciting AI-driven vision-based research and projects (with a limited quota). Candidates with research experience in vision-based Deep Learning, Object Tracking/ Detection/ Segmentation/ Recognition will be considered with higher priority. Please send your CV and a 3-slide PPT describing your previous research experience via changhongfu@tongji.edu.cn if you have an interest. For PhD/PostDoc application, please also attach one of your best papers. （备注：针对硕士与博士（后）申请，为更有效地开展前沿研究工作，原则上优先考虑已具备计算机视觉研究基础与较好python/C++编程经验的申请，亦欢迎已获得省部级及其以上科技竞（比）赛奖励的申请。针对研究生阶段表现优异者，将派去世界名校/研究所、著名AI企业等访问/实习。）\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://vision4robotics.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"The vision4robotics group is a multidisciplinary research group at Tongji University. Our research interests focus on intelligent vision and control technologies for robotics.\nRecruit Looking for new students/engineers to work on exciting AI-driven vision-based research and projects (with a limited quota). Candidates with research experience in vision-based Deep Learning, Object Tracking/ Detection/ Segmentation/ Recognition will be considered with higher priority. Please send your CV and a 3-slide PPT describing your previous research experience via changhongfu@tongji.","tags":null,"title":"Vision4robotics Group","type":"authors"},{"authors":["hao-chen"],"categories":null,"content":"Hao Chen received his B.Eng. degree in mechanical engineering from Chongqing University, Chongqing, China, in 2019 and his M.Sc degree in mechanical engineering from Tongji University, in 2022. He is currently a computer engineer at iFLYTEK, China.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"7123babb776e1a2719ea30a1de6294ce","permalink":"https://vision4robotics.github.io/authors/hao-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/hao-chen/","section":"authors","summary":"Hao Chen received his B.Eng. degree in mechanical engineering from Chongqing University, Chongqing, China, in 2019 and his M.Sc degree in mechanical engineering from Tongji University, in 2022. He is currently a computer engineer at iFLYTEK, China.","tags":null,"title":"Hao Chen","type":"authors"},{"authors":["haowen-zheng"],"categories":null,"content":"Haowen Zheng received the B.Eng. degree in vehicle engineering in 2022 from Central South University, Changsha, China. He is currently working towards the Ph.D. degree in mechanical engineering in Tongji University. His research interests include deep learning, computer vision and edge intelligence.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"b5bbd8c619fc13f8c942d6a6d44e4075","permalink":"https://vision4robotics.github.io/authors/haowen-zheng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/haowen-zheng/","section":"authors","summary":"Haowen Zheng received the B.Eng. degree in vehicle engineering in 2022 from Central South University, Changsha, China. He is currently working towards the Ph.D. degree in mechanical engineering in Tongji University. His research interests include deep learning, computer vision and edge intelligence.","tags":null,"title":"Haowen Zheng","type":"authors"},{"authors":["haoyang-li"],"categories":null,"content":"Haoyang Li is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include object segmentation, computer vision and robotics.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"e8ed46297cf02b34d0c0d140d0f1f137","permalink":"https://vision4robotics.github.io/authors/haoyang-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/haoyang-li/","section":"authors","summary":"Haoyang Li is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include object segmentation, computer vision and robotics.","tags":null,"title":"Haoyang Li","type":"authors"},{"authors":["hongwei-chen"],"categories":null,"content":"Hongwei Chen is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include artificial intelligence and computer vision.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"53d24991868ce24c560936307d9fb7dd","permalink":"https://vision4robotics.github.io/authors/hongwei-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/hongwei-chen/","section":"authors","summary":"Hongwei Chen is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include artificial intelligence and computer vision.","tags":null,"title":"Hongwei Chen","type":"authors"},{"authors":["hua-lin"],"categories":null,"content":"Hua Lin is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include object detection and OCR.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"47346bda2c44d8264ec1e61e55e78354","permalink":"https://vision4robotics.github.io/authors/hua-lin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/hua-lin/","section":"authors","summary":"Hua Lin is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include object detection and OCR.","tags":null,"title":"Hua lin","type":"authors"},{"authors":["jianqiao-lu"],"categories":null,"content":"Jianqiao Lu received his B.Eng. degree in vehicle engineering from Tongji University, Shanghai, China. He is currently pursuing PhD in Computer Science, HongKong University (HKU), China. His research interests include robotics motion planning, biorobotics control, and place recognition.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"816035b05c4c141078b704fe19a1c71e","permalink":"https://vision4robotics.github.io/authors/jianqiao-lu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jianqiao-lu/","section":"authors","summary":"Jianqiao Lu received his B.Eng. degree in vehicle engineering from Tongji University, Shanghai, China. He is currently pursuing PhD in Computer Science, HongKong University (HKU), China. His research interests include robotics motion planning, biorobotics control, and place recognition.","tags":null,"title":"Jianqiao Lu","type":"authors"},{"authors":["jiuchun-yang"],"categories":null,"content":"JiuChun Yang is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include UAV,computer vision,and image enhancement.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"9b14a1d56bd15c62f5ee9818102f7a46","permalink":"https://vision4robotics.github.io/authors/jiuchun-yang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jiuchun-yang/","section":"authors","summary":"JiuChun Yang is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include UAV,computer vision,and image enhancement.","tags":null,"title":"Jiuchun Yang","type":"authors"},{"authors":["liangliang-yao"],"categories":null,"content":"Liangliang Yao is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include artificial intelligence, computer vision, and deep learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"651994acd0d97920b2165a0a2e8fe696","permalink":"https://vision4robotics.github.io/authors/liangliang-yao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/liangliang-yao/","section":"authors","summary":"Liangliang Yao is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include artificial intelligence, computer vision, and deep learning.","tags":null,"title":"Liangliang Yao","type":"authors"},{"authors":["liguo-zhang"],"categories":null,"content":"Liguo Zhang received the B.ENG . Degree in mechanical engineering in 2012 from Nanjing University of Aeronautics and Astronautics, Nanjing, China. He is currently working toward the M.Sc. Degree in mechanical engineering in Tongji University, Shanghai, china. His research interests include visual object tracking, Unmanned aerial manipulator and deep learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"78381d1c5769d070007aa8bf5620b5d5","permalink":"https://vision4robotics.github.io/authors/liguo-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/liguo-zhang/","section":"authors","summary":"Liguo Zhang received the B.ENG . Degree in mechanical engineering in 2012 from Nanjing University of Aeronautics and Astronautics, Nanjing, China. He is currently working toward the M.Sc. Degree in mechanical engineering in Tongji University, Shanghai, china. His research interests include visual object tracking, Unmanned aerial manipulator and deep learning.","tags":null,"title":"Liguo Zhang","type":"authors"},{"authors":["mutian-cai"],"categories":null,"content":"Mutian Cai is an undergraduate student at Tongji University and currently pursing B.Eng. degree in mechanical engineering. His research interests include robotics, UAV, computer vision, and artificial intelligence.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"35f90a22827fedac409d6cef7ca3daa2","permalink":"https://vision4robotics.github.io/authors/mutian-cai/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/mutian-cai/","section":"authors","summary":"Mutian Cai is an undergraduate student at Tongji University and currently pursing B.Eng. degree in mechanical engineering. His research interests include robotics, UAV, computer vision, and artificial intelligence.","tags":null,"title":"Mutian Cai","type":"authors"},{"authors":["shaoqiu-xu"],"categories":null,"content":"Shaoqiu Xu received a bachelor\u0026rsquo;s degree in mechanical engineering at Tongji University and is currently pursuing a Master\u0026rsquo;s degree at Shanghai Jiao Tong University. His research interests include object tracking and pose estimation.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"278d4354562b1001d3ddd31b27aaafc0","permalink":"https://vision4robotics.github.io/authors/shaoqiu-xu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/shaoqiu-xu/","section":"authors","summary":"Shaoqiu Xu received a bachelor\u0026rsquo;s degree in mechanical engineering at Tongji University and is currently pursuing a Master\u0026rsquo;s degree at Shanghai Jiao Tong University. His research interests include object tracking and pose estimation.","tags":null,"title":"Shaoqiu Xu","type":"authors"},{"authors":["shengbiao-lu"],"categories":null,"content":"Shengbiao Lu is an undergraduate student at Tongji University and now studying for B.Eng. degree in intelligent manufacturing engineering. Robot, artificial intelligence, and computer vision are his research interests.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"bae373687ff8c5017bbc825327ce84bb","permalink":"https://vision4robotics.github.io/authors/shengbiao-lu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/shengbiao-lu/","section":"authors","summary":"Shengbiao Lu is an undergraduate student at Tongji University and now studying for B.Eng. degree in intelligent manufacturing engineering. Robot, artificial intelligence, and computer vision are his research interests.","tags":null,"title":"Shengbiao Lu","type":"authors"},{"authors":["teng-li"],"categories":null,"content":"Li Teng received the B.Eng. degree in mechanical engineering in 2022 from Tongji University, Shanghai, China. His research interests include unmanned aerial vehicle and computer Vision.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"17be8edc583687f02f1b94788fa21e9e","permalink":"https://vision4robotics.github.io/authors/teng-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/teng-li/","section":"authors","summary":"Li Teng received the B.Eng. degree in mechanical engineering in 2022 from Tongji University, Shanghai, China. His research interests include unmanned aerial vehicle and computer Vision.","tags":null,"title":"Teng Li","type":"authors"},{"authors":["xiangpeng-zeng"],"categories":null,"content":"Xiangpeng Zeng received his B.Eng. degree in material forming and control engineering from East China University of Science and Technology, Shanghai, China, in 2019 and his M.Sc degree in mechanical engineering from Tongji University, Shanghai, China, in 2022. Currently, he is a computer engineer at Eastmoney Securities, China.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"1cb22594f1d06998e93397a29f031698","permalink":"https://vision4robotics.github.io/authors/xiangpeng-zeng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xiangpeng-zeng/","section":"authors","summary":"Xiangpeng Zeng received his B.Eng. degree in material forming and control engineering from East China University of Science and Technology, Shanghai, China, in 2019 and his M.Sc degree in mechanical engineering from Tongji University, Shanghai, China, in 2022. Currently, he is a computer engineer at Eastmoney Securities, China.","tags":null,"title":"Xiangpeng Zeng","type":"authors"},{"authors":["xiaogang-yu"],"categories":null,"content":"Xiaogang Yu received his B. Eng. degree in mechanical engineering in 2017 from Shanghai University, Shanghai, China. He is now pursuing the M. Sc. degree in mechanical engineering at Tongji University, Shanghai, China. His research interests include robotics, visual object tracking, and unmanned aerial vehicle.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"a1946ddff3d62eba9496eb47aa29182e","permalink":"https://vision4robotics.github.io/authors/xiaogang-yu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xiaogang-yu/","section":"authors","summary":"Xiaogang Yu received his B. Eng. degree in mechanical engineering in 2017 from Shanghai University, Shanghai, China. He is now pursuing the M. Sc. degree in mechanical engineering at Tongji University, Shanghai, China. His research interests include robotics, visual object tracking, and unmanned aerial vehicle.","tags":null,"title":"Xiaogang Yu","type":"authors"},{"authors":["xining-lu"],"categories":null,"content":"Xining Lu is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include robotics, automation, and computer vision.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"80d229c4e1f167d5ed819f404e02318b","permalink":"https://vision4robotics.github.io/authors/xining-lu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xining-lu/","section":"authors","summary":"Xining Lu is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include robotics, automation, and computer vision.","tags":null,"title":"Xining Lu","type":"authors"},{"authors":["yicheng-kang"],"categories":null,"content":"Yicheng Kang is an undergraduate student at Tongji University and currently pursuing B.Eng.degree in mechanical engineering. His research interests include deep learning, robot control and robot structure design.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"39dc250ed6206d29d89ff26578015ded","permalink":"https://vision4robotics.github.io/authors/yicheng-kang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yicheng-kang/","section":"authors","summary":"Yicheng Kang is an undergraduate student at Tongji University and currently pursuing B.Eng.degree in mechanical engineering. His research interests include deep learning, robot control and robot structure design.","tags":null,"title":"Yicheng Kang","type":"authors"},{"authors":["yihu-wang"],"categories":null,"content":"Yihu Wang received his B.Eng. degree in mechanical engineering in 2021 from Tongji University, Shanghai, China, where he is currently working toward the M.Sc. degree in mechanical engineering. His research interests include robotics, visual object tracking, and object detection.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"44f6f5f89fd816de69db2f36b1e38dba","permalink":"https://vision4robotics.github.io/authors/yihu-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yihu-wang/","section":"authors","summary":"Yihu Wang received his B.Eng. degree in mechanical engineering in 2021 from Tongji University, Shanghai, China, where he is currently working toward the M.Sc. degree in mechanical engineering. His research interests include robotics, visual object tracking, and object detection.","tags":null,"title":"Yihu Wang","type":"authors"},{"authors":["yiyong-sun"],"categories":null,"content":"Yiyong Sun is an incoming master student in Mechanical Engineering at the National University of Singapore (NUS) and currently pursuing his BEng degree in Mechanical Engineering at Tongji University. His research interests include robotics, visual object tracking for unmanned aerial vehicle (UAV), and machine learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"289a010c77c85364a2de7389ba94f586","permalink":"https://vision4robotics.github.io/authors/yiyong-sun/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yiyong-sun/","section":"authors","summary":"Yiyong Sun is an incoming master student in Mechanical Engineering at the National University of Singapore (NUS) and currently pursuing his BEng degree in Mechanical Engineering at Tongji University. His research interests include robotics, visual object tracking for unmanned aerial vehicle (UAV), and machine learning.","tags":null,"title":"Yiyong Sun","type":"authors"},{"authors":["yuchen-li"],"categories":null,"content":"Yuchen Li received the B.Eng. degree in mechanical engineering in 2022 from Tongji University, Shanghai, China, where she is currently working toward the M.Sc. degree in mechanical engineering. Her research interests include visual object tracking and machine learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"133708bcf28a9fc87b36f6ed906c506a","permalink":"https://vision4robotics.github.io/authors/yuchen-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yuchen-li/","section":"authors","summary":"Yuchen Li received the B.Eng. degree in mechanical engineering in 2022 from Tongji University, Shanghai, China, where she is currently working toward the M.Sc. degree in mechanical engineering. Her research interests include visual object tracking and machine learning.","tags":null,"title":"Yuchen Li","type":"authors"},{"authors":["yucheng-wang"],"categories":null,"content":"Yucheng Wang is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in automation. His research interests include unmanned aerial vehicle and computer Vision.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"5116650ab780aca88124bfcbd6ed58fb","permalink":"https://vision4robotics.github.io/authors/yucheng-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yucheng-wang/","section":"authors","summary":"Yucheng Wang is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in automation. His research interests include unmanned aerial vehicle and computer Vision.","tags":null,"title":"Yucheng Wang","type":"authors"},{"authors":["yufeng-liu"],"categories":null,"content":"Yufeng Liu is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include deep learning, object detection and robot control.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"70295c63dd9df6a5571d086fcf120093","permalink":"https://vision4robotics.github.io/authors/yufeng-liu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yufeng-liu/","section":"authors","summary":"Yufeng Liu is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include deep learning, object detection and robot control.","tags":null,"title":"Yufeng Liu","type":"authors"},{"authors":["yulin-li"],"categories":null,"content":"Yulin Li is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include robotics motion planning, biorobotics control, and place recognition.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"b1258af42bb4dfb5508c09da7b48273f","permalink":"https://vision4robotics.github.io/authors/yulin-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yulin-li/","section":"authors","summary":"Yulin Li is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include robotics motion planning, biorobotics control, and place recognition.","tags":null,"title":"Yulin Li","type":"authors"},{"authors":["zheng-shen"],"categories":null,"content":"Zheng Shen is currently a senior student at Tongji University, in pursuit of a B.Eng. degree in mechanical engineering with a specialization in mechatronics. His research interests involve control theory for unmanned aerial vehicles.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"96d70063e0299291f33c32adbd420931","permalink":"https://vision4robotics.github.io/authors/zheng-shen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zheng-shen/","section":"authors","summary":"Zheng Shen is currently a senior student at Tongji University, in pursuit of a B.Eng. degree in mechanical engineering with a specialization in mechatronics. His research interests involve control theory for unmanned aerial vehicles.","tags":null,"title":"Zheng Shen","type":"authors"},{"authors":["zhihua-hua"],"categories":null,"content":"Zhihua Hua is an undergraduate student at Tongji University and currently pursuing B.Eng.degree in mechanical engineering. His research interests include computer vision, artificial intelligence and object tracking.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"dab3735dc8c770b0e26d31b0da7e9913","permalink":"https://vision4robotics.github.io/authors/zhihua-hua/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zhihua-hua/","section":"authors","summary":"Zhihua Hua is an undergraduate student at Tongji University and currently pursuing B.Eng.degree in mechanical engineering. His research interests include computer vision, artificial intelligence and object tracking.","tags":null,"title":"Zhihua Hua","type":"authors"},{"authors":["zicheng-luo"],"categories":null,"content":"Zicheng Luo is an undergraduate student at Tongji University and now studying for B.Eng. degree in intelligent manufacturing engineering. His research interests include robot simulation and control, artificial intelligence, and computer network.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"bf17487a81994cf20cc5c1a8c885a7f0","permalink":"https://vision4robotics.github.io/authors/zicheng-luo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zicheng-luo/","section":"authors","summary":"Zicheng Luo is an undergraduate student at Tongji University and now studying for B.Eng. degree in intelligent manufacturing engineering. His research interests include robot simulation and control, artificial intelligence, and computer network.","tags":null,"title":"Zicheng Luo","type":"authors"},{"authors":["zijie-zhang"],"categories":null,"content":"Zijie Zhang received the B.Eng. degree in mechanical engineering in 2022 from Tongji University, Shanghai, China, where he is currently working toward the M.Sc. degree in mechanical engineering. His research interests include Adversarial attack, computer vision and robotics.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"fa2e3f90819426016e1f397bbbeb640b","permalink":"https://vision4robotics.github.io/authors/zijie-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zijie-zhang/","section":"authors","summary":"Zijie Zhang received the B.Eng. degree in mechanical engineering in 2022 from Tongji University, Shanghai, China, where he is currently working toward the M.Sc. degree in mechanical engineering. His research interests include Adversarial attack, computer vision and robotics.","tags":null,"title":"Zijie Zhang","type":"authors"},{"authors":["Haobo Zuo","Changhong Fu","Sihang Li","Kunhan Lu","Yiming Li","Chen Feng"],"categories":null,"content":" Overview of the proposed ABDNet. \n","date":1671667200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671667200,"objectID":"dcd225471c8d79a794daa675c06ea9d3","permalink":"https://vision4robotics.github.io/publication/2022_ral_abdnet/","publishdate":"2022-12-22T00:00:00Z","relpermalink":"/publication/2022_ral_abdnet/","section":"publication","summary":"Unmanned aerial vehicle (UAV) tracking has been widely applied in real-world applications such as surveillance and monitoring. However, the inherent high maneuverability and agility of UAV often lead to motion blur, which can impair the visual appearance of the target object and easily degrade the existing trackers. To overcome this challenge, this work proposes a tracking-oriented adversarial blur-deblur network (ABDNet), composed of a novel deblurrer to recover the visual appearance of the tracked object, and a brand-new blur generator to produce realistic blurry images for adversarial training. More specifically, the deblurrer progressively refines the features through pixel-wise, spatial-wise, and channel-wise stages to achieve excellent deblurring performance. The blur generator adaptively fuses an image sequence with a learnable kernel to create realistic blurry images. During training, ABDNet is plugged in the off-the-shelf real-time tracker, and trained with blurring-deblurring loss as well as tracking loss. During inference, the blur generator is removed, while the deblurrer and the tracker can work together for UAV tracking. Extensive experiments in both public datasets and real-world testing have validated the effectiveness of ABDNet.","tags":["UAV Tracking","Tracking-oriented adversarial blur-deblur"],"title":"Adversarial Blur-Deblur Network for Robust UAV Tracking","type":"publication"},{"authors":["Sihang Li","Changhong Fu","Kunhan Lu","Haobo Zuo","Yiming Li","Chen Feng"],"categories":null,"content":" Overview of our TRTrack. \n","date":1671667200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671667200,"objectID":"07740f5e3466b5bf6e7f88410a5331c9","permalink":"https://vision4robotics.github.io/publication/2022_ral_trtrack/","publishdate":"2022-12-22T00:00:00Z","relpermalink":"/publication/2022_ral_trtrack/","section":"publication","summary":"Siamese network-based object tracking has remarkably promoted the automatic capability for highly-maneuvered unmanned aerial vehicles (UAVs). However, the leading-edge tracking framework often depends on template matching, making it trapped when facing multiple views of object in consecutive frames. Moreover, the general image-level pretrained backbone can overfit to holistic representations, causing the misalignment to learn object-level properties in UAV tracking. To tackle these issues, this work presents TRTrack, a comprehensive framework to fully exploit the stereoscopic representation for UAV tracking. Specifically, a novel pretraining paradigm method is proposed. Through trajectory-aware reconstruction training, the capability of the backbone to extract stereoscopic structure feature is strengthened without any parameter increment. Accordingly, an innovative hierarchical self-attention Transformer is proposed to capture the local detail information and global structure knowledge. For optimizing the correlation map, we proposed a novel spatial correlation refinement (SCR) module, which promotes the capability of modeling the long-range spatial dependencies. Comprehensive experiments on three UAV challenging benchmarks demonstrate that the proposed TRTrack achieves superior UAV tracking performance in both precision and efficiency. Quantitative tests in real-world settings fully prove the effectiveness of our work. The code and demo videos are available at https://github.com/vision4robotics/TRTrack.","tags":["UAV Tracking","Trajectory-aware reconstruction training","Hierarchical self-attention Transformer","Spatial correlation refinement"],"title":"Boosting UAV Tracking with Voxel-Based Trajectory-Aware Pre-Training","type":"publication"},{"authors":["Guangze Zheng","Changhong Fu","Junjie Ye","Bowen Li","Geng Lu","Jia Pan"],"categories":null,"content":" Demonstration of the vision-based UAM approaching system and qualitative comparison.\n","date":1669420800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669420800,"objectID":"b955eef0b8aa0a18c8a9855b7efb2ddb","permalink":"https://vision4robotics.github.io/publication/2022_tii_siamsa/","publishdate":"2022-11-26T00:00:00Z","relpermalink":"/publication/2022_tii_siamsa/","section":"publication","summary":"In many industrial applications of unmanned aerial manipulator (UAM), visual approaching to the object is crucial to subsequent manipulating. In comparison with the widely-studied manipulating, the key to efficient vision-based UAM approaching, i.e., UAM object tracking, is still limited. Since traditional model-based UAM tracking is costly and cannot track arbitrary objects, an intuitive solution is to introduce state-of-the-art model-free Siamese trackers from the visual tracking field. Although Siamese tracking is most suitable for the onboard embedded processors, severe object scale variation in UAM tracking brings formidable challenges. To address these problems, this work proposes a novel model-free scale-aware Siamese tracker (SiamSA). Specifically, a scale attention network is proposed to emphasize scale awareness in feature processing. A scale-aware anchor proposal network is designed to achieve anchor proposing. Besides, two novel UAM tracking benchmarks are first recorded. Comprehensive experiments on benchmarks validate the effectiveness of SiamSA. Furthermore, real-world tests also confirm practicality for industrial UAM approaching tasks with high efficiency and robustness.","tags":["Unmanned aerial manipulator","UAM approaching","Visual tracking"],"title":"Scale-Aware Siamese Object Tracking for Vision-Based UAM Approaching","type":"publication"},{"authors":["Bowen Li","Ziyuan Huang","Junjie Ye","Yiming Li","Sebastian Scherer","Hang Zhao","Changhong Fu"],"categories":null,"content":"  \n","date":1668902400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668902400,"objectID":"af39454b029c1db87c23821547d22fdf","permalink":"https://vision4robotics.github.io/publication/2022_arxiv_pvt++/","publishdate":"2022-11-20T00:00:00Z","relpermalink":"/publication/2022_arxiv_pvt++/","section":"publication","summary":"Visual object tracking is an essential capability of intelligent robots. Most existing approaches have ignored the online latency that can cause severe performance degradation during real-world processing. Especially for unmanned aerial vehicle, where robust tracking is more challenging and onboard computation is limited, latency issue could be fatal. In this work, we present a simple framework for end-to-end latency-aware tracking, i.e., end-to-end predictive visual tracking (PVT++). PVT++ is capable of turning most leading-edge trackers into predictive trackers by appending an online predictor. Unlike existing solutions that use model-based approaches, our framework is learnable, such that it can take not only motion information as input but it can also take advantage of visual cues or a combination of both. Moreover, since PVT++ is end-to-end optimizable, it can further boost the latency-aware tracking performance by joint training. Additionally, this work presents an extended latency-aware evaluation benchmark for assessing an any-speed tracker in the online setting. Empirical results on robotic platform from aerial perspective show that PVT++ can achieve up to 60% performance gain on various trackers and exhibit better robustness than prior modelbased solution, largely mitigating the degradation brought by latency. Code and models will be made public.","tags":["Unmanned aerial vehicle","Visual object tracking","Latency-aware tracking"],"title":"PVT++: A Simple End-to-End Latency-Aware Visual Tracking Framework","type":"publication"},{"authors":["Haobo Zuo","Changhong Fu","Sihang Li","Junjie Ye","Guangze Zheng"],"categories":null,"content":" Overview of the proposed DeconNet. \n","date":1665619200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665619200,"objectID":"55d94224963f968019c289fda46116f7","permalink":"https://vision4robotics.github.io/publication/2022_tgrs_deconnet/","publishdate":"2022-10-13T00:00:00Z","relpermalink":"/publication/2022_tgrs_deconnet/","section":"publication","summary":"Vision-based aerial tracking has proven enormous potential in the field of remote sensing recently. However, challenges such as occlusion, fast motion, and illumination variation remain crucial issues for realistic aerial tracking applications. These challenges, frequently occurring from the aerial perspectives, can easily cause object feature pollution. With the contaminated object features, the credibility of trackers is prone to be substantially degraded. To address this issue, this work proposes a novel end-to-end decontaminated network, i.e., DeconNet, to alleviate object feature pollution efficiently and effectively. DeconNet mainly consists of downsampling and upsampling phases. Specifically, the decontaminated downsampling network first decreases the polluted object information with two convolution branches, enhancing the object location information. Subsequently, the decontaminated upsampling network applies the super-resolution technology to restore the object scale and shape information, with the low-to-high encoder for further decontamination. Additionally, the pooling distance loss function is carefully designed to improve the decontamination effect of the decontaminated downsampling network. Comprehensive evaluations on four well-known aerial tracking benchmarks validate the effectiveness of DeconNet. Especially, the proposed tracker has superior performance on the sequences with feature pollution. Besides, real-world tests on an aerial platform have proven the efficiency of DeconNet with 30.6 frames per second.","tags":["Vision-based aerial tracking","End-to-end decontaminated network","Downsampling-upsampling strategy","Low-to-high encoder","Pooling distance loss"],"title":"DeconNet: End-to-End Decontaminated Network for Vision-Based Aerial Tracking","type":"publication"},{"authors":["Haobo Zuo","Changhong Fu","Sihang Li","Junjie Ye","Guangze Zheng"],"categories":null,"content":" Overview of the proposed FDNT tracker.\n","date":1656547200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656547200,"objectID":"a62041a8ded3196945d91992fec0f9ee","permalink":"https://vision4robotics.github.io/publication/2022_iros_fdnt/","publishdate":"2022-06-30T00:00:00Z","relpermalink":"/publication/2022_iros_fdnt/","section":"publication","summary":"Object feature pollution is one of the burning issues in vision-based UAV tracking, commonly caused by occlusion, fast motion, and illumination variation. Due to the contaminated information in the polluted object features, most trackers fail to precisely estimate the object location and scale. To address the above disturbing issue, this work proposes a novel end-to-end feature decontaminated network for efficient and effective UAV tracking, i.e., FDNT. FDNT mainly includes two modules, a decontaminated downsampling network and a decontaminated upsampling network. The former reduces the interference information of the feature pollution and enhanced the expression of the object location information with two asymmetric convolution branches. The latter restores the object scale information with the super-resolution technology-based low-to-high encoder, achieving a further decontamination effect. Moreover, a novel pooling distance loss is carefully developed to assist the decontaminated downsampling network in concentrating on the critical regions with the object information. Exhaustive experiments on three well-known benchmarks validate the effectiveness of FDNT, especially on the sequences with feature pollution. In addition, real-world tests show the efficiency of FDNT with 31.4 frames per second. The code and demo videos are available at https://github.com/vision4robotics/FDNT.","tags":["Feature Decontamination","UAV tracking"],"title":"End-to-End Feature Decontaminated Network for UAV Tracking","type":"publication"},{"authors":["Changhong Fu","Haolin Dong","Junjie Ye","Guangze Zheng","Sihang Li","Jilin Zhao"],"categories":null,"content":" Overview of our HighlightNet pipeline.\n","date":1656547200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656547200,"objectID":"eef60c6bb8fef1ccbd07cdb4c063a45c","permalink":"https://vision4robotics.github.io/publication/2022_iros_highlightnet/","publishdate":"2022-06-30T00:00:00Z","relpermalink":"/publication/2022_iros_highlightnet/","section":"publication","summary":"Low-light environments have posed a formidable challenge for robust UAV tracking even with state-of-the-art trackers since the potential image features are hard to extract under adverse light conditions. Besides, due to the low visibility, accurate online selection of the object also becomes extremely difficult for human monitors to initialize UAV tracking in ground control stations (GCSs). To address these problems, this work proposed a novel enhancer, i.e., HighlightNet, to light up potential objects for both human operators and UAV trackers. By employing Transformer, HighlightNet can adjust enhancement parameters according to global features and is thus adaptive for illumination variation. Pixel-level range mask is introduced to make HighlightNet more focused on the enhancement of the tracking object and regions without light sources. Furthermore, a soft truncation mechanism is built to prevent background noise from being mistaken for crucial features. Experiments on image enhancement benchmarks demonstrate HighlightNet has advantages in facilitating human perception. Evaluations on the public UAVDark135 benchmark show that HightlightNet is more suitable for UAV tracking tasks than other top-ranked low-light enhancers. In addition, with real-world tests on a typical UAV platform, HighlightNet verifies its practicability and efficiency in nighttime aerial tracking-related applications. The code and demo videos are available at https://github.com/vision4robotics/HighlightNet.","tags":["Nighttime UAV tracking","Low-light enhancement"],"title":"HighlightNet: Highlighting Low-Light Potential Features for Real-Time UAV Tracking","type":"publication"},{"authors":["Changhong Fu","Weiyu Peng","Sihang Li","Junjie Ye","Ziang Cao"],"categories":null,"content":" The proposed framework.\n","date":1656547200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656547200,"objectID":"71a6432aeaf6f6ee4a6a6346797058e0","permalink":"https://vision4robotics.github.io/publication/2022_iros_lpat/","publishdate":"2022-06-30T00:00:00Z","relpermalink":"/publication/2022_iros_lpat/","section":"publication","summary":"Visual object tracking has been utilized in numerous aerial platforms, where is facing the challenges of more extremely complex conditions. To address the inefficient long-range modeling of traditional networks with fully convolutional neural networks, Transformer is introduced into the state-of-the-art trackers’ frameworks. Benefiting from full receptive field of global attention, these Transformer trackers can efficiently model long-range information. However, the structure of vanilla Transformer is lack of enough inductive bias and directly adopting global attention will lead to overfocusing on global information which does harm to modeling local details. This work proposes a local perception-aware Transformer for aerial tracking, i.e., LPAT. Specifically, this novel tracker is constructed with modified local-recognition attention and local element correction network to process information via local-modeling to global-search mechanism. To grab local details and strengthen the local inductive bias of Transformer structure. The Transformer encoder with local-recognition attention is constructed to fuse local features for accurate feature modeling and the local element correction network can strengthen the capability of both Transformer encoder and decoder to distinguish local details. The proposed method achieves competitive accuracy and robustness in several benchmarks with 316 sequences in total. The proposed tracker’s practicability and efficiency have been validated by the real-world tests on a typical aerial platform. The code is available at https://github.com/vision4robotics/LPAT.","tags":["Aerial object tracking","Local perception-aware Transformer"],"title":"Local Perception-Aware Transformer for Aerial Tracking","type":"publication"},{"authors":["Guangze Zheng","Changhong Fu","Junjie Ye","Bowen Li","Geng Lu","Jia Pan"],"categories":null,"content":" An overview of the proposed Siamese tracking with pairwise scale-channel attention (SiamPSA) for UAM approaching.\n","date":1656547200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656547200,"objectID":"778857e5a8a94eb84cf3c8f49d6491fc","permalink":"https://vision4robotics.github.io/publication/2022_iros_siamsa/","publishdate":"2022-06-30T00:00:00Z","relpermalink":"/publication/2022_iros_siamsa/","section":"publication","summary":"Although the manipulating of the unmanned aerial manipulator (UAM) has been widely studied, vision-based UAM approaching, which is crucial to the subsequent manipulating, generally lacks effective design. The key to the visual UAM approaching lies in object tracking, while current UAM tracking typically relies on costly model-based methods. Besides, UAM approaching often confronts more severe object scale variation issues, which makes it inappropriate to directly employ state-of-the-art model-free Siamese-based methods from the object tracking field. To address the above problems, this work proposes a novel Siamese network with pairwise scale-channel attention (SiamSA) for vision-based UAM approaching. Specifically, SiamSA consists of a pairwise scale-channel attention network (PSAN) and a scale-aware anchor proposal network (SA-APN). PSAN acquires valuable scale information for feature processing, while SA-APN mainly attaches scale awareness to anchor proposing. Moreover, a new tracking benchmark for UAM approaching, namely UAMT100, is recorded with 35K frames on a flying UAM platform for evaluation. Exhaustive experiments on the benchmarks and real-world tests validate the efficiency and practicality of SiamSA with a promising speed. Both the code and UAMT100 benchmark are now available at https://github.com/vision4robotics/SiamSA.","tags":["Unmanned aerial manipulator","UAM approaching","Visual tracking"],"title":"Siamese Object Tracking for Vision-Based UAM Approaching with Pairwise Scale-Channel Attention","type":"publication"},{"authors":["Changhong Fu","Kunhui Chen","Kunhan Lu","Guangze Zheng","Jilin Zhao"],"categories":null,"content":" Lightweight aviation fastener rotation detection method. \n","date":1652313600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652313600,"objectID":"a564582c32c9f853543a04e90198936a","permalink":"https://vision4robotics.github.io/publication/2022_jao_kunhui/","publishdate":"2022-05-12T00:00:00Z","relpermalink":"/publication/2022_jao_kunhui/","section":"publication","summary":"Aiming at the problems of low efficiency, high cost and poor accuracy in existing methods in aviation fastener sorting process, a rotation target detection method for intelligent optical perception with edge computing was proposed. To further improve the performance of the target detection model, a feature fusion mechanism based on enhanced semantics and optimized space was constructed. A type of dilated ghost module to lower the parameter quantity of the feature fusion network was designed, and enable the edge computing deployment in industrial scenes. Using the Gaussian-like circular smooth label method, the rotation target detection was realized on the prediction branch of the model detection layer, which significantly enhanced model detection performance and was more favorable for automated grasping of industrial robots. The detection accuracy on the authoritative public rotation dataset reached 77.16%. Finally, the proposed detection method was implemented in an embedded intelligent device. The edge computing deployment shows that the total accuracy reaches 99.76%, and the inference speed is more than 20 frames per second (FPS), which is sufficient for industrial applications.","tags":["Aviation fastener","Rotation target detection","Feature fusion","Dilated ghost module","Edge computing deployment"],"title":"Aviation Fastener Rotation Detection for Intelligent Optical Perception with Edge Computing","type":"publication"},{"authors":["Changhong Fu","Kunhan Lu","Guangze Zheng","Junjie Ye","Ziang Cao","Bowen Li","Geng Lu"],"categories":null,"content":" The category of leading-edge Siamese trackers over the years. \n","date":1652313600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652313600,"objectID":"fdcf692b908bd5fd4ae3faeb65148b2d","permalink":"https://vision4robotics.github.io/publication/2022_grsm_siam/","publishdate":"2022-05-12T00:00:00Z","relpermalink":"/publication/2022_grsm_siam/","section":"publication","summary":"Unmanned aerial vehicle (UAV)-based visual object tracking has enabled a wide range of applications and attracted increasing attention in the field of remote sensing because of its versatility and effectiveness. As a new force in the revolutionary trend of deep learning, Siamese networks shine in visual object tracking with their promising balance of accuracy, robustness, and speed. Thanks to the development of embedded processors and the gradual optimization of deep neural networks, Siamese trackers receive extensive research and realize preliminary combinations with UAVs. However, due to the UAV's limited onboard computational resources and the complex real-world circumstances, aerial tracking with Siamese networks still faces severe obstacles in many aspects. To further explore the deployment of Siamese networks in UAV tracking, this work presents a comprehensive review of leading-edge Siamese trackers, along with an exhaustive UAV-specific analysis based on the evaluation using a typical UAV onboard processor. Then, the onboard tests are conducted to validate the feasibility and efficacy of representative Siamese trackers in real-world UAV deployment. Furthermore, to better promote the development of the tracking community, this work analyzes the limitations of existing Siamese trackers and conducts additional experiments represented by low-illumination evaluations. In the end, prospects for the development of Siamese UAV tracking in the remote sensing field are discussed. The unified framework of leading-edge Siamese trackers, i.e., code library, and the results of their experimental evaluations are available at https://github.com/vision4robotics/SiameseTracking4UAV.","tags":["Unmanned aerial vehicle (UAV)","Vision-based aerial object tracking","Siamese networks","Review \u0026 comprehensive analysis"],"title":"Siamese Object Tracking for Unmanned Aerial Vehicle: A Review and Comprehensive Analysis","type":"publication"},{"authors":["Bowen Li","Changhong Fu","Fangqiang Ding","Junjie Ye","Fuling Lin"],"categories":null,"content":" Pipeline of ADTrack. \n","date":1647907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647907200,"objectID":"f4f390c57691f15cf24a00c711c64ce9","permalink":"https://vision4robotics.github.io/publication/2022_tmc_adtrack/","publishdate":"2022-03-22T00:00:00Z","relpermalink":"/publication/2022_tmc_adtrack/","section":"publication","summary":"Unmanned aerial vehicle (UAV) has facilitated a wide range of real-world applications and attracted extensive research in the mobile computing field. Specially, developing real-time robust visual onboard trackers for all-day aerial maneuver can remarkably broaden the scope of intelligent deployment of UAV. However, prior tracking methods have merely focused on robust tracking in the well-illuminated scenes, while ignoring trackers’ capabilities to be deployed in the dark. In darkness, the conditions can be more complex and harsh, easily posing inferior robust tracking or even tracking failure. To this end, this work proposes a novel discriminative correlation filter-based tracker with illumination adaptive and anti-dark capability, namely ADTrack. ADTrack firstly exploits image illuminance information to enable adaptability of the model to the given light condition. Then, by virtue of an efficient enhancer, ADTrack carries out image pretreatment where a target aware mask is generated. Benefiting from the mask, ADTrack aims to solve a novel dual regression problem where dual filters are online trained with mutual constraint. Besides, this work also constructs a UAV nighttime tracking benchmark UAVDark135. Exhaustive experiments on authoritative benchmarks and onboard tests are implemented to validate the superiority and robustness of ADTrack in all-day conditions.","tags":["Unmanned aerial vehicle","Visual object tracking","Discriminative correlation filter","Dark tracking benchmark","Image illumination based mask","Dual regression model"],"title":"All-Day Object Tracking for Unmanned Aerial Vehicle","type":"publication"},{"authors":null,"categories":null,"content":"","date":1647734400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647734400,"objectID":"5e954174e24cba20a80a9ff290aa9775","permalink":"https://vision4robotics.github.io/project/nat2021/","publishdate":"2022-03-20T00:00:00Z","relpermalink":"/project/nat2021/","section":"project","summary":"NAT2021 is a pioneering benchmark designed for unsupervised domain adaptive nighttime tracking.","tags":["UAV dark tracking","Tracking benchmark"],"title":"NAT2021","type":"project"},{"authors":["Ziang Cao","Ziyuan Huang","Liang Pan","Shiwei Zhang","Ziwei Liu","Changhong Fu"],"categories":null,"content":"\nOverview of our framework..\n\n","date":1646179200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646179200,"objectID":"bb83183235aff3a5a30a460c6b577011","permalink":"https://vision4robotics.github.io/publication/2022_cvpr_tctrack/","publishdate":"2022-03-01T00:00:00Z","relpermalink":"/publication/2022_cvpr_tctrack/","section":"publication","summary":"Temporal contexts among consecutive frames are far from been fully utilized in existing visual trackers. In this work, we present TCTrack1, a comprehensive framework to fully exploit temporal contexts for aerial tracking. The temporal contexts are incorporated at two levels： the extraction of features and the refinement of similarity maps. Specifically, for feature extraction, an online temporally adaptive convolution is proposed to enhance the spatial features using temporal information, which is achieved by dynamically calibrating the convolution weights according to the previous frames. For similarity map refinement, we propose an adaptive temporal transformer, which first effectively encodes temporal knowledge in a memory-efficient way, before the temporal knowledge is decoded for accurate adjustment of the similarity map. TCTrack is effective and efficient： evaluation on four aerial tracking benchmarks shows its impressive performance; real-world UAV tests show its high speed of over 27 FPS on NVIDIA Jetson AGX Xavier.","tags":["Siamese network","Aerial object tracking","Temporal Contexts","Transformer"],"title":"TCTrack: Temporal Contexts for Aerial Tracking","type":"publication"},{"authors":["Junjie Ye","Changhong Fu","Guangze Zheng","Danda Pani Paudel","Guang Chen"],"categories":null,"content":"\n Illustration of the proposed unsupervised domain adaptation framework for nighttime aerial tracking.\n\n","date":1646179200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646179200,"objectID":"10d2a58c9f73c59e61d4a01ed1b791b3","permalink":"https://vision4robotics.github.io/publication/2022_cvpr_udat/","publishdate":"2022-03-01T00:00:00Z","relpermalink":"/publication/2022_cvpr_udat/","section":"publication","summary":"Previous advances in object tracking mostly reported on favorable illumination circumstances while neglecting performance at nighttime, which significantly impeded the development of related aerial robot applications. This work instead develops a novel unsupervised domain adaptation framework for nighttime aerial tracking (named UDAT). Specifically, a unique object discovery approach is provided to generate training patches from raw nighttime tracking videos. To tackle the domain discrepancy, we employ a Transformer-based bridging layer post to the feature extractor to align image features from both domains. With a Transformer day/night feature discriminator, the daytime tracking model is adversarially trained to track at night. Moreover, we construct a pioneering benchmark namely NAT2021 for unsupervised domain adaptive nighttime tracking, which comprises a test set of 180 manually annotated tracking sequences and a train set of over 276k unlabelled nighttime tracking frames. Exhaustive experiments demonstrate the robustness and domain adaptability of the proposed framework in nighttime aerial tracking. The code and benchmark are available at https://github.com/vision4robotics/UDAT.","tags":["Unsupervised domain adaptation","Nighttime aerial tracking","Benchmark","Transformer"],"title":"Unsupervised Domain Adaptation for Nighttime Aerial Tracking","type":"publication"},{"authors":["Changhong Fu","Sihang Li","Xinnan Yuan","Junjie Ye","Ziang Cao","Fangqiang Ding"],"categories":null,"content":"\nOverview of our Ad2Attack pipeline.\n\n","date":1643702400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643702400,"objectID":"4c886f02ab35d2f45fd991574daea621","permalink":"https://vision4robotics.github.io/publication/2022_icra_ad2attack/","publishdate":"2022-01-31T08:00:00Z","relpermalink":"/publication/2022_icra_ad2attack/","section":"publication","summary":"Visual tracking is adopted to extensive unmanned aerial vehicle (UAV)-related applications, which leads to a highly demanding requirement on the robustness of UAV trackers. However, adding imperceptible perturbations can easily fool the tracker and cause tracking failures. This risk is often overlooked and rarely researched at present. Therefore, to help increase awareness of the potential risk and the robustness of UAV tracking, this work proposes a novel adaptive adversarial attack approach, i.e., Ad2Attack, against UAV object tracking. Specifically, adversarial examples are generated online during the resampling of the search patch image, which leads trackers to lose the target in the following frames. Ad2Attack is composed of a direct downsampling module and a super-resolution upsampling module with adaptive stages. A novel optimization function is proposed for balancing the imperceptibility and efficiency of the attack. Comprehensive experiments on several well-known benchmarks and real-world conditions show the effectiveness of our attack method, which dramatically reduces the performance of the most advanced Siamese trackers.","tags":["Visual tracking","Unmanned aerial vehicles","Adversarial attack"],"title":"Ad2Attack: Adaptive Adversarial Attack on Real-Time UAV Tracking","type":"publication"},{"authors":["Junjie Ye","Changhong Fu","Ziang Cao","Shan An","Guangze Zheng","Bowen Li"],"categories":null,"content":" Overall performance of SOTA trackers with the proposed SCT enabled (markers in a dark color) or not (markers in a light color) in the newly constructed nighttime UAV tracking benchmark\u0026mdash;DarkTrack2021. SCT significantly boosts the nighttime tracking performance of trackers in a plug-and-play manner. \n","date":1641168000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641168000,"objectID":"e9308ae6e15c9f7ab24895829b29945f","permalink":"https://vision4robotics.github.io/publication/2022_ral_sct/","publishdate":"2022-01-03T00:00:00Z","relpermalink":"/publication/2022_ral_sct/","section":"publication","summary":"Most previous progress in object tracking is realized in daytime scenes with favorable illumination. State-of-the-arts can hardly carry on their superiority at night so far, thereby considerably blocking the broadening of visual tracking-related unmanned aerial vehicle (UAV) applications. To realize reliable UAV tracking at night, a spatial-channel Transformer-based low-light enhancer (namely SCT), which is trained in a novel task-inspired manner, is proposed and plugged prior to tracking approaches. To achieve semantic-level low-light enhancement targeting the high-level task, the novel spatial-channel attention module is proposed to model global information while preserving local context. In the enhancement process, SCT denoises and illuminates nighttime images simultaneously through a robust non-linear curve projection. Moreover, to provide a comprehensive evaluation, we construct a challenging nighttime tracking benchmark, namely DarkTrack2021, which contains 110 challenging sequences with over 100K frames in total. Evaluations on both the public UAVDark135 benchmark and the newly constructed DarkTrack2021 benchmark show that the task-inspired design enables SCT with significant performance gains for nighttime UAV tracking compared with other top-ranked low-light enhancers. Real-world tests on a typical UAV platform further verify the practicability of the proposed approach. The DarkTrack2021 benchmark and the code of the proposed approach are publicly available at https://github.com/vision4robotics/SCT.","tags":["Unmanned aerial vehicle","Nighttime tracking","Low-light enhancement","Transformer"],"title":"Tracker Meets Night: A Transformer Enhancer for UAV Tracking","type":"publication"},{"authors":["Ran Duan","Danda Pani Paudel","Changhong Fu","Peng Lu"],"categories":null,"content":" Comparison of traditional method and the proposed method. \n","date":1641081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641081600,"objectID":"2140233833c722a3b40c70172fcfbbc7","permalink":"https://vision4robotics.github.io/publication/2021_tmech_odometry/","publishdate":"2022-01-02T00:00:00Z","relpermalink":"/publication/2021_tmech_odometry/","section":"publication","summary":"This paper presents a novel outlier rejection approach for feature-based visual odometry. The proposed approach is based on an empirical observation that shows that some 2D-3D correspondences with very low reprojection error can cause a high error in pose estimation. This work exploits such observations for odometry when a stereo camera is available. We argue that an explicit pose error measure is desired over that of implicit reprojection – whenever the former is possible – to classify correspondences into inliers vs. outliers for robust long-term odometry. To explicitly measure the plausible pose error, we derive bounds on the individual pose parameters with the help of the known orientation of stereo cameras. In this process, we formulate our bounds using the sum-of-square polynomials, which allow us to test whether a given correspondence satisfies any solution within the expected bounds. If the correspondence does not satisfy bounds for any parameter, it is considered to be an outlier. We implemented and tested the proposed method for the unmanned aerial vehicle (UAV) indoor navigation. The experiments from both benchmark evaluations (EuRoC and KITTI) and UAV onboard tests indicate that the inlier group refined by the proposed method significantly improves odometry estimation compared to the traditional outlier rejection methods. In fact, the proposed method performs as accurately as inertial measurement unit (IMU) aided methods in the state of the art. ROS project page (open-source code with demo video) is available at https://github.com/arclab-hku/SOPVO.","tags":["Stereo visual odometry","sum-of-square theory","UAV indoor navigation"],"title":"Stereo Orientation Prior for UAV Robust and Accurate Visual Odometry","type":"publication"},{"authors":null,"categories":null,"content":"","date":1632700800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632700800,"objectID":"a3ca2af2b6970d3faedb710a28839ba4","permalink":"https://vision4robotics.github.io/project/darktrack2021/","publishdate":"2021-09-27T00:00:00Z","relpermalink":"/project/darktrack2021/","section":"project","summary":"DarkTrack2021 is a nighttime tracking benchmark comprises 110 challenging sequences with 100K frames in total.","tags":["UAV dark tracking","Tracking benchmark"],"title":"DarkTrack2021","type":"project"},{"authors":null,"categories":null,"content":"","date":1632700800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632700800,"objectID":"7c217d7a0d9af29d165dd79e76d663fe","permalink":"https://vision4robotics.github.io/project/uamt100/","publishdate":"2021-09-27T00:00:00Z","relpermalink":"/project/uamt100/","section":"project","summary":"UAM tracking benchmark is built for UAM tracking method evaluation. It contains 100 image sequences recorded on a flying UAM platform.","tags":["UAM tracking","Tracking benchmark"],"title":"UAM Tracking Benchmark","type":"project"},{"authors":["Changhong Fu","Jin Jin","Fangqiang Ding","Yiming Li","Geng Lu"],"categories":null,"content":" The main workflow of the SRECF tracker. \n","date":1632614400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632614400,"objectID":"b702659de440c31f69171b9c4255e5bc","permalink":"https://vision4robotics.github.io/publication/2021_tmm_srecf-tracker/","publishdate":"2021-09-26T00:00:00Z","relpermalink":"/publication/2021_tmm_srecf-tracker/","section":"publication","summary":"Traditional discriminative correlation filter (DCF) has received great popularity due to its high computational efficiency. However, the lightweight framework of DCF cannot promise robust performance when the tracker faces appearance variations within the background. These unpredictable appearance variations always distract the filter. Most existing DCF-based trackers either utilize deep convolutional features or incorporate additional constraints to elevate tracking robustness. Despite some improvements, both of them hamper the tracking speed and can only roughly alleviate the distractions of appearance variations. In this paper, a novel spatial reliability enhanced learning strategy is proposed to handle the problems aforementioned. By monitoring the variation of response produced in detection phase, a dynamic reliability map is generated to indicate the reliability of each background subregion. Then, label adjustment is conducted to repress the distractions of these unreliable areas. Compared with the conventional way of constraint where a new term is always added to realize the desired goal, label adjustment is simultaneously more efficient and effective. Moreover, to promise the accuracy and dependability of the reliability map, an adaptively updated response pool recording reliable historical response values is proposed. Extensive and exhaustive experiments on three challenging unmanned aerial vehicle (UAV) benchmarks, i.e., UAV123@10fps, DTB70 and UAVDT, which totally include 243 video sequences, validate the superiority of the proposed method against other state-of-the-art trackers and exhibit a remarkable generality in a variety of scenarios. Meanwhile, the tracking speed of 65.2 FPS on a cheap CPU makes it suitable for real-time UAV applications.","tags":["Visual tracking","Unmanned aerial vehicles","Correlation filters"],"title":"Spatial Reliability Enhanced Correlation Filter: An Efficient Approach for Real-Time UAV Tracking","type":"publication"},{"authors":["Ziang Cao","Changhong Fu","Junjie Ye","Bowen Li","Yiming Li"],"categories":null,"content":" Fig. 1 Overview of the HiFT tracker. Fig. 2 Framework of the hierarchical feature transformer. \n","date":1626912000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626912000,"objectID":"c9ab8c003b230e357d009edabed830f2","permalink":"https://vision4robotics.github.io/publication/2021_iccv_hift/","publishdate":"2019-07-22T00:00:00Z","relpermalink":"/publication/2021_iccv_hift/","section":"publication","summary":"Siamese-based visual tracking methods generally execute the classification and regression of the target object based on the similarity maps. However, existing works either solely employ a single map generated by the last convolutional layer which degrades the localization accuracy, or separately use multiple maps for decision making, introducing intractable computations for aerial mobile platforms. In this work, we propose an efficient and effective hierarchical feature transformer (HiFT) in Siamese tracking. Hierarchical similarity maps generated by multi-level convolutional layers are fed into a feature transformer network. Not only the global contextual information can be raised, facilitating the target search, but also our end-to-end architecture with the transformer can learn the inter-dependencies among multi-level features, and discover a tracking-tailored feature space with strong discriminability due to the interactive fusion of spatial (early layers) and semantics cues (deep layers). Comprehensive evaluations on aerial benchmarks have proven the effectiveness of HiFT, and the real-world tests on the aerial platform have validated its practicability and robustness with a real-time speed.","tags":["Siamese network","Real-time object tracking","Unmanned aerial vehicles","Transformer"],"title":"HiFT: Hierarchical Feature Transformer for Aerial Tracking","type":"publication"},{"authors":["Junjie Ye","Changhong Fu","Guangze Zheng","Ziang Cao","Bowen Li"],"categories":null,"content":" Tracking performance comparison in a typical dark scene with the proposed DarkLighter module activated (in red) or not (in pink).\n","date":1625011200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625011200,"objectID":"c98ba9d164d6dc005d7f59a9f3dff2f1","permalink":"https://vision4robotics.github.io/publication/2021_iros_darklighter/","publishdate":"2021-06-30T00:00:00Z","relpermalink":"/publication/2021_iros_darklighter/","section":"publication","summary":"Recent years have witnessed the fast evolution and promising performance of the convolutional neural network (CNN)-based trackers, which aim at imitating biological visual systems. However, current CNN-based trackers can hardly generalize well to low-light scenes that are commonly lacked in the existing training set. In indistinguishable night scenarios frequently encountered in unmanned aerial vehicle (UAV) tracking-based applications, the robustness of the state-of-the-art (SOTA) trackers drops significantly. To facilitate aerial tracking in the dark through a general fashion, this work proposes a low-light image enhancer namely DarkLighter, which dedicates to alleviate the impact of poor illumination and noise iteratively. A lightweight map estimation network, \\textit{i.e.}, ME-Net, is trained to efficiently estimate illumination maps and noise maps jointly. Experiments are conducted with several SOTA trackers on numerous UAV dark tracking scenes. Exhaustive evaluations demonstrate the reliability and universality of DarkLighter, with high efficiency. Moreover, DarkLighter has further been implemented on a typical UAV system. Real-world tests at night scenes have verified its practicability and dependability.","tags":["Low-light enhancement","Visual tracking","Unmanned aerial vehicles"],"title":"DarkLighter: Light Up the Darkness for UAV Tracking","type":"publication"},{"authors":["Fuling Lin","Changhong Fu","Yujie He","Weijiang Xiong","Fan Li"],"categories":null,"content":" Main difference between the proposed ReCF and SRDCF. \n","date":1625011200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625011200,"objectID":"2b243256a5532c4b7695ddfee9169404","permalink":"https://vision4robotics.github.io/publication/2021_tits_recf_tracker/","publishdate":"2021-06-29T00:00:00Z","relpermalink":"/publication/2021_tits_recf_tracker/","section":"publication","summary":"Object tracking is a fundamental task for the visual perception system on the intelligent unmanned aerial vehicle (UAV). The high efficiency of correlation filter (CF) based trackers has advanced the widespread development of online UAV object tracking. This kind of method can effectively train a filter to discriminate the target from the background. However, most CF-based methods require a fixed label function over all the previous samples, leading to over-fitting and filter degradation, especially in complex drone scenarios. To address this problem, a novel adaptive response reasoning approach is proposed for CF learning. It can leverage temporal information in filter training and significantly promote the robustness of the tracker. Specifically, the proposed response reasoning method goes beyond the standard response consistency requirement and constructs an auxiliary label of the current sample. Besides, it helps learn a generic relationship between the previous and current filters, thereby realizing self-regulated filter updating and enhancing the discriminability of the filter. Extensive experiments on four wellknown challenging UAV tracking benchmarks with 278 videos sequences show that the presented method yields superior results to 40 state-of-the-art trackers with real-time performance on a single CPU, which is suitable for UAV online tracking missions.","tags":["Real-time UAV tracking","Discriminative correlation filter","Adaptive response reasoning"],"title":"ReCF: Exploiting Response Reasoning for Correlation Filters in Real-Time UAV Tracking","type":"publication"},{"authors":["Ziang Cao","Changhong Fu","Junjie Ye","Bowen Li","Yiming Li"],"categories":null,"content":" The overview of the SiamAPN++ tracker.\n","date":1625011200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625011200,"objectID":"0c32adec2cf596405b8de13633f93db3","permalink":"https://vision4robotics.github.io/publication/2021_iros_siamapn++/","publishdate":"2021-06-30T00:00:00Z","relpermalink":"/publication/2021_iros_siamapn++/","section":"publication","summary":"Recently, the Siamese-based method has stood out from multitudinous tracking methods owing to its state-of-the-art (SOTA) performance. Nevertheless, due to various special challenges in UAV tracking, e.g., severe occlusion, and fast motion, most existing Siamese-based trackers hardly combine superior performance with high efficiency. To this concern, in this paper, a novel attentional Siamese tracker (SiamAPN++) is proposed for real-time UAV tracking. By virtue of the attention mechanism, the attentional aggregation network (AAN) is conducted with self-AAN and cross-AAN, raising the expression ability of features eventually. The former AAN aggregates and models the self-semantic interdependencies of the single feature map via spatial and channel dimensions. The latter aims to aggregate the cross-interdependencies of different semantic features including the location information of anchors. In addition, the dual features version of the anchor proposal network is proposed to raise the robustness of proposing anchors, increasing the perception ability to objects with various scales. Experiments on two well-known authoritative benchmarks are conducted, where SiamAPN++ outperforms its baseline SiamAPN and other SOTA trackers. Besides, real-world tests onboard a typical embedded platform demonstrate that SiamAPN++ achieves promising tracking results with real-time speed.","tags":["Siamese network","Visual tracking","Unmanned aerial vehicles"],"title":"SiamAPN++: Siamese Attentional Aggregation Network for Real-Time UAV Tracking","type":"publication"},{"authors":["Junjie Ye","Changhong Fu","Fuling Lin","Fangqiang Ding","Shan An","Geng Lu"],"categories":null,"content":" Overall flowchart of the proposed MRCF. \n","date":1621814400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621814400,"objectID":"2dc6df5d1fe73a4908c6864fe515e2ba","permalink":"https://vision4robotics.github.io/publication/2021_tie_mrcf_tracker/","publishdate":"2021-05-24T00:00:00Z","relpermalink":"/publication/2021_tie_mrcf_tracker/","section":"publication","summary":"As a sort of model-free tracking approach, discriminative correlation filter (DCF)-based trackers have shown prominent performance in unmanned aerial vehicle (UAV) tracking. Nevertheless, typical DCFs acquire all samples oriented to filter training merely from the current frame by cyclic shift operation in the spatial domain but ignore the consistency between samples across the timeline. The lack of temporal cues restricts the performance of DCFs under object appearance variations arising from object/UAV motion, scale variations, and viewpoint changes. Besides, many existing methods commonly neglect the channel discrepancy in object position estimation and generally treat all channels equally, thus limiting the further promotion of the tracking discriminability. To these concerns, this work proposes a novel tracking approach based on a multi-regularized correlation filter, i.e., MRCF tracker. By regularizing the deviation of responses and the reliability of channels, the tracker enables smooth response variations and adaptive channel weight distributions simultaneously, leading to favorable adaption to object appearance variations and enhancement of discriminability. Exhaustive experiments on five authoritative UAV-specific benchmarks validate the competitiveness and efficiency of MRCF against top-ranked trackers. Furthermore, we apply our proposed tracker to monocular UAV self-localization under air-ground robot coordination. Evaluations indicate the practicability of the presented method in UAV localization applications.","tags":["Unmanned aerial vehicle (UAV)","Model-free object tracking","Multi-regularized correlation filter","Vision-based UAV self-localization"],"title":"Multi-Regularized Correlation Filter for UAV Tracking and Self-Localization","type":"publication"},{"authors":["Changhong Fu","Ziang Cao","Yiming Li","Junjie Ye","Chen Feng"],"categories":null,"content":" The workflow of SiamAPN. It is composed of four subnetworks and two stages, i.e., feature extraction network, feature fusion network, anchor proposal network, and classification\u0026amp;regression network. Stage-1 includes feature extraction network and anchor proposal network (APN). Stage-2 contains feature fusion network and classification\u0026amp;regression network. \n","date":1620950400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620950400,"objectID":"f33f3154720a3c4a64b5aa1335e2e34f","permalink":"https://vision4robotics.github.io/publication/2021_tgrs_siamapn_ext/","publishdate":"2021-05-14T00:00:00Z","relpermalink":"/publication/2021_tgrs_siamapn_ext/","section":"publication","summary":"Object tracking approaches based on siamese network have demonstrated their huge potential in remote sensing field recently. Nevertheless, due to the limited computing resource of aerial platforms and special challenges in aerial tracking, most existing siamese-based methods can hardly meet the real-time and state-of-the-art performance at the same time. Consequently, a novel siamese-based method is proposed in this work for onboard real-time aerial tracking, i.e., SiamAPN. The proposed method is a no-prior two-stage method, i.e., stage-1 for proposing adaptive anchors to enhance the ability of object perception, stage-2 for fine-tuning the proposed anchors to obtain accurate results. Distinct from pre-defined fixed-sized anchors, our adaptive anchors are adapt automatically to accommodate the tracking object. Besides, the internal information of adaptive anchors is utilized to feedback SiamAPN for enhancing the object perception. Attributing to the feature fusion network, different semantic information is integrated, enriching the information flow. In the end, the regression and multi-classification operation refine the proposed anchors meticulously. Comprehensive evaluations on three well-known benchmarks have proven the superior performance of our approach. Moreover, to verify the practicability of the proposed method, SiamAPN is implemented in an onboard system. Real-world flight tests are conducted on aerial tracking specific scenarios, e.g., low resolution, fast motion, and long-term tracking, the results demonstrate the efficiency and accuracy of our approach, with a processing speed of over 30 frame/s. In addition, the image sequences in the real-world flight tests are collected and annotated as a new benchmark, i.e., UAVTrack112.","tags":["Real-time aerial tracking","Efficient siamese structure","Anchor proposal network","No-prior adaptive anchors","Onboard embedded processing","Real-world flight tests"],"title":"Onboard Real-Time Aerial Tracking with Efficient Siamese Anchor Proposal Network","type":"publication"},{"authors":["Changhong Fu","Bowen Li","Fangqiang Ding","Fuling Lin","Geng Lu"],"categories":null,"content":" General tracking structure of DCF-based methods onboard the UAV platform, which can be divided into the training stage, model update, and detection stage. \n","date":1617321600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617321600,"objectID":"ebb1040cd9d24405fedb8632d04ff414","permalink":"https://vision4robotics.github.io/publication/2021_grsm_cfmg/","publishdate":"2021-04-02T00:00:00Z","relpermalink":"/publication/2021_grsm_cfmg/","section":"publication","summary":"Aerial tracking, which has exhibited its omnipresent dedication and splendid performance, is one of the most active applications in the remote sensing field. Especially, unmanned aerial vehicle (UAV)-based remote sensing system, equipped with a visual tracking approach, has been widely used in aviation, navigation, agriculture, transportation, and public security, etc. As is mentioned above, the UAV-based aerial tracking platform has been gradually developed from research to practical application stage, reaching one of the main aerial remote sensing technologies in the future. However, due to the real-world onerous situations, e.g., harsh external challenges, the vibration of the UAV’s mechanical structure (especially under strong wind conditions), the maneuvering flight in complex environment, and the limited computation resources onboard, accuracy, robustness, and high efficiency are all crucial for the onboard tracking methods. Recently, the discriminative correlation filter (DCF)-based trackers have stood out for their high computational efficiency and appealing robustness on a single CPU, and have flourished in the UAV visual tracking community. In this work, the basic framework of the DCF-based trackers is firstly generalized, based on which, 23 state-of-the-art DCF-based trackers are orderly summarized according to their innovations for solving various issues. Besides, exhaustive and quantitative experiments have been extended on various prevailing UAV tracking benchmarks, i.e., UAV123, UAV123@10fps, UAV20L, UAVDT, DTB70, and VisDrone2019-SOT, which contain 371,903 frames in total. The experiments show the performance, verify the feasibility, and demonstrate the current challenges of DCF-based trackers onboard UAV tracking. Besides, this work also implements the brilliant DCF-based trackers on a typical CPU-based onboard PC to achieve real flight UAV tracking tests to further validate their real-time capabilities and robustness under challenging scenes. A concise summary of future research trends in the area of DCF-based methods for UAV tracking is further provided. Finally, comprehensive conclusions on the directions for future research are presented.","tags":["Real-time remote sensing","Unmanned aerial vehicle","Aerial object tracking","Discriminative correlation filter","Review and experimental evaluation"],"title":"Correlation Filters for Unmanned Aerial Vehicle-Based Aerial Tracking: A Review and Experimental Evaluation","type":"publication"},{"authors":["Bowen Li","Yiming Li","Junjie Ye","Changhong Fu","Hang Zhao"],"categories":null,"content":"  \n","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"8f69f97ecf740fb99d1a9fb280cd8166","permalink":"https://vision4robotics.github.io/publication/2021_arxiv_pvt/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/2021_arxiv_pvt/","section":"publication","summary":"As a crucial robotic perception capability, visual tracking has been intensively studied recently. In the real-world scenarios, the onboard processing time of the image streams inevitably leads to a discrepancy between the tracking results and the real-world states. However, existing visual tracking benchmarks commonly run the trackers offline and ignore such latency in the evaluation. In this work, we aim to deal with a more realistic problem of latency-aware tracking. The state-ofthe-art trackers are evaluated in the aerial scenarios with new metrics jointly assessing the tracking accuracy and efficiency. Moreover, a new predictive visual tracking baseline is developed to compensate for the latency stemming from the onboard computation. Our latency-aware benchmark can provide a more realistic evaluation of the trackers for the robotic applications. Besides, exhaustive experiments have proven the effectiveness of the proposed predictive visual tracking baseline approach. Our code is on https://github.com/vision4robotics/LAE-PVT-master.","tags":["Unmanned aerial vehicle","Visual object tracking","Latency-aware tracking"],"title":"Predictive Visual Tracking: A New Benchmark and Baseline Approach","type":"publication"},{"authors":["Bowen Li","Changhong Fu","Fangqiang Ding","Junjie Ye","Fuling Lin"],"categories":null,"content":" Overall framework of the proposed ADTrack. ADTrack includes 3 stages: pretreatment, training, and detection, which are marked out by boxes in different colors. Dual filters, i.e., context filter and target-focused filter, training and detection follow routes in different colors. It can be seen that the final response shaded noises in context response, which indicates the validity of proposed dual filter.\n\n","date":1614499200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614499200,"objectID":"1433737504ce4cad2e1d5e6586c8e05f","permalink":"https://vision4robotics.github.io/publication/2021_icra_adtrack/","publishdate":"2021-02-28T08:00:00Z","relpermalink":"/publication/2021_icra_adtrack/","section":"publication","summary":"Prior correlation filter (CF)-based tracking methods for unmanned aerial vehicles (UAVs) have virtually focused on tracking in the daytime. However, when the night falls, the trackers will encounter more harsh scenes, which can easily lead to tracking failure. In this regard, this work proposes a novel tracker with anti-dark function (ADTrack). The proposed method integrates an effcient and effective low-light image enhancer into a CF-based tracker. Besides, a target-aware mask is simultaneously generated by virtue of image illumination variation. The target-aware mask can be applied to jointly train a target-focused filter that assists the context filter for robust tracking. Specifically, ADTrack adopts dual regression, where the context filter and the target-focused filter restrict each other for dual filter learning. Exhaustive experiments are conducted on typical dark sceneries benchmark, consisting of 37 typical night sequences from authoritative benchmarks, i.e., UAVDark, and our newly constructed benchmark UAVDark70. The results have shown that ADTrack favorably outperforms other state-of-the-art trackers and achieves a real-time speed of 34 frames/s on a single CPU, thus greatly extending robust UAV tracking to night scenes.","tags":["Visual tracking","Unmanned aerial vehicles","Correlation filter"],"title":"ADTrack: Target-Aware Dual Filter Learning for Real-Time Anti-Dark UAV Tracking","type":"publication"},{"authors":["Guangze Zheng","Changhong Fu","Junjie Ye","Fuling Lin","Fangqiang Ding"],"categories":null,"content":" Tracking procedure of the proposed MSCF tracker. Dashed boxes denote the variables to be solved in the main regression. As MTF in the red box is generated from search region in frame k, it is applied to adjust the altitude value of the cruciform pedestal.\n\n","date":1614499200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614499200,"objectID":"2b51ed03d91fc1ed1d4cc28c2abf7736","permalink":"https://vision4robotics.github.io/publication/2021_icra_mscf_tracker/","publishdate":"2021-02-28T08:00:00Z","relpermalink":"/publication/2021_icra_mscf_tracker/","section":"publication","summary":"Unmanned aerial vehicle (UAV) based visual tracking has been confronted with numerous challenges, e.g., object motion and occlusion. These challenges generally bring about target appearance mutations and cause tracking failure. However, most prevalent discriminative correlation filter (DCF) based trackers are insensitive to target mutations due to a predefined label, which concentrates on merely the centre of the target. Meanwhile, appearance mutations incited by occlusion or similar objects commonly lead to inevitable learning of erroneous information. To cope with appearance mutations, this paper proposes a novel DCF-based method to enhance the sensitivity and resistance to mutations with an adaptive hybrid label, i.e., MSCF. The ideal label is optimized jointly with the correlation filter and remains consistent with the previous label. Meanwhile, a novel measurement of mutations called mutation threat factor (MTF) is applied to correct the label dynamically. Through the revision of label into hybrid shape, MSCF can demonstrate preferable adaptability during appearance mutations. Considerable experiments are conducted on widely used UAV benchmarks. Results manifest the performance of MSCF tracker surpassing other 26 state-ofthe-art DCF-based and deep-based trackers. With a real-time speed of ~ 38 frames/s, the proposed approach is sufficient for UAV tracking commissions.","tags":["Visual tracking","Unmanned aerial vehicles","Correlation filter"],"title":"Mutation Sensitive Correlation Filter for Real-Time UAV Tracking with Adaptive Hybrid Label","type":"publication"},{"authors":["Ran Duan","Changhong Fu","Kostas Alexis","Erdal Kayacan"],"categories":null,"content":"\nThe overview of SiamAPN tracker. It composes of four subnetworks, i.e., feature extraction network, feature fusion network, anchor proposal network (APN), and muti-classification®ression network.\n\n","date":1614499200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614499200,"objectID":"6b17c9d26a327a2e52573f21e997da82","permalink":"https://vision4robotics.github.io/publication/2021_icra_orcf/","publishdate":"2021-02-28T08:00:00Z","relpermalink":"/publication/2021_icra_orcf/","section":"publication","summary":"In this paper, we develop an online learning-based visual tracking framework that can optimize the target model and estimate the scale variation for object tracking. We propose a recommender-based tracker, which is capable of selecting the representative convolutional neural network (CNN) layers and feature maps autonomously. A sub-network is extracted from the pre-trained CNN to optimize the convolutional feature computing. In addition, the proposed recommender computes the weights of these layers and feature maps. A discriminative target percept of each recommended layer is reconstructed by the weighted sum of the recommended feature maps. Then the target model of the correlation filter is updated by the weighted sum of the target percepts. To deal with scale changes, we propose a spatio-temporal-based min-channel method to estimate the target size variation over time. Experimental results on 50 benchmark datasets and video data from rescue drone demonstrate that the proposed tracker is quite competitive with the state-of-the-art CNN-based trackers in terms of accuracy, scale adaptation, and robustness for UAV related application.","tags":["Visual tracking","Unmanned aerial vehicles","Correlation filter"],"title":"Online Recommendation-based Convolutional Features for Scale-Aware Visual Tracking","type":"publication"},{"authors":["Changhong Fu","Ziang Cao","Yiming Li","Junjie Ye","Chen Feng"],"categories":null,"content":"\nThe overview of SiamAPN tracker. It composes of four subnetworks, i.e., feature extraction network, feature fusion network, anchor proposal network (APN), and muti-classification®ression network.\n\n","date":1614499200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614499200,"objectID":"9e61feeea128ae22e4769a11f6af92fb","permalink":"https://vision4robotics.github.io/publication/2021_icra_siamapn/","publishdate":"2021-02-28T08:00:00Z","relpermalink":"/publication/2021_icra_siamapn/","section":"publication","summary":"In the domain of visual tracking, most deep learning-based trackers highlight the accuracy but casting aside efficiency, thereby impeding their real-world deployment on mobile platforms like the unmanned aerial vehicle (UAV). In this work, a novel two-stage siamese network-based method is proposed for aerial tracking, \\textit{i.e.}, stage-1 for high-quality anchor proposal generation, stage-2 for refining the anchor proposal. Different from anchor-based methods with numerous pre-defined fixed-sized anchors, our no-prior method can 1) make tracker robust and general to different objects with various sizes, especially to small, occluded, and fast-moving objects, under complex scenarios in light of the adaptive anchor generation, 2) make calculation feasible due to the substantial decrease of anchor numbers. In addition, compared to anchor-free methods, our framework has better performance owing to refinement at stage-2. Comprehensive experiments on three benchmarks have proven the state-of-the-art performance of our approach, with a speed of ~ 200 frames/s.","tags":["Visual tracking","Unmanned aerial vehicles","Anchor proposal network"],"title":"Siamese Anchor Proposal Network for High-Speed Aerial Tracking","type":"publication"},{"authors":null,"categories":null,"content":"","date":1614384000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614384000,"objectID":"a8f5650c03497fa0a8aab6c57df848b4","permalink":"https://vision4robotics.github.io/project/pvt/","publishdate":"2021-02-27T00:00:00Z","relpermalink":"/project/pvt/","section":"project","summary":"Predictive visual tracking (PVT) is a latency-aware benchmark jointly assessing the tracking accuracy and efficiency of trackers.","tags":["UAV tracking","Latency-aware benchmark"],"title":"PVT","type":"project"},{"authors":["Changhong Fu","Fangqiang Ding","Yiming Li","Jin Jin","Chen Feng"],"categories":null,"content":" The object tracking workflow of DR2Track. \n","date":1607990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607990400,"objectID":"cc005773b1cbcd8344c2bd2cf8744382","permalink":"https://vision4robotics.github.io/publication/2021_eaai_dr2track-tracker/","publishdate":"2020-12-15T00:00:00Z","relpermalink":"/publication/2021_eaai_dr2track-tracker/","section":"publication","summary":"With high efficiency and efficacy, the trackers based on the discriminative correlation filter have experienced rapid development in the field of unmanned aerial vehicle (UAV) over the past decade. In literature, these trackers aim at solving a regression problem in which the circulated samples are mapped into a Gaussian label for online filter training. However, the fixed target label for regression makes trackers lose adaptivity in uncertain tracking scenarios. One of the typical failure cases is that the distractors, e.g., background clutter, camouflage, and similar object, are prone to confuse these trackers. In this work, an efficient approach to instantly monitor the local maximums of the response map for discovering distractors automatically is proposed. In addition, the regression target is accordingly learned, i.e., the location possessing local maximum indicates latent distractor and thus should be repressed by reducing its target response value in filter training. Qualitative and quantitative experiments performed on three challenging well-known benchmarks demonstrate that the presented method not only outperforms the state-of-the-art handcrafted feature-based trackers but also exhibits comparable performance compared to deep learning-based approaches. Specifically, the presented tracker has phenomenal practicability in real-time UAV applications with an average speed of ∼50 frames per second on an affordable CPU.","tags":["Unmanned aerial vehicles","Visual object tracking","Discriminative correlation filter","Learning dynamic regression","Local maximums repression"],"title":"Learning Dynamic Regression with Automatic Distractor Repression for Real-Time UAV Tracking","type":"publication"},{"authors":null,"categories":null,"content":" UAVDark135 is the very first UAV dark tracking benchmark dedicated to providing a comprehensive evaluation of tracking performance at night.\nUAVDark135 consists of 135 sequences, most of which were shot by a standard UAV at night, including more than 125k manually annotated frames. The benchmark covers a wide range of scenes, e.g., road, ocean, street, highway, and lakeside, including a large number of objects, such as person, car, building, athlete, truck, and bike.\nThe benchmark is available here (password: axci).\nUAVDark135 Tracking Benchmark A. Platform and Statistics Standing as the first UAV dark tracking benchmark, the UAVDark135 contains totally 135 sequences captured by a standard UAV2 at night. The benchmark includes various tracking scenes, e.g., crossings, t-junctions, road, highway, and consists of different kinds of tracked objects like people, boat, bus, car, truck, athletes, house, etc. To extent the covered scenes, the benchmark also contains some sequences from YouTube, which were shot on the sea. The total frames, mean frames, maximum frames, and minimum frames of the benchmark are 125466, 929, 4571, and 216 respectively, making it suitable for large-scale evaluation. The videos are captured at a frame-rate of 30 frames/s (FPS), with the resolution of 1920×1080.\nB. Annotation The frames in UAVDark135 are all manually annotated, where a sequence is completely processed by the same annotator to ensure consistency. Since in some dark scenes the object is nearly invisible, annotation process is much more strenuous. After the first round, 5 professional annotators carefully checked the results and made revision for several rounds to reduce errors as much as possible in nearly 2 months.\nSince the boundary contour of the object is not obvious in the dark, the result boxes of the first annotation fluctuates in continuous image frames. However, the actual motion process of the object should be smooth. In these considerations, we record the original annotation every 5 frames for the sequence with extremely severe vibration, and the results of the remaining frames are obtained by linear interpolation, which is closer to the position and scale variation of the real object.\nC. Attributes For more details, please refer to our paper.\n","date":1604102400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604102400,"objectID":"2d77a9e9fdf8af223bcf640d5b739add","permalink":"https://vision4robotics.github.io/project/uavdark135/","publishdate":"2020-10-31T00:00:00Z","relpermalink":"/project/uavdark135/","section":"project","summary":"A pioneering UAV dark tracking benchmark consists of 135 videos with a variety of objects.","tags":["UAV dark tracking","Tracking benchmark"],"title":"UAVDark135","type":"project"},{"authors":["Changhong Fu","Junjie Ye","Juntao Xu","Yujie He","Fuling Lin"],"categories":null,"content":" Tracking procedure of the proposed IBRI tracker in the k-th frame. Historical interval responses are incorporated into the filter training phase after denoising by a novel disruptor-aware scheme based on response bucketing. \n","date":1602028800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602028800,"objectID":"06a2c27d3e614c9491702be79f0c1bcf","permalink":"https://vision4robotics.github.io/publication/2020_tgrs_ibri-tracker/","publishdate":"2020-10-07T00:00:00Z","relpermalink":"/publication/2020_tgrs_ibri-tracker/","section":"publication","summary":"Aerial object tracking approaches based on discriminative correlation filter (DCF) have attracted wide attention in the tracking community due to their impressive progress recently. Many studies introduce temporal regularization into the DCF-based framework to achieve a more robust appearance model and further enhance the tracking performance. However, existing temporal regularization approaches usually utilize the information of two consecutive frames, which are not robust enough due to limited information. Although some methods attempt to incorporate abundant training samples and generally improve the tracking performance, these improvements are at the expense of significantly increased computing consumption. Besides, most existing methods introduce historical information directly without denoising, which means background noises are also introduced into the filter training and may degrade the tracking accuracy. To tackle the drawbacks mentioned above, this work proposes a novel aerial object tracking approach to exploit disruptor-aware interval-based response inconsistency, i.e., IBRI tracker. The proposed method is able to incorporate historical interval information by utilizing responses in the filter training process, thereby obtaining a robust tracking performance while maintaining the real-time speed. Moreover, to reduce the disruptions caused by similar object, partial occlusion, and other challenging scenes, a novel disruptor-aware scheme based on response bucketing is introduced to detect the disruptor and enforce a spatial penalty for the disruptive area around the tracked object. Exhausted experiments on multiple well-known challenging aerial tracking benchmarks demonstrate the accuracy and robustness of the proposed IBRI tracker against other 35 state-of-the-art trackers. With a real-time speed of ~32 frames per second on a single CPU, the proposed approach can be applied for typical aerial platforms to achieve aerial visual object tracking efficiently.","tags":["Aerial object tracking","Discriminative correlation filter (DCF)","Temporal regularization","Historical frame information","Interval-based response inconsistency","Disruptor-aware bucketing"],"title":"Disruptor-Aware Interval-Based Response Inconsistency for Correlation Filters in Real-Time Aerial Tracking","type":"publication"},{"authors":["Fuling Lin","Changhong Fu","Yujie He","Fuyu Guo","Qian Tang"],"categories":null,"content":" A flowchart of the proposed TB-BiCF tracker. \n","date":1599436800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599436800,"objectID":"63a546ae784c6d754ee9ab3741c2dc20","permalink":"https://vision4robotics.github.io/publication/2020_tcsvt_tb-bicf-tracker/","publishdate":"2020-09-07T00:00:00Z","relpermalink":"/publication/2020_tcsvt_tb-bicf-tracker/","section":"publication","summary":"In the field of UAV object tracking, correlation filter based approaches have received lots of attention due to their computational efficiency. The methods learn filters by the ridge regression and generate response maps to distinguish the specified target from the background. An ideal filter can predict the object's position in a new frame, and in turn, can backtrack the object in the past frames. However, the neglect of tracking reversibility in most methods limits the potential of using inter-frame information to improve performance. In this work, a novel bidirectional incongruity-aware correlation filter is presented based on the nature of tracking reversibility. The proposed method incorporates the response-based bidirectional incongruity, which represents the gap between the filters' discriminative difference in the forward and backward tracking perspective caused by object appearance changes. It enables the filter not only to inherit the discriminability from previous filters but also to enhance the generalization capability to unpredictable appearance variations in upcoming frames. Moreover, a temporary block-based strategy is introduced to empower the filter accommodate more drastic object appearance changes and make more effective use of inter-frame information. Comprehensive experiments are conducted on three challenging UAV tracking benchmarks, including UAV123@10fps, DTB70, and UAVDT. Experimental results indicate that the proposed method has superior performance compared with the other 34 state-of-the-art trackers. Our approach permits real-time performance at ∼46.8 FPS on a single CPU and is suitable for UAV online tracking applications.","tags":["Aerial video analysis","Unmanned aerial vehicle","Visual object tracking","Discriminative correlation filter","Temporary block-based bidirectional incongruity"],"title":"Learning Temporary Block-Based Bidirectional Incongruity-Aware Correlation Filters for Efficient UAV Object Tracking","type":"publication"},{"authors":["Yiming Li","Changhong Fu","Fangqiang Ding","Ziyuan Huang","Jia Pan"],"categories":null,"content":" Overall structure of AMCF. \n","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"465de267b6953cf02f6e1d667dfc86a1","permalink":"https://vision4robotics.github.io/publication/2020_iros_amcf-tracker/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/2020_iros_amcf-tracker/","section":"publication","summary":"The outstanding computational efficiency of discriminative correlation filter (DCF) fades away with various complicated improvements. Previous appearances are also gradually forgotten due to the exponential decay of historical views in traditional appearance updating scheme of DCF framework, reducing the model's robustness. In this work, a novel tracker based on DCF framework is proposed to augment memory of previously appeared views while running at a real-time speed. Several historical views and the current view are simultaneously introduced in training to allow tracker to adapt to new appearances as well as memorize previous ones. A novel rapid compressive context learning is proposed to increase the discriminative ability of the filter efficiently. Substantial experiments on UAVDT and UAV123 datasets have validated that the proposed tracker performs competitively against other 26 top DCF and deep-based trackers with over 40 FPS on CPU.","tags":["Visual tracking","Unmanned aerial vehicles","Augmented memory"],"title":"Augmented Memory for Correlation Filters in Real-Time UAV Tracking","type":"publication"},{"authors":["Fangqiang Ding","Changhong Fu","Yiming Li","Jin Jin","Chen Feng"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"4c9cb98cabea629fb7a37ba71c5310a6","permalink":"https://vision4robotics.github.io/publication/2020_iros_jsar-tracker/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/2020_iros_jsar-tracker/","section":"publication","summary":"Current unmanned aerial vehicle (UAV) visual tracking algorithms are primarily limited with respect to\":\" (i) the kind of size variation they can deal with, (ii) the implementation speed which hardly meet the real-time requirement. In this work, a real-time UAV tracking algorithm with powerful size estimation ability is proposed. Specifically, the overall tracking task is allocated to two 2D filters\":\" (i) translation filter for location prediction in the space domain, (ii) size filter for scale and aspect ratio optimization in the size domain. Besides, an efficient two-stage re-detection strategy is introduced for long-term UAV tracking tasks. Large-scale experiments on four UAV benchmarks demonstrate the superiority of the presented method which has computation feasibility on a low-cost CPU.","tags":["Visual tracking","Unmanned aerial vehicles","Joint scale and aspect ratio optimization"],"title":"Automatic Failure Recovery and Re-Initialization for Online UAV Tracking with Joint Scale and Aspect Ratio Optimization","type":"publication"},{"authors":["Changhong Fu","Fangqiang Ding","Yiming Li","Jin Jin","Chen Feng"],"categories":null,"content":" Overall work-flow of DR^2Track. \n","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"8c2ab231206a62eb37d14ffed9bf43ff","permalink":"https://vision4robotics.github.io/publication/2020_iros_dr2track-tracker/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/2020_iros_dr2track-tracker/","section":"publication","summary":"Visual tracking has yielded promising applications with unmanned aerial vehicle (UAV). In literature, the advanced discriminative correlation filter (DCF) type trackers generally distinguish the foreground from the background with a learned regressor which regresses the implicit circulated samples into a fixed target label. However, the predefined and unchanged regression target results in low robustness and adaptivity to uncertain aerial tracking scenarios. In this work, we exploit the local extreme points of the response map generated in the detection phase to automatically locate current distractors. By repressing the response of distractors in the regressor learning, we can dynamically and adaptively alter our regression target to leverage the tracking robustness as well as adaptivity. Substantial experiments conducted on three challenging UAV benchmarks demonstrate both the excellent performance and extraordinary speed (∼50fps on a cheap CPU) of our tracker.","tags":["Visual tracking","Unmanned aerial vehicles","Distractor repressed dynamic regression"],"title":"DR^2Track: Towards Real-Time Visual Tracking for UAV via Distractor Repressed Dynamic Regression","type":"publication"},{"authors":["Changhong Fu","Xiaoxiao Yang","Fan Li","Changjing Liu","Peng Lu"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"5850528e163470bcdde66d49766e4b65","permalink":"https://vision4robotics.github.io/publication/2020_iros_cpcf-tracker/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/2020_iros_cpcf-tracker/","section":"publication","summary":"Correlation filter (CF) has proven its superb efficiency in visual tracking for unmanned aerial vehicle (UAV) applications. To enhance the temporal smoothness of the filter, many CF-based approaches introduce temporal regularization terms to penalize the variation of coefficients in an element-wise manner. However, this element-wise smoothness is stiff to the filter coefficients and can lead to poor adaptiveness in case of various challenges, e.g., fast motion and viewpoint changes, which frequently occur in the UAV tracking process. To tackle this issue, this work introduces a novel tracker with consistency pursed correlation filter, i.e., CPCF tracker. It is able to achieve flexible temporal smoothness by evaluating the similarity between two consecutive response maps with a correlation operation. By correlation operations, the consistency constraint allows for flexible variations in the response map without losing temporal smoothness. Besides, a dynamic label function is introduced to further increase adaptiveness in the training process. Considerable experiments on three challenging UAV tracking benchmarks verify that the presented tracker has surpassed the other 25 state-of-the-art trackers with satisfactory speed (~25 FPS) for real-time applications on a single CPU.","tags":["Visual tracking","Unmanned aerial vehicles","Consistency pursued"],"title":"Learning Consistency Pursued Correlation Filters for Real-Time UAV Tracking","type":"publication"},{"authors":["Yujie He","Changhong Fu","Fuling Lin","Yiming Li","Peng Lu"],"categories":null,"content":" The main workflow of the TACF tracker. \n","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"577e5083b7eff43d607721263bb9babd","permalink":"https://vision4robotics.github.io/publication/2020_iros_tacf-tracker/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/2020_iros_tacf-tracker/","section":"publication","summary":"Object tracking has been broadly applied in unmanned aerial vehicle (UAV) tasks in recent years. However, existing algorithms still face difficulties such as partial occlusion, clutter background, and other challenging visual factors. Inspired by the cutting-edge attention mechanisms, a new visual tracking framework leveraging multi-level visual attention to make full use of the information during tracking. Three primary attention, i.e., contextual attention, dimensional attention, and spatiotemporal attention, are integrated into the training and detection stages of correlation filter-based tracking pipeline. Therefore, the proposed tracker is equipped with robust discriminative power against challenging factors while maintains high operational efficiency in UAV scenarios. Quantitative and qualitative experiments on two well-known benchmark with 173 challenging UAV video sequences demonstrate the effectiveness of the proposed framework. The proposed tracking algorithm compares favorably against state-of-the-art methods, yielding 4.8% relative gain in UAVDT and 8.2% relative gain in UAV123@10fps against the baseline tracker while operating at the speed of ∼28 frames per second","tags":["Visual tracking","Unmanned aerial vehicles","Multi-level visual attention"],"title":"Towards Robust Visual Tracking for Unmanned Aerial Vehicle with Tri-Attentional Correlation Filters","type":"publication"},{"authors":["Changhong Fu","Juntao Xu","Fuling Lin","Fuyu Guo","Tingcong Liu","Zhijun Zhang"],"categories":null,"content":" Comparison between the tracking pipeline of the baseline SRDCF tracker and the proposed DRCF tracker. \n","date":1587772800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587772800,"objectID":"5af1766e7fd3898b385f4b0d358898af","permalink":"https://vision4robotics.github.io/publication/2020_tgrs_drcf-tracker/","publishdate":"2020-04-25T00:00:00Z","relpermalink":"/publication/2020_tgrs_drcf-tracker/","section":"publication","summary":"Spatial regularization has proven itself to be an effective method in terms of alleviating the boundary effect and boosting the performance of a discriminative correlation filter (DCF) in aerial visual object tracking. However, existing spatial regularization methods usually treat the regularizer as a supplementary term apart from the main regression and neglect to regularize the filter involved in the correlation operation. To address the aforementioned issue, this work introduces a novel object saliency-aware dual regularized correlation filter, i.e., DRCF. Specifically, the proposed DRCF tracker suggests a dual regularization strategy to directly regularize the filter involved with the correlation operation inside the core of the filter generating ridge regression. This allows the DRCF tracker to suppress the boundary effect and consequently enhance the performance of the tracker. Furthermore, an efficient method based on a saliency detection algorithm is employed to generate the dual regularizers dynamically and provide the regularizers with online adjusting ability. This enables the generated dynamic regularizers to automatically discern the object from the background and actively regularize the filter to accentuate the object during its unpredictable appearance changes. By the merits of the dual regularization strategy and the saliency-aware dynamical regularizers, the proposed DRCF tracker performs favorably in terms of suppressing the boundary effect, penalizing the irrelevant background noise coefficients and boosting the overall performance of the tracker. Exhaustive evaluations on 193 challenging video sequences from multiple well-known challenging aerial object tracking benchmarks validate the accuracy and robustness of the proposed DRCF tracker against 27 other state-of-the-art methods. Meanwhile, the proposed tracker can perform real-time aerial tracking applications on a single CPU with a sufficient speed of 38.4 frames per second.","tags":["Visual tracking","Unmanned aerial vehicles","Keyfilter"],"title":"Object Saliency-Aware Dual Regularized Correlation Filter for Real-Time Aerial Tracking","type":"publication"},{"authors":["Yiming Li","Changhong Fu","Ziyuan Huang","Yinqiang Zhang","Jia Pan"],"categories":null,"content":" Main work-flow of the KAOT tracker. \n","date":1586217600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586217600,"objectID":"40b96a8cb3779f78b5457015094a6642","permalink":"https://vision4robotics.github.io/publication/2020_tmm_kaot-tracker/","publishdate":"2020-04-07T00:00:00Z","relpermalink":"/publication/2020_tmm_kaot-tracker/","section":"publication","summary":"Correlation filter-based tracking has been widely applied in unmanned aerial vehicle (UAV) with high efficiency. However, it has two imperfections, i.e., boundary effect and filter corruption. Several methods enlarging the search area can mitigate boundary effect, yet introducing undesired background distraction. Existing frame-by-frame context learning strategies for repressing background distraction nevertheless lower the tracking speed. Inspired by keyframe-based simultaneous localization and mapping, keyfilter is proposed in visual tracking for the first time, in order to handle the above issues efficiently and effectively. Keyfilters generated by periodically selected keyframes learn the context intermittently and are used to restrain the learning of filters, so that 1) context awareness can be transmitted to all the filters via keyfilter restriction, and 2) filter corruption can be repressed. Compared to the state-of-the-art results, our tracker performs better on two challenging benchmarks, with enough speed for UAV real-time applications.","tags":["Visual tracking","Unmanned aerial vehicles","Keyfilter"],"title":"Intermittent Contextual Learning for Keyfilter-Aware UAV Object Tracking Using Deep Convolutional Feature","type":"publication"},{"authors":["Yiming Li","Changhong Fu","Fangqiang Ding","Ziyuan Huang","Geng Lu"],"categories":null,"content":" Central idea of AutoTrack.\n\n","date":1582502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582502400,"objectID":"f96897038cb4fd1e59d7ddbd39f3388f","permalink":"https://vision4robotics.github.io/publication/2020_cvpr_autotrack/","publishdate":"2020-02-24T00:00:00Z","relpermalink":"/publication/2020_cvpr_autotrack/","section":"publication","summary":"Most existing trackers based on discriminative correlation filters (DCF) try to introduce predefined regularization term to improve the learning of target objects, e.g., by sup-pressing background learning or by restricting change rate of correlation filters. However, predefined parameters intro-duce much effort in tuning them and they still fail to adapt to new situations that the designer did not think of. In this work, a novel approach is proposed to online automatically and adaptively learn spatio-temporal regularization term. Spatially local response map variation is introduced as spatial regularization to make DCF focus on the learning of trust-worthy parts of the object, and global response map variation determines the updating rate of the filter. Extensive experiments on four UAV benchmarks have proven the superiority of our method compared to the state-of-the-art CPU- and GPU-based trackers, with a speed of ∼60 frames per second running on a single CPU. Our tracker is additionally proposed to be applied in UAV localization. Considerable tests in the indoor practical scenarios have proven the effectiveness and versatility of our localization method. The code is available at .","tags":["Correlation filter","Real-time object tracking","Unmanned aerial vehicles","Localization by tracking"],"title":"AutoTrack: Towards High-Performance Visual Tracking for UAV with Automatic Spatio-Temporal Regularization","type":"publication"},{"authors":["Fuling Lin","Changhong Fu","Yujie He","Fuyu Guo","Qian Tang"],"categories":null,"content":"\n Comparison between discriminative correlation filter (DCF) and the proposed BiCF tracker\n\n","date":1579694400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579694400,"objectID":"6bb33494dc7d24845b77b9fe07c72ab1","permalink":"https://vision4robotics.github.io/publication/2020_icra_bicf-tracker/","publishdate":"2020-01-22T12:00:00Z","relpermalink":"/publication/2020_icra_bicf-tracker/","section":"publication","summary":"Correlation filters (CFs) have shown excellent performance in unmanned aerial vehicle (UAV) tracking scenarios due to their high computational efficiency. During the UAV tracking process, viewpoint variations are usually accompanied by changes in the object and background appearance, which poses a unique challenge to CF-based trackers. Since the appearance is gradually changing over time, an ideal tracker can not only forward predict the object position but also backtrack to locate its position in the previous frame. There exist response-based errors in the reversibility of the tracking process containing the information on the changes in appearance. However, some existing methods do not consider the forward and backward errors based on while using only the current training sample to learn the filter. For other ones, the applicants of considerable historical training samples impose a computational burden on the UAV. In this work, a novel bidirectional incongruity-aware correlation filter (BiCF) is proposed. By integrating the response-based bidirectional incongruity error into the CF, BiCF can efficiently learn the changes in appearance and suppress the inconsistent error. Extensive experiments on 243 challenging sequences from three UAV datasets (UAV123, UAVDT, and DTB70) are conducted to demonstrate that BiCF favorably outperforms other 25 state-of-the-art trackers and achieves a real-time speed of 45.4 FPS on a single CPU, which can be applied in UAV efficiently.","tags":["Visual tracking","Unmanned aerial vehicles","Bidirectional incongruity learning"],"title":"BiCF: Learning Bidirectional Incongruity-Aware Correlation Filter for Efficient UAV Object Tracking","type":"publication"},{"authors":["Fan Li","Changhong Fu","Fuling Lin","Yiming Li","Peng Lu"],"categories":null,"content":"\nComparison between our TSD tracker with the baseline BACF tracker\n\n","date":1579680000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579680000,"objectID":"6bba38c858ff464c41cde65aad3840f8","permalink":"https://vision4robotics.github.io/publication/2020_icra_tsd-tracker/","publishdate":"2020-01-22T08:00:00Z","relpermalink":"/publication/2020_icra_tsd-tracker/","section":"publication","summary":"Correlation filter (CF) has recently exhibited promising performance in visual object tracking for unmanned aerial vehicle (UAV). Such online learning method heavily depends on the quality of the training-set, yet complicated aerial scenarios like occlusion or out of view can reduce its reliability. In this work, a novel time slot-based distillation approach is proposed to efficiently and effectively optimize the training-set’s quality on the fly. A cooperative energy minimization function is established to score the historical samples adaptively. To accel-erate the scoring process, frames with high confident tracking results are employed as the keyframes to divide the tracking process into multiple time slots. After the establishment of a new slot, the weighted fusion of the previous samples generates one key-sample, in order to reduce the number of samples to be scored. Besides, when the current time slot exceeds the maximum frame number, which can be scored, the sample with the lowest score will be discarded. Consequently, the training-set can be efficiently and reliably distilled. Comprehensive tests on two well-known UAV benchmarks prove the effectiveness of our method with real-time speed on a single CPU.","tags":["Visual tracking","Unmanned aerial vehicles","Training-Set distillation"],"title":"Training-Set Distillation for Real-Time UAV Object Tracking","type":"publication"},{"authors":["Yiming Li","Changhong Fu","Ziyuan Huang","Yinqiang Zhang","Jia Pan"],"categories":null,"content":" Comparison between response maps of our tracker and baseline.\n\n","date":1579651200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579651200,"objectID":"3bcd6c1a928b6f63c32bf6c64f38f25b","permalink":"https://vision4robotics.github.io/publication/2020_icra_kaot-tracker/","publishdate":"2020-01-22T00:00:00Z","relpermalink":"/publication/2020_icra_kaot-tracker/","section":"publication","summary":"Correlation filter-based tracking has been widely applied in unmanned aerial vehicle (UAV) with high efficiency. However, it has two imperfections, i.e., boundary effect and filter corruption. Several methods enlarging the search area can mitigate boundary effect, yet introducing undesired background distraction. Existing frame-by-frame context learning strategies for repressing background distraction nevertheless lower the tracking speed. Inspired by keyframe-based simultaneous localization and mapping, keyfilter is proposed in visual tracking for the first time, in order to handle the above issues efficiently and effectively. Keyfilters generated by periodically selected keyframes learn the context intermittently and are used to restrain the learning of filters, so that 1) context awareness can be transmitted to all the filters via keyfilter restriction, and 2) filter corruption can be repressed. Compared to the state-of-the-art results, our tracker performs better on two challenging benchmarks, with enough speed for UAV real-time applications.","tags":["Visual tracking","Unmanned aerial vehicles","Keyfilter"],"title":"Keyfilter-Aware Real-Time UAV Object Tracking","type":"publication"},{"authors":["Changhong Fu","Yujie He","Fuling Lin","Weijiang Xiong"],"categories":null,"content":"\nMain structure of the proposed MKCT-Tracker\n\n","date":1575504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575504000,"objectID":"aa813cef24765cfa64a8db4422313184","permalink":"https://vision4robotics.github.io/publication/2020_ncaa_mkct-tracker/","publishdate":"2020-01-06T00:00:00Z","relpermalink":"/publication/2020_ncaa_mkct-tracker/","section":"publication","summary":"In recent years, the correlation filter (CF)-based method has significantly advanced in the tracking for unmanned aerial vehicles (UAV). As the core component of most trackers, CF is a discriminative classifier to distinguish the object from the surrounding environment. However, the poor representation of the object and lack of contextual information have restricted the tracker to gain better performance. In this work, a robust framework with multi-kernelized correlators is proposed to improve robustness and accuracy simultaneously. Both convolutional features extracted from the neural network and hand-crafted features are employed to enhance expressions for object appearances. Then, the adaptive context analysis strategy helps filters to effectively learn the surrounding information by introducing context patches with the GMSD index. In the training stage, multiple dynamic filters with time-attenuated factors are introduced to avoid tracking failure caused by dramatic appearance changes. The response maps corresponding to different features are finally fused before the novel resolution enhancement operation to increase distinguishing capability. As a result, the optimization problem is reformulated, and a closed-form solution for the proposed framework can be obtained in the kernel space. Extensive experiments on 100 challenging UAV tracking sequences demonstrate the proposed tracker outperforms other 23 state-of-the-art trackers and can effectively handle unexpected appearance variations under the complex and constantly changing working conditions.","tags":["Visual tracking","Unmanned aerial vehicles","Multi-kernelized correlators","Adaptive context analysis","Dynamic weighted filters"],"title":"Robust Multi-Kernelized Correlators for UAV Tracking with Adaptive Context Analysis and Dynamic Weighted Filters","type":"publication"},{"authors":["Changhong Fu","Weijiang Xiong","Fuling Lin","Yufeng Yue"],"categories":null,"content":"\nFig. 1 Main workflow of the proposed SASR tracker.\n\n","date":1569801600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569801600,"objectID":"d2d81b87c37cfff0a97221e87a865c7a","permalink":"https://vision4robotics.github.io/publication/2019_signal_processing_sasr-tracker/","publishdate":"2019-09-30T00:00:00Z","relpermalink":"/publication/2019_signal_processing_sasr-tracker/","section":"publication","summary":"The great advance of visual object tracking has provided unmanned aerial vehicle (UAV) with intriguing capability for various practical applications. With promising performance and efficiency, discriminative correlation filter-based trackers have drawn great attention and undergone remarkable progress. However, background interference and boundary effect remain two thorny problems. In this paper, a surrounding-aware tracker with selective spatial regularization (SASR) is presented. SASR tracker extracts surrounding samples according to the size and shape of the object in order to utilize context and maintain the integrality of the object. Additionally, a selective spatial regularizer is introduced to address boundary effect. Central coefficients in the filter are evenly regularized to preserve valid information from the object. While the others are penalized according to their spatial location. Under the framework of SASR tracker, surrounding information and selective spatial regularization prove to be complementary to each other, which actually did not draw much attention before. They managed to improve not only the robustness against various distractions in the surrounding but also the flexibility to catch up with frequent appearance change of the object. Qualitative evaluation and quantitative experiments on challenging UAV tracking sequences have shown that SASR tracker has performed favorably against 23 state-of-the-art trackers.","tags":["Unmanned aerial vehicle (UAV)","Visual object tracking","Discriminative correlation filter","Surrounding information","Selective spatial regularization"],"title":"Surrounding-Aware Correlation Filter for UAV Tracking with Selective Spatial Regularization","type":"publication"},{"authors":["Ziyuan Huang","Changhong Fu","Yiming Li","Fuling Lin","Peng Lu"],"categories":null,"content":"\nFig. 1 Comparison between background-aware correlation filter (BACF) and the proposed ARCF tracker.\nFig. 2 Main structure of the proposed ARCF tracker.\n\n","date":1563753600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563753600,"objectID":"f83555e1f1f71a6d7b37896ad8adbdd5","permalink":"https://vision4robotics.github.io/publication/2019_iccv_arcf-tracker/","publishdate":"2019-07-22T00:00:00Z","relpermalink":"/publication/2019_iccv_arcf-tracker/","section":"publication","summary":"Traditional framework of discriminative correlation filters (DCF) is often subject to undesired boundary effects. Several approaches to enlarge search regions have been already proposed in the past years to make up for this shortcoming. However, with excessive background information, more background noises are also introduced and the discriminative filter is prone to learn from the ambiance rather than the object. This situation, along with appearance changes of objects caused by full/partial occlusion, illumination variation, and other reasons has made it more likely to have aberrances in the detection process, which could substantially degrade the credibility of its result. Therefore, in this work, a novel approach to repress the aberrances happening during the detection process is proposed, i.e., aberrance repressed correlation filter (ARCF). By enforcing restriction to the rate of alteration in response maps generated in the detection phase, the ARCF tracker can evidently suppress aberrances and is thus more robust and accurate to track objects. Considerable experiments are conducted on different UAV datasets to perform object tracking from an aerial view, i.e., UAV123, UAVDT, and DTB70, with 243 challenging image sequences containing over 90K frames to verify the performance of the ARCF tracker and it has proven itself to have outperformed other 20 state-of-the-art trackers based on DCF and deep-based frameworks with sufficient speed for real-time applications.","tags":["Correlation filter","Real-time object tracking","Unmanned aerial vehicles","Multi-Frame Consensus Veriﬁcation"],"title":"Learning Aberrance Repressed Correlation Filters for Real-Time UAV Tracking","type":"publication"},{"authors":["Changhong Fu","Ziyuan Huang","Yiming Li","Ran Duan","Peng Lu"],"categories":null,"content":"\nFig. 1 Main structure of the proposed tracking approach.\n\n","date":1561334400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561334400,"objectID":"261d6e3cb4942a75c4083e904f782650","permalink":"https://vision4robotics.github.io/publication/2019_iros_bevt-tracker/","publishdate":"2019-06-24T00:00:00Z","relpermalink":"/publication/2019_iros_bevt-tracker/","section":"publication","summary":"Due to implicitly introduced periodic shifting of limited searching area, visual object tracking using correlation filters often has to confront undesired boundary effect. As boundary effect severely degrade the quality of object model, it has made it a challenging task for unmanned aerial vehicles (UAV) to perform robust and accurate object following. Traditional hand-crafted features are also not precise and robust enough to describe the object in the viewing point of UAV. In this work, a novel tracker with online enhanced background learning is specifically proposed to tackle boundary effects. Real background samples are densely extracted to learn as well as update correlation filters. Spatial penalization is introduced to offset the noise introduced by exceedingly more background information so that a more accurate appearance model can be established. Meanwhile, convolutional features are extracted to provide a more comprehensive representation of the object. In order to mitigate changes of objects' appearances, multi-frame technique is applied to learn an ideal response map and verify the generated one in each frame. Exhaustive experiments were conducted on 100 challenging UAV image sequences and the proposed tracker has achieved state-of-the-art performance.","tags":["Correlation filter","Onject tracking","Unmanned aerial vehicles","Multi-Frame Consensus Veriﬁcation"],"title":"Boundary Effect-Aware Visual Tracking for UAV with Online Enhanced Background Learning and Multi-Frame Consensus Veriﬁcation","type":"publication"},{"authors":["Changhong Fu","Yinqiang Zhang","Ziyuan Huang","Ran Duan","Zongwu Xie"],"categories":null,"content":"\nFig. 1 Main structure of the proposed tracking approach.\n\n","date":1560211200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560211200,"objectID":"d52b309a8d46898dba500a69ea2a9fdd","permalink":"https://vision4robotics.github.io/publication/2019_ieee_access_pbbat-tracker/","publishdate":"2019-06-11T00:00:00Z","relpermalink":"/publication/2019_ieee_access_pbbat-tracker/","section":"publication","summary":"In recent years, visual tracking is a challenging task in UAV applications. The standard correlation filter (CF) has been extensively applied for UAV object tracking. However, the CF-based tracker severely suffers from boundary effects and cannot effectively cope with object occlusion, which results in suboptimal performance. Besides, it is still a tough task to obtain an appearance model precisely with hand-crafted features. In this paper, a novel part-based tracker is proposed for the UAV. With successive cropping operations, the tracking object is separated into several parts. More specially, background-aware correlation filters with different cropping matrices are applied. To estimate the translation and scale variation of the tracking object, a structure comparison, and a Bayesian inference approach are proposed, which jointly achieve a coarse-to-fine strategy. Moreover, an adaptive mechanism is used to update the local appearance model of each part with a Gaussian process regression method. To construct a better appearance model, features extracted from the convolutional neural network are utilized instead of hand-crafted features. Through extensive experiments, the proposed tracker reaches competitive performance on 123 challenging UAV image sequences and outperforms other 20 popular state-of-the-art visual trackers in terms of overall performance and different challenging attributes.","tags":["Visual tracking","Unmanned aerial vehicle (UAV)"],"title":"Part-Based Background-Aware Tracking for UAV with Convolutional Features","type":"publication"},{"authors":["Guang Chen","Shu Liu","Kejia Ren","Zhongnan Qu","Changhong Fu","Gereon Hinz","Alois Knoll"],"categories":null,"content":"![SSIM-WMIL_workflow](featured.jpg) Fig. 1 The closed-loop control structure for the long-term navigation of the quadrotor UAV in real-time application.  -- ","date":1560124800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560124800,"objectID":"67e10887ad282922fd5dee0e7a0e3c87","permalink":"https://vision4robotics.github.io/publication/2019_iet_intell_transp_syst/","publishdate":"2019-06-10T00:00:00Z","relpermalink":"/publication/2019_iet_intell_transp_syst/","section":"publication","summary":"Advanced communication technology of IoT era enables a heterogeneous connectivity where mobile devices broadcast information to everything. Previous short-range on-board sensor perception system attached to moblie applications such as robots and vehicles could be transferred to long-range mobilesensing perception system, which can be used as part of a more extensive intelligent system surveilling real-time state of the environment. However, the mobile sensing perception brings new challenges for how to efficiently analyze and intelligently interpret the deluge of IoT data in mission-critical services. In this article, we model the challenges as latency, packet loss and measurement noise which severely deteriorate the reliability and quality of IoT data. We integrate the artificial intelligence into IoT to tackle these challenges. We propose a novel architecture that leverages recurrent neural networks (RNN) and Kalman filtering to anticipate motions and interactions between objects. The basic idea is to learn environment dynamics by recurrent networks. To improve the robustness of IoT communication, we use the idea of Kalman filtering and deploy a prediction and correction step. In this way, the architecture learns to develop a biased belief between prediction and measurement in the different situation. We demonstrate our approach with synthetic and real-world datasets with noise that mimics the challenges of IoT communications. Our method brings a new level of IoT intelligence. It is also lightweight compared to other state-of-theart convolutional recurrent architecture and is ideally suitable for the resource-limited mobile applications","tags":["Recurrent neural network","Internet of things","Kalman filtering","Convolutional LSTM","Factor graph"],"title":"Deep Anticipation: Lightweight Intelligent Mobile Sensing for Unmanned Vehicles in IoT by Recurrent Architecture","type":"publication"},{"authors":["Andriy Sarabakha","Changhong Fu","Erdal Kayacan"],"categories":null,"content":"![SSIM-WMIL_workflow](featured.jpg) Fig. 1 The closed-loop control structure for the long-term navigation of the quadrotor UAV in real-time application.  -- ","date":1557446400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557446400,"objectID":"8ce82b86a51519a1e30d70a72724343f","permalink":"https://vision4robotics.github.io/publication/2019_appl_soft_comput/","publishdate":"2019-05-10T00:00:00Z","relpermalink":"/publication/2019_appl_soft_comput/","section":"publication","summary":"Although a considerable amount of effort has been put in to show that fuzzy logic controllers have exceptional capabilities of dealing with uncertainty, there are still noteworthy concerns, e.g., the design of fuzzy logic controllers is an arduous task due to the lack of closed-form input–output relationships which is a limitation to interpretability of these controllers. The role of design parameters in fuzzy logic controllers, such as position, shape, and height of membership functions, is not straightforward. Motivated by the fact that the availability of an interpretable relationship from input to output will simplify the design procedure of fuzzy logic controllers, the main aims in this work are derive fuzzy mappings for both type-1 and interval type-2 fuzzy logic controllers, analyse them, and eventually benefit from such a nonlinear mapping to design fuzzy logic controllers. Thereafter, simulation and real-time experimental results support the presented theoretical findings.","tags":["Type-1 fuzzy logic controllers","Interval type-2 fuzzy logic controllers","Fuzzy mapping","Aerial robotics","Unmanned aerial vehicles"],"title":"Intuit Before Tuning: Type-1 and Type-2 Fuzzy Logic Controllers","type":"publication"},{"authors":["Changhong Fu","Fuling Lin","Yiming Li","Guang Chen"],"categories":null,"content":"\nFig. 1 Main structure of the proposed tracking approach.\n\n","date":1551830400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551830400,"objectID":"9c89d8499279bdf9ef669630a3a66cf0","permalink":"https://vision4robotics.github.io/publication/2019_remote_sens_omfl-tracker/","publishdate":"2019-03-06T00:00:00Z","relpermalink":"/publication/2019_remote_sens_omfl-tracker/","section":"publication","summary":"In this paper, a novel online learning-based tracker is presented for the unmanned aerial vehicle (UAV) in different types of tracking applications.","tags":["Visual tracking","Unmanned aerial vehicle (UAV)","Background-aware correlation filter","Online multi-feature learning","Peak-to-sidelobe ratio (PSR)","Response map fusion"],"title":"Correlation Filter-Based Visual Tracking for UAV with Online Multi-Feature Learning","type":"publication"},{"authors":["Changhong Fu","Ran Duan","Erdal Kayacan"],"categories":null,"content":"\nFig. 1 Main structure of the proposed tracking approach.\n\n","date":1546041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546041600,"objectID":"cd794bb37b2cab0f1083e488a11e406a","permalink":"https://vision4robotics.github.io/publication/2019_inf_sci_ssim-wmil/","publishdate":"2018-12-29T00:00:00Z","relpermalink":"/publication/2019_inf_sci_ssim-wmil/","section":"publication","summary":"This paper presents an online adaptive tracker, which employs a novel weighted multiple instance learning (WMIL) approach.In the proposed tracker, both positive and negative sample importances are integrated into an online learning mechanism for improving tracking performance in challenging environments. The sample importance is computed based on a new measure, i.e., structural similarity (SSIM), instead of using the Euclidean distance. Moreover, a novel bag probability function, which adopts both positive and negative weighted instance probabilities, is designed. Furthermore, a novel efficient weak classifier selection solution is developed for the proposed tracker. Qualitative and quantitative experiments on 30 challenging image sequences show that the novel tracking algorithm, i.e., SSIM-WMIL tracker, performs favorably against the MIL and WMIL counterparts as well as other 13 recently-proposed state-of-the-art trackers in terms of accuracy, robustness and efficiency. In addition, the negative sample importance can be used to enhance the multiple instance learning, and the SSIM-based approach is capable of improving the multiple instance learning performance for object tracking when compared to the Euclidean distance-based method.","tags":["Visual tracking","Multiple instance learning","Structural similarity","Negative sample importance","Bag probability function","Weak classifier selection"],"title":"Visual Tracking With Online Structural Similarity-Based Weighted Multiple Instance Learning","type":"publication"},{"authors":["Changhong Fu","Yinqiang Zhang","Ran Duan","Zongwu Xie"],"categories":null,"content":"\nFig. 1 Main structure of the proposed tracking approach.\n\n","date":1539820800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539820800,"objectID":"0930f45b6aefa5864f0fe417409c6351","permalink":"https://vision4robotics.github.io/publication/2018_robio_spbacf-tracker/","publishdate":"2018-10-18T00:00:00Z","relpermalink":"/publication/2018_robio_spbacf-tracker/","section":"publication","summary":"Robust visual tracking for the unmanned aerial vehicle (UAV) is a challenging task in different types of civilian UAV applications. Although the classical correlation filter (CF) has been widely applied for UAV object tracking, the background of the object is not learned in the classical CF. In addition, the classical CF cannot estimate the object scale changes, and it is not able to cope with object occlusion effectively. Part-based tracking approach is often used for the visual tracker to solve the occlusion issue. However, its real-time performance for the UAV cannot be achieved due to the high cost of object appearance updating. In this paper, a novel robust visual tracker is presented for the UAV. The object is initially divided into multiple parts, and different background-aware correlation filters are applied for these divided object parts, respectively. An efficient coarse-to-fine strategy with structure comparison and Bayesian inference approach is proposed to locate object and estimate the object scale changes. In addition, an adaptive threshold is presented to update each local appearance model with a Gaussian process regression method. Qualitative and quantitative tests show that the presented visual tracking algorithm reaches real-time performance (i.e., more than twenty frames per second) on an i7 processor with 640×360 image resolution, and performs favorably against the most popular state-of-the-art visual trackers in terms of robustness and accuracy. To the best of our knowledge, it is the first time that this novel scalable part-based visual tracker is presented, and applied for the UAV tracking applications.","tags":["Correlation filter","Onject tracking","Unmanned aerial vehicles","Real-time systems"],"title":"Robust Scalable Part-Based Visual Tracking for UAV with Background-Aware Correlation Filter","type":"publication"},{"authors":["Changhong Fu","Andriy Sarabakha","Erdal Kayacan","Christian Wagner","Robert John","Jon Garibaldi"],"categories":null,"content":"\nFig. 1 The closed-loop control structure for the long-term navigation of the quadrotor UAV in real-time application.\n\n","date":1519776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519776000,"objectID":"d6e3c6233bd65d3fee4a94c25879af84","permalink":"https://vision4robotics.github.io/publication/2018_tmech/","publishdate":"2018-02-28T00:00:00Z","relpermalink":"/publication/2018_tmech/","section":"publication","summary":"Input uncertainty, e.g., noise on the on-board camera and inertial measurement unit, in vision-based control of unmanned aerial vehicles (UAVs) is an inevitable problem. In order to handle input uncertainties as well as further analyze the interaction between the input and the antecedent fuzzy sets (FSs) of nonsingleton fuzzy logic controllers (NSFLCs), an input uncertainty sensitivity enhanced NSFLC has been developed in robot operating system using the C++ programming language. Based on recent advances in nonsingleton inference, the centroid of the intersection of the input and antecedent FSs (Cen-NSFLC) is utilized to calculate the firing strength of each rule instead of the maximum of the intersection used in traditional NSFLC (Tra-NSFLC). An 8-shaped trajectory, consisting of straight and curved lines, is used for the real-time validation of the proposed controllers for a trajectory following problem. An accurate monocular keyframe-based visual-inertial simultaneous localization and mapping (SLAM) approach is used to estimate the position of the quadrotor UAV in GPS-denied unknown environments. The performance of the Cen-NSFLC is compared with a conventional proportional-integral derivative (PID) controller, a singleton FLC and a Tra-NSFLC. All controllers are evaluated for different flight speeds, thus introducing different levels of uncertainty into the control problem. Visual-inertial SLAM-based real-time quadrotor UAV flight tests demonstrate that not only does the Cen-NSFLC achieve the best control performance among the four controllers, but it also shows better control performance when compared to their singleton counterparts. Considering the bias in the use of model-based controllers, e.g., PID, for the control of UAVs, this paper advocates an alternative method, namely Cen-NSFLCs, in uncertain working environments.","tags":["Fuzzy logic controller (FLC)","input uncertainty sensitivity enhanced nonsingleton FLC (NSFLC)","Monocular visual-inertial simultaneous localization and mapping (SLAM)","Unmanned aerial vehicle (UAV)"],"title":"Input Uncertainty Sensitivity Enhanced Nonsingleton Fuzzy Logic Controllers for Long-Term Navigation of Quadrotor UAVs","type":"publication"}]