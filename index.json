[{"authors":["changhong-fu"],"categories":null,"content":"Dr. Changhong Fu received his Ph.D. degree in Robotics \u0026amp; Automation from Computer Vision \u0026amp; Aerial Robotics Lab, Technical University of Madrid, Spain. During his Ph.D., he held two research positions at Arizona State University, USA \u0026amp; Nanyang Technological University (NTU), Singapore. After received his Ph.D., he worked at the NTU as Post-doc Research Fellow. He has worked on 2 international, 2 national \u0026amp; 4 industrial projects related to the vision for UAV. Currently, he is an Assistant Professor at School of Mechanical Engineering, Tongji University, China, and leading 4 projects related to the vision for multi-UAV. His research areas are Intelligent Vision \u0026amp; Control for Unmanned Systems in Complex Environments.\n","date":1632614400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1632614400,"objectID":"4d44576c929bb566eeb3c1d75eab7732","permalink":"https://vision4robotics.github.io/authors/changhong-fu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/changhong-fu/","section":"authors","summary":"Dr. Changhong Fu received his Ph.D. degree in Robotics \u0026amp; Automation from Computer Vision \u0026amp; Aerial Robotics Lab, Technical University of Madrid, Spain. During his Ph.D., he held two research positions at Arizona State University, USA \u0026amp; Nanyang Technological University (NTU), Singapore. After received his Ph.D., he worked at the NTU as Post-doc Research Fellow. He has worked on 2 international, 2 national \u0026amp; 4 industrial projects related to the vision for UAV.","tags":null,"title":"Changhong Fu","type":"authors"},{"authors":["fangqiang-ding"],"categories":null,"content":"Fangqiang Ding received his B.Eng. degree in Mechanical Engineering from Tongji University, Shanghai, China. He is currently pursuing the PhD degree in Robotics and Autonomous System at The University of Edinburgh, Edinburgh, U.K.. His research interests include unmanned aerial vehicle and computer Vision.\n","date":1632614400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1632614400,"objectID":"e5afbfabb290cc0d7ec63af348a5ecf6","permalink":"https://vision4robotics.github.io/authors/fangqiang-ding/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/fangqiang-ding/","section":"authors","summary":"Fangqiang Ding received his B.Eng. degree in Mechanical Engineering from Tongji University, Shanghai, China. He is currently pursuing the PhD degree in Robotics and Autonomous System at The University of Edinburgh, Edinburgh, U.K.. His research interests include unmanned aerial vehicle and computer Vision.","tags":null,"title":"Fangqiang Ding","type":"authors"},{"authors":["jin-jin"],"categories":null,"content":"Jin Jin received his B.Eng. degree in Mechanical Engineering from Tongji University, Shanghai, China. He is currently pursuing the M.Sc. degree in Management, Technology, and Economics at ETH, Zurich, Switzerland. He majors in Mechanical Engineering and is now specializing in visual object tracking as well.\n","date":1632614400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1632614400,"objectID":"5a89def764852e0117af9268b9bc82b1","permalink":"https://vision4robotics.github.io/authors/jin-jin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jin-jin/","section":"authors","summary":"Jin Jin received his B.Eng. degree in Mechanical Engineering from Tongji University, Shanghai, China. He is currently pursuing the M.Sc. degree in Management, Technology, and Economics at ETH, Zurich, Switzerland. He majors in Mechanical Engineering and is now specializing in visual object tracking as well.","tags":null,"title":"Jin Jin","type":"authors"},{"authors":["yiming-li"],"categories":null,"content":"Yiming Li received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing PhD in Robotics, New York University, USA. His research interests include robotics and visual object tracking.\n","date":1632614400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1632614400,"objectID":"74ab2b740364fa8299cfb4d6742cccdd","permalink":"https://vision4robotics.github.io/authors/yiming-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yiming-li/","section":"authors","summary":"Yiming Li received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing PhD in Robotics, New York University, USA. His research interests include robotics and visual object tracking.","tags":null,"title":"Yiming Li","type":"authors"},{"authors":["bowen-li"],"categories":null,"content":"Bowen Li is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include robotics, artificial intelligence, and computer vision.\n","date":1626912000,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1626912000,"objectID":"50b3204098dafa7a3d9415e91dcb3d02","permalink":"https://vision4robotics.github.io/authors/bowen-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/bowen-li/","section":"authors","summary":"Bowen Li is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include robotics, artificial intelligence, and computer vision.","tags":null,"title":"Bowen Li","type":"authors"},{"authors":["junjie-ye"],"categories":null,"content":"Junjie Ye received the B.Eng. degree in mechanical engineering in 2020 from Tongji University, Shanghai, China, where he is currently working toward the M.Sc. degree in mechanical engineering. His research interests include visual object tracking, deep learning, and robotics.\n","date":1626912000,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1626912000,"objectID":"fe2fccb538ad42b64242c0535fb42663","permalink":"https://vision4robotics.github.io/authors/junjie-ye/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/junjie-ye/","section":"authors","summary":"Junjie Ye received the B.Eng. degree in mechanical engineering in 2020 from Tongji University, Shanghai, China, where he is currently working toward the M.Sc. degree in mechanical engineering. His research interests include visual object tracking, deep learning, and robotics.","tags":null,"title":"Junjie Ye","type":"authors"},{"authors":["ziang-cao"],"categories":null,"content":"Ziang Cao is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in Vehicle Engineering. His research interests include unmanned aerial vehicle and visual object tracking.\n","date":1626912000,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1626912000,"objectID":"4fd79dd332235ee46766fc8986a07748","permalink":"https://vision4robotics.github.io/authors/ziang-cao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ziang-cao/","section":"authors","summary":"Ziang Cao is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in Vehicle Engineering. His research interests include unmanned aerial vehicle and visual object tracking.","tags":null,"title":"Ziang Cao","type":"authors"},{"authors":["fuling-lin"],"categories":null,"content":"Fuling Lin received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing M.Sc. degree in mechanical engineering in Tongji University, Shanghai, China. His research interests include robotics, visual object tracking and computer vision.\n","date":1625011200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1625011200,"objectID":"3bcddded919311d425d4fbd3ccb1c10d","permalink":"https://vision4robotics.github.io/authors/fuling-lin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/fuling-lin/","section":"authors","summary":"Fuling Lin received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing M.Sc. degree in mechanical engineering in Tongji University, Shanghai, China. His research interests include robotics, visual object tracking and computer vision.","tags":null,"title":"Fuling Lin","type":"authors"},{"authors":["guangze-zheng"],"categories":null,"content":"Guangze Zheng is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include visual object tracking and machine learning.\n","date":1625011200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1625011200,"objectID":"1ca1e48351fd704681e10048d6811556","permalink":"https://vision4robotics.github.io/authors/guangze-zheng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/guangze-zheng/","section":"authors","summary":"Guangze Zheng is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include visual object tracking and machine learning.","tags":null,"title":"Guangze Zheng","type":"authors"},{"authors":["weijiang-xiong"],"categories":null,"content":"Weijiang Xiong received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing M.Sc. degree in Robotics, Aalto University, Finland. His research interests include visual object tracking and artificial intelligence.\n","date":1625011200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1625011200,"objectID":"a60def9cac4a1c8f7aad2558c4a06e69","permalink":"https://vision4robotics.github.io/authors/weijiang-xiong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/weijiang-xiong/","section":"authors","summary":"Weijiang Xiong received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing M.Sc. degree in Robotics, Aalto University, Finland. His research interests include visual object tracking and artificial intelligence.","tags":null,"title":"Weijiang Xiong","type":"authors"},{"authors":["yujie-he"],"categories":null,"content":"Yujie He received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing M.Sc. degree in Robotics, École Polytechnique Fédérale de Lausanne (EPFL), Switzerland. His research interests include robotics, visual object tracking, and place recognition.\n","date":1625011200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1625011200,"objectID":"388dc41701aaac09a116f4f6c6c06830","permalink":"https://vision4robotics.github.io/authors/yujie-he/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yujie-he/","section":"authors","summary":"Yujie He received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing M.Sc. degree in Robotics, École Polytechnique Fédérale de Lausanne (EPFL), Switzerland. His research interests include robotics, visual object tracking, and place recognition.","tags":null,"title":"Yujie He","type":"authors"},{"authors":["ran-duan"],"categories":null,"content":"Ran Duan received the Master degree in computer vision in 2015, from the University of Bourgogne, France (European VIBOT program). From 2015 to 2017, he worked at Nanyang Technological University (NTU) as a research associate. He is currently a PhD candidate at the Department of Aeronautical and Aviation Engineering (AAE), the Hong Kong Polytechnic University. His research areas include image processing, visual tracking, and UAV visual navigation.\n","date":1614499200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1614499200,"objectID":"400624d450edf15bad2dda1fe6682867","permalink":"https://vision4robotics.github.io/authors/ran-duan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ran-duan/","section":"authors","summary":"Ran Duan received the Master degree in computer vision in 2015, from the University of Bourgogne, France (European VIBOT program). From 2015 to 2017, he worked at Nanyang Technological University (NTU) as a research associate. He is currently a PhD candidate at the Department of Aeronautical and Aviation Engineering (AAE), the Hong Kong Polytechnic University. His research areas include image processing, visual tracking, and UAV visual navigation.","tags":null,"title":"Ran Duan","type":"authors"},{"authors":["juntao-xu"],"categories":null,"content":"Juntao Xu received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing PhD in Mechanical Engineering, Hong Kong University, China. His research interests involve visual object tracking and computer vision.\n","date":1602028800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1602028800,"objectID":"284a7f7acfea514aee71c7fe9afc9da8","permalink":"https://vision4robotics.github.io/authors/juntao-xu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/juntao-xu/","section":"authors","summary":"Juntao Xu received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing PhD in Mechanical Engineering, Hong Kong University, China. His research interests involve visual object tracking and computer vision.","tags":null,"title":"Juntao Xu","type":"authors"},{"authors":["changjing-liu"],"categories":null,"content":"Changjing Liu received the B.E. degree in Mechanical Engineering from Tongji University, Shanghai, China. He is currently working toward the M.Sc. degree in Instrument Science and Technology at Shanghai Jiao Tong University, Shanghai, China. His research interests include robotics and computer Vision.\n","date":1593561600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1593561600,"objectID":"296ffb07ffade55bcb1583aff8e5990c","permalink":"https://vision4robotics.github.io/authors/changjing-liu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/changjing-liu/","section":"authors","summary":"Changjing Liu received the B.E. degree in Mechanical Engineering from Tongji University, Shanghai, China. He is currently working toward the M.Sc. degree in Instrument Science and Technology at Shanghai Jiao Tong University, Shanghai, China. His research interests include robotics and computer Vision.","tags":null,"title":"Changjing Liu","type":"authors"},{"authors":["xiaoxiao-yang"],"categories":null,"content":"Xiaoxiao Yang received a B.E. degree in control science and engineering from Tongji University, Shanghai, China. He is currently working toward the M.Sc. degree in Automation at Shanghai Jiao Tong University, Shanghai, China. His research interests include robotics and visual object tracking.\n","date":1593561600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1593561600,"objectID":"683d6e977f4a6d54ea09b4b67d85888f","permalink":"https://vision4robotics.github.io/authors/xiaoxiao-yang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xiaoxiao-yang/","section":"authors","summary":"Xiaoxiao Yang received a B.E. degree in control science and engineering from Tongji University, Shanghai, China. He is currently working toward the M.Sc. degree in Automation at Shanghai Jiao Tong University, Shanghai, China. His research interests include robotics and visual object tracking.","tags":null,"title":"Xiaoxiao Yang","type":"authors"},{"authors":["ziyuan-huang"],"categories":null,"content":"Ziyuan Huang received his B.Eng. degree in vehicle engineering from Tongji University, Shanghai, China. He is currently pursuing PhD in Mechanical Engineering, National University of Singapore (NUS), Singapore. His research interests involve visual tracking for unmanned aerial vehicles and computer vision.\n","date":1593561600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1593561600,"objectID":"1d5ccc4aa1c5de52ed53972a45961a66","permalink":"https://vision4robotics.github.io/authors/ziyuan-huang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ziyuan-huang/","section":"authors","summary":"Ziyuan Huang received his B.Eng. degree in vehicle engineering from Tongji University, Shanghai, China. He is currently pursuing PhD in Mechanical Engineering, National University of Singapore (NUS), Singapore. His research interests involve visual tracking for unmanned aerial vehicles and computer vision.","tags":null,"title":"Ziyuan Huang","type":"authors"},{"authors":["yinqiang-zhang"],"categories":null,"content":"Yinqiang Zhang received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing M.Sc. degree in mechatronics and information technology, Technical University of Munich (TUM), Munich, Germany. His research interests include visual tracking and computer vision.\n","date":1586217600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1586217600,"objectID":"b2c3db1f8b9ba4f5d084ca76dfab0758","permalink":"https://vision4robotics.github.io/authors/yinqiang-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yinqiang-zhang/","section":"authors","summary":"Yinqiang Zhang received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing M.Sc. degree in mechatronics and information technology, Technical University of Munich (TUM), Munich, Germany. His research interests include visual tracking and computer vision.","tags":null,"title":"Yinqiang Zhang","type":"authors"},{"authors":["admin"],"categories":null,"content":" The vision4robotics group is a multidisciplinary research group at Tongji University. Our research interests focus on intelligent vision and control technologies for robotics.\nRecruit We are looking for new students to work on exciting AI-driven research and projects (with limited quota). Candidates with research experience in Deep Learning, Object Tracking/ Detection/ Segmentation/ Recognition will be considered with higher priority. Please send your CV if you have an interest.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://vision4robotics.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"The vision4robotics group is a multidisciplinary research group at Tongji University. Our research interests focus on intelligent vision and control technologies for robotics.\nRecruit We are looking for new students to work on exciting AI-driven research and projects (with limited quota). Candidates with research experience in Deep Learning, Object Tracking/ Detection/ Segmentation/ Recognition will be considered with higher priority. Please send your CV if you have an interest.","tags":null,"title":"Vision4robotics Group","type":"authors"},{"authors":["hao-chen"],"categories":null,"content":"Hao Chen received his B.Eng. degree in mechanical engineering from Chongqing University, Chongqing, China. He is now pursuing M.Sc. degree in mechanical engineering at Tongji University. His research interests include robotics and visual/lidar SLAM.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"7123babb776e1a2719ea30a1de6294ce","permalink":"https://vision4robotics.github.io/authors/hao-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/hao-chen/","section":"authors","summary":"Hao Chen received his B.Eng. degree in mechanical engineering from Chongqing University, Chongqing, China. He is now pursuing M.Sc. degree in mechanical engineering at Tongji University. His research interests include robotics and visual/lidar SLAM.","tags":null,"title":"Hao Chen","type":"authors"},{"authors":["haobo-zuo"],"categories":null,"content":"Haobo Zuo is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include UAV, artificial intelligence, and computer vision.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"0f0f6be7d2bd3b6e5f9b256966e4a5d3","permalink":"https://vision4robotics.github.io/authors/haobo-zuo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/haobo-zuo/","section":"authors","summary":"Haobo Zuo is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include UAV, artificial intelligence, and computer vision.","tags":null,"title":"Haobo Zuo","type":"authors"},{"authors":["jianqiao-lu"],"categories":null,"content":"Jianqiao Lu received his B.Eng. degree in vehicle engineering from Tongji University, Shanghai, China. He is currently pursuing PhD in Computer Science, HongKong University (HKU), China. His research interests include robotics motion planning, biorobotics control, and place recognition.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"816035b05c4c141078b704fe19a1c71e","permalink":"https://vision4robotics.github.io/authors/jianqiao-lu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jianqiao-lu/","section":"authors","summary":"Jianqiao Lu received his B.Eng. degree in vehicle engineering from Tongji University, Shanghai, China. He is currently pursuing PhD in Computer Science, HongKong University (HKU), China. His research interests include robotics motion planning, biorobotics control, and place recognition.","tags":null,"title":"Jianqiao Lu","type":"authors"},{"authors":["jilin-zhao"],"categories":null,"content":"Jilin Zhao received his B.Eng. degree in mechanical engineering in 2021 from Tongji University, Shanghai, China, where he is currently pursuing the M.Sc. degree. His research interests include visual object tracking, deep learning, and computer vision.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"702af82e1991df761b4f5893ce247ec8","permalink":"https://vision4robotics.github.io/authors/jilin-zhao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jilin-zhao/","section":"authors","summary":"Jilin Zhao received his B.Eng. degree in mechanical engineering in 2021 from Tongji University, Shanghai, China, where he is currently pursuing the M.Sc. degree. His research interests include visual object tracking, deep learning, and computer vision.","tags":null,"title":"Jilin Zhao","type":"authors"},{"authors":["kunhan-lu"],"categories":null,"content":"Kunhan Lu received his B.Eng. degree in mechanical engineering from Chongqing University, Chongqing, China. He is now pursuing the M.Sc. degree in mechanical engineering at Tongji University, Shanghai, China. His research interests include visual object tracking, deep learning, and computer vision.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"b49d63425a5e1f8655b7a44f15162a02","permalink":"https://vision4robotics.github.io/authors/kunhan-lu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/kunhan-lu/","section":"authors","summary":"Kunhan Lu received his B.Eng. degree in mechanical engineering from Chongqing University, Chongqing, China. He is now pursuing the M.Sc. degree in mechanical engineering at Tongji University, Shanghai, China. His research interests include visual object tracking, deep learning, and computer vision.","tags":null,"title":"Kunhan Lu","type":"authors"},{"authors":["kunhui-chen"],"categories":null,"content":"Kunhui Chen received his B. Eng. degree in mechanical and electronic engineering From Guangdong University of Technology, Guangzhou, China. He is currently pursuing M. Sc. degree in mechanical engineering in Tongji University, Shanghai, China. His research interests include visual measurement, object detection, and deep learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"86ad1f48c0fb1cfc4b202cebe4497480","permalink":"https://vision4robotics.github.io/authors/kunhui-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/kunhui-chen/","section":"authors","summary":"Kunhui Chen received his B. Eng. degree in mechanical and electronic engineering From Guangdong University of Technology, Guangzhou, China. He is currently pursuing M. Sc. degree in mechanical engineering in Tongji University, Shanghai, China. His research interests include visual measurement, object detection, and deep learning.","tags":null,"title":"Kunhui Chen","type":"authors"},{"authors":["liangliang-yao"],"categories":null,"content":"Liangliang Yao is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include artificial intelligence, computer vision, and deep learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"651994acd0d97920b2165a0a2e8fe696","permalink":"https://vision4robotics.github.io/authors/liangliang-yao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/liangliang-yao/","section":"authors","summary":"Liangliang Yao is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include artificial intelligence, computer vision, and deep learning.","tags":null,"title":"Liangliang Yao","type":"authors"},{"authors":["mutian-cai"],"categories":null,"content":"Mutian Cai is an undergraduate student at Tongji University and currently pursing B.Eng. degree in mechanical engineering. His research interests include robotics, UAV, computer vision, and artificial intelligence.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"35f90a22827fedac409d6cef7ca3daa2","permalink":"https://vision4robotics.github.io/authors/mutian-cai/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/mutian-cai/","section":"authors","summary":"Mutian Cai is an undergraduate student at Tongji University and currently pursing B.Eng. degree in mechanical engineering. His research interests include robotics, UAV, computer vision, and artificial intelligence.","tags":null,"title":"Mutian Cai","type":"authors"},{"authors":["shaoqiu-xu"],"categories":null,"content":"Shaoqiu Xu received a bachelor\u0026rsquo;s degree in mechanical engineering at Tongji University and is currently pursuing a Master\u0026rsquo;s degree at Shanghai Jiao Tong University. His research interests include object tracking and pose estimation.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"278d4354562b1001d3ddd31b27aaafc0","permalink":"https://vision4robotics.github.io/authors/shaoqiu-xu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/shaoqiu-xu/","section":"authors","summary":"Shaoqiu Xu received a bachelor\u0026rsquo;s degree in mechanical engineering at Tongji University and is currently pursuing a Master\u0026rsquo;s degree at Shanghai Jiao Tong University. His research interests include object tracking and pose estimation.","tags":null,"title":"Shaoqiu Xu","type":"authors"},{"authors":["teng-li"],"categories":null,"content":"Li Teng is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include unmanned aerial vehicle and computer Vision.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"17be8edc583687f02f1b94788fa21e9e","permalink":"https://vision4robotics.github.io/authors/teng-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/teng-li/","section":"authors","summary":"Li Teng is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include unmanned aerial vehicle and computer Vision.","tags":null,"title":"Teng Li","type":"authors"},{"authors":["weiyu-peng"],"categories":null,"content":"Weiyu Peng received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing M.Sc. degree in mechanical engineering at Tongji University, Shanghai, China. His research interests include deep learning, robotics, and visual object tracking.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"da145e55e892c20569410284ed79d85f","permalink":"https://vision4robotics.github.io/authors/weiyu-peng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/weiyu-peng/","section":"authors","summary":"Weiyu Peng received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing M.Sc. degree in mechanical engineering at Tongji University, Shanghai, China. His research interests include deep learning, robotics, and visual object tracking.","tags":null,"title":"Weiyu Peng","type":"authors"},{"authors":["xiangpeng-zeng"],"categories":null,"content":"Xiangpeng Zeng received his B.Eng. degree in material forming and control engineering from East China University of Science and Technology, Shanghai, China. He is currently pursuing M.Sc. degree in mechanical engineering at Tongji University, Shanghai, China.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"1cb22594f1d06998e93397a29f031698","permalink":"https://vision4robotics.github.io/authors/xiangpeng-zeng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xiangpeng-zeng/","section":"authors","summary":"Xiangpeng Zeng received his B.Eng. degree in material forming and control engineering from East China University of Science and Technology, Shanghai, China. He is currently pursuing M.Sc. degree in mechanical engineering at Tongji University, Shanghai, China.","tags":null,"title":"Xiangpeng Zeng","type":"authors"},{"authors":["xiaogang-yu"],"categories":null,"content":"Xiaogang Yu received his B. Eng. degree in mechanical engineering in 2017 from Shanghai University, Shanghai, China. He is now pursuing the M. Sc. degree in mechanical engineering at Tongji University, Shanghai, China. His research interests include robotics, visual object tracking, and unmanned aerial vehicle.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"a1946ddff3d62eba9496eb47aa29182e","permalink":"https://vision4robotics.github.io/authors/xiaogang-yu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xiaogang-yu/","section":"authors","summary":"Xiaogang Yu received his B. Eng. degree in mechanical engineering in 2017 from Shanghai University, Shanghai, China. He is now pursuing the M. Sc. degree in mechanical engineering at Tongji University, Shanghai, China. His research interests include robotics, visual object tracking, and unmanned aerial vehicle.","tags":null,"title":"Xiaogang Yu","type":"authors"},{"authors":["xining-lu"],"categories":null,"content":"Xining Lu is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include robotics, automation, and computer vision.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"80d229c4e1f167d5ed819f404e02318b","permalink":"https://vision4robotics.github.io/authors/xining-lu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xining-lu/","section":"authors","summary":"Xining Lu is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include robotics, automation, and computer vision.","tags":null,"title":"Xining Lu","type":"authors"},{"authors":["xinnan-yuan"],"categories":null,"content":"Xinnan Yuan is an undergraduate student at Tongji University and currently pursing B.Eng. degree in mechanical engineering. His research interests include robotics and visual object tracking.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"1c061ec3fba49c8f929b6323eb1b7566","permalink":"https://vision4robotics.github.io/authors/xinnan-yuan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xinnan-yuan/","section":"authors","summary":"Xinnan Yuan is an undergraduate student at Tongji University and currently pursing B.Eng. degree in mechanical engineering. His research interests include robotics and visual object tracking.","tags":null,"title":"Xinnan Yuan","type":"authors"},{"authors":["yihu-wang"],"categories":null,"content":"Yihu Wang received his B.Eng. degree in mechanical engineering in 2021 from Tongji University, Shanghai, China, where he is currently working toward the M.Sc. degree in mechanical engineering. His research interests include robotics, visual object tracking, and object detection.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"44f6f5f89fd816de69db2f36b1e38dba","permalink":"https://vision4robotics.github.io/authors/yihu-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yihu-wang/","section":"authors","summary":"Yihu Wang received his B.Eng. degree in mechanical engineering in 2021 from Tongji University, Shanghai, China, where he is currently working toward the M.Sc. degree in mechanical engineering. His research interests include robotics, visual object tracking, and object detection.","tags":null,"title":"Yihu Wang","type":"authors"},{"authors":["yiyong-sun"],"categories":null,"content":"Yiyong Sun is an incoming master student in Mechanical Engineering at the National University of Singapore (NUS) and currently pursuing his BEng degree in Mechanical Engineering at Tongji University. His research interests include robotics, visual object tracking for unmanned aerial vehicle (UAV), and machine learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"289a010c77c85364a2de7389ba94f586","permalink":"https://vision4robotics.github.io/authors/yiyong-sun/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yiyong-sun/","section":"authors","summary":"Yiyong Sun is an incoming master student in Mechanical Engineering at the National University of Singapore (NUS) and currently pursuing his BEng degree in Mechanical Engineering at Tongji University. His research interests include robotics, visual object tracking for unmanned aerial vehicle (UAV), and machine learning.","tags":null,"title":"Yiyong Sun","type":"authors"},{"authors":["yuchen-li"],"categories":null,"content":"Yuchen Li is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. Her research interests include visual object tracking and machine learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"133708bcf28a9fc87b36f6ed906c506a","permalink":"https://vision4robotics.github.io/authors/yuchen-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yuchen-li/","section":"authors","summary":"Yuchen Li is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. Her research interests include visual object tracking and machine learning.","tags":null,"title":"Yuchen Li","type":"authors"},{"authors":["yulin-li"],"categories":null,"content":"Yulin Li is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include robotics motion planning, biorobotics control, and place recognition.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"b1258af42bb4dfb5508c09da7b48273f","permalink":"https://vision4robotics.github.io/authors/yulin-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yulin-li/","section":"authors","summary":"Yulin Li is an undergraduate student at Tongji University and currently pursuing B.Eng. degree in mechanical engineering. His research interests include robotics motion planning, biorobotics control, and place recognition.","tags":null,"title":"Yulin Li","type":"authors"},{"authors":["zheng-shen"],"categories":null,"content":"Zheng Shen is currently a senior student at Tongji University, in pursuit of a B.Eng. degree in mechanical engineering with a specialization in mechatronics. His research interests involve control theory for unmanned aerial vehicles.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"96d70063e0299291f33c32adbd420931","permalink":"https://vision4robotics.github.io/authors/zheng-shen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zheng-shen/","section":"authors","summary":"Zheng Shen is currently a senior student at Tongji University, in pursuit of a B.Eng. degree in mechanical engineering with a specialization in mechatronics. His research interests involve control theory for unmanned aerial vehicles.","tags":null,"title":"Zheng Shen","type":"authors"},{"authors":null,"categories":null,"content":"","date":1632700800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632700800,"objectID":"a3ca2af2b6970d3faedb710a28839ba4","permalink":"https://vision4robotics.github.io/project/darktrack2021/","publishdate":"2021-09-27T00:00:00Z","relpermalink":"/project/darktrack2021/","section":"project","summary":"DarkTrack2021 is a nighttime tracking benchmark comprises 110 challenging sequences with 100K frames in total.","tags":["UAV dark tracking","Tracking benchmark"],"title":"DarkTrack2021","type":"project"},{"authors":null,"categories":null,"content":"","date":1632700800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632700800,"objectID":"7c217d7a0d9af29d165dd79e76d663fe","permalink":"https://vision4robotics.github.io/project/uamt100/","publishdate":"2021-09-27T00:00:00Z","relpermalink":"/project/uamt100/","section":"project","summary":"UAMT100 benchmark is built for UAM tracking method evaluation. It contains 100 image sequences recorded on a flying UAM platform.","tags":["UAM tracking","Tracking benchmark"],"title":"UAMT100","type":"project"},{"authors":["Changhong Fu","Jin Jin","Fangqiang Ding","Yiming Li","Geng Lu"],"categories":null,"content":" The main workflow of the SRECF tracker. \n","date":1632614400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632614400,"objectID":"b702659de440c31f69171b9c4255e5bc","permalink":"https://vision4robotics.github.io/publication/2021_tmm_srecf-tracker/","publishdate":"2021-09-26T00:00:00Z","relpermalink":"/publication/2021_tmm_srecf-tracker/","section":"publication","summary":"Traditional discriminative correlation filter (DCF) has received great popularity due to its high computational efficiency. However, the lightweight framework of DCF cannot promise robust performance when the tracker faces appearance variations within the background. These unpredictable appearance variations always distract the filter. Most existing DCF-based trackers either utilize deep convolutional features or incorporate additional constraints to elevate tracking robustness. Despite some improvements, both of them hamper the tracking speed and can only roughly alleviate the distractions of appearance variations. In this paper, a novel spatial reliability enhanced learning strategy is proposed to handle the problems aforementioned. By monitoring the variation of response produced in detection phase, a dynamic reliability map is generated to indicate the reliability of each background subregion. Then, label adjustment is conducted to repress the distractions of these unreliable areas. Compared with the conventional way of constraint where a new term is always added to realize the desired goal, label adjustment is simultaneously more efficient and effective. Moreover, to promise the accuracy and dependability of the reliability map, an adaptively updated response pool recording reliable historical response values is proposed. Extensive and exhaustive experiments on three challenging unmanned aerial vehicle (UAV) benchmarks, i.e., UAV123@10fps, DTB70 and UAVDT, which totally include 243 video sequences, validate the superiority of the proposed method against other state-of-the-art trackers and exhibit a remarkable generality in a variety of scenarios. Meanwhile, the tracking speed of 65.2 FPS on a cheap CPU makes it suitable for real-time UAV applications.","tags":["Visual tracking","Unmanned aerial vehicles","Correlation filters"],"title":"Spatial Reliability Enhanced Correlation Filter: An Efficient Approach for Real-Time UAV Tracking","type":"publication"},{"authors":["Ziang Cao","Changhong Fu","Junjie Ye","Bowen Li","Yiming Li"],"categories":null,"content":" Fig. 1 Overview of the HiFT tracker. Fig. 2 Framework of the hierarchical feature transformer. \n","date":1626912000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626912000,"objectID":"c9ab8c003b230e357d009edabed830f2","permalink":"https://vision4robotics.github.io/publication/2021_iccv_hift/","publishdate":"2019-07-22T00:00:00Z","relpermalink":"/publication/2021_iccv_hift/","section":"publication","summary":"Siamese-based visual tracking methods generally execute the classification and regression of the target object based on the similarity maps. However, existing works either solely employ a single map generated by the last convolutional layer which degrades the localization accuracy, or separately use multiple maps for decision making, introducing intractable computations for aerial mobile platforms. In this work, we propose an efficient and effective hierarchical feature transformer (HiFT) in Siamese tracking. Hierarchical similarity maps generated by multi-level convolutional layers are fed into a feature transformer network. Not only the global contextual information can be raised, facilitating the target search, but also our end-to-end architecture with the transformer can learn the inter-dependencies among multi-level features, and discover a tracking-tailored feature space with strong discriminability due to the interactive fusion of spatial (early layers) and semantics cues (deep layers). Comprehensive evaluations on aerial benchmarks have proven the effectiveness of HiFT, and the real-world tests on the aerial platform have validated its practicability and robustness with a real-time speed.","tags":["Siamese network","Real-time object tracking","Unmanned aerial vehicles","Transformer"],"title":"HiFT: Hierarchical Feature Transformer for Aerial Tracking","type":"publication"},{"authors":["Junjie Ye","Changhong Fu","Guangze Zheng","Ziang Cao","Bowen Li"],"categories":null,"content":" Tracking performance comparison in a typical dark scene with the proposed DarkLighter module activated (in red) or not (in pink).\n","date":1625011200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625011200,"objectID":"c98ba9d164d6dc005d7f59a9f3dff2f1","permalink":"https://vision4robotics.github.io/publication/2021_iros_darklighter/","publishdate":"2021-06-30T00:00:00Z","relpermalink":"/publication/2021_iros_darklighter/","section":"publication","summary":"Recent years have witnessed the fast evolution and promising performance of the convolutional neural network (CNN)-based trackers, which aim at imitating biological visual systems. However, current CNN-based trackers can hardly generalize well to low-light scenes that are commonly lacked in the existing training set. In indistinguishable night scenarios frequently encountered in unmanned aerial vehicle (UAV) tracking-based applications, the robustness of the state-of-the-art (SOTA) trackers drops significantly. To facilitate aerial tracking in the dark through a general fashion, this work proposes a low-light image enhancer namely DarkLighter, which dedicates to alleviate the impact of poor illumination and noise iteratively. A lightweight map estimation network, \\textit{i.e.}, ME-Net, is trained to efficiently estimate illumination maps and noise maps jointly. Experiments are conducted with several SOTA trackers on numerous UAV dark tracking scenes. Exhaustive evaluations demonstrate the reliability and universality of DarkLighter, with high efficiency. Moreover, DarkLighter has further been implemented on a typical UAV system. Real-world tests at night scenes have verified its practicability and dependability.","tags":["Low-light enhancement","Visual tracking","Unmanned aerial vehicles"],"title":"DarkLighter: Light Up the Darkness for UAV Tracking","type":"publication"},{"authors":["Fuling Lin","Changhong Fu","Yujie He","Weijiang Xiong","Fan Li"],"categories":null,"content":" Main difference between the proposed ReCF and SRDCF. \n","date":1625011200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625011200,"objectID":"2b243256a5532c4b7695ddfee9169404","permalink":"https://vision4robotics.github.io/publication/2021_tits_recf_tracker/","publishdate":"2021-06-29T00:00:00Z","relpermalink":"/publication/2021_tits_recf_tracker/","section":"publication","summary":"Object tracking is a fundamental task for the visual perception system on the intelligent unmanned aerial vehicle (UAV). The high efficiency of correlation filter (CF) based trackers has advanced the widespread development of online UAV object tracking. This kind of method can effectively train a filter to discriminate the target from the background. However, most CF-based methods require a fixed label function over all the previous samples, leading to over-fitting and filter degradation, especially in complex drone scenarios. To address this problem, a novel adaptive response reasoning approach is proposed for CF learning. It can leverage temporal information in filter training and significantly promote the robustness of the tracker. Specifically, the proposed response reasoning method goes beyond the standard response consistency requirement and constructs an auxiliary label of the current sample. Besides, it helps learn a generic relationship between the previous and current filters, thereby realizing self-regulated filter updating and enhancing the discriminability of the filter. Extensive experiments on four wellknown challenging UAV tracking benchmarks with 278 videos sequences show that the presented method yields superior results to 40 state-of-the-art trackers with real-time performance on a single CPU, which is suitable for UAV online tracking missions.","tags":["Real-time UAV tracking","Discriminative correlation filter","Adaptive response reasoning"],"title":"ReCF: Exploiting Response Reasoning for Correlation Filters in Real-Time UAV Tracking","type":"publication"},{"authors":["Ziang Cao","Changhong Fu","Junjie Ye","Bowen Li","Yiming Li"],"categories":null,"content":" The overview of the SiamAPN++ tracker.\n","date":1625011200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625011200,"objectID":"0c32adec2cf596405b8de13633f93db3","permalink":"https://vision4robotics.github.io/publication/2021_iros_siamapn++/","publishdate":"2021-06-30T00:00:00Z","relpermalink":"/publication/2021_iros_siamapn++/","section":"publication","summary":"Recently, the Siamese-based method has stood out from multitudinous tracking methods owing to its state-of-the-art (SOTA) performance. Nevertheless, due to various special challenges in UAV tracking, e.g., severe occlusion, and fast motion, most existing Siamese-based trackers hardly combine superior performance with high efficiency. To this concern, in this paper, a novel attentional Siamese tracker (SiamAPN++) is proposed for real-time UAV tracking. By virtue of the attention mechanism, the attentional aggregation network (AAN) is conducted with self-AAN and cross-AAN, raising the expression ability of features eventually. The former AAN aggregates and models the self-semantic interdependencies of the single feature map via spatial and channel dimensions. The latter aims to aggregate the cross-interdependencies of different semantic features including the location information of anchors. In addition, the dual features version of the anchor proposal network is proposed to raise the robustness of proposing anchors, increasing the perception ability to objects with various scales. Experiments on two well-known authoritative benchmarks are conducted, where SiamAPN++ outperforms its baseline SiamAPN and other SOTA trackers. Besides, real-world tests onboard a typical embedded platform demonstrate that SiamAPN++ achieves promising tracking results with real-time speed.","tags":["Siamese network","Visual tracking","Unmanned aerial vehicles"],"title":"SiamAPN++: Siamese Attentional Aggregation Network for Real-Time UAV Tracking","type":"publication"},{"authors":["Junjie Ye","Changhong Fu","Fuling Lin","Fangqiang Ding","Shan An","Geng Lu"],"categories":null,"content":" Overall flowchart of the proposed MRCF. \n","date":1621814400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621814400,"objectID":"2dc6df5d1fe73a4908c6864fe515e2ba","permalink":"https://vision4robotics.github.io/publication/2021_tie_mrcf_tracker/","publishdate":"2021-05-24T00:00:00Z","relpermalink":"/publication/2021_tie_mrcf_tracker/","section":"publication","summary":"As a sort of model-free tracking approach, discriminative correlation filter (DCF)-based trackers have shown prominent performance in unmanned aerial vehicle (UAV) tracking. Nevertheless, typical DCFs acquire all samples oriented to filter training merely from the current frame by cyclic shift operation in the spatial domain but ignore the consistency between samples across the timeline. The lack of temporal cues restricts the performance of DCFs under object appearance variations arising from object/UAV motion, scale variations, and viewpoint changes. Besides, many existing methods commonly neglect the channel discrepancy in object position estimation and generally treat all channels equally, thus limiting the further promotion of the tracking discriminability. To these concerns, this work proposes a novel tracking approach based on a multi-regularized correlation filter, i.e., MRCF tracker. By regularizing the deviation of responses and the reliability of channels, the tracker enables smooth response variations and adaptive channel weight distributions simultaneously, leading to favorable adaption to object appearance variations and enhancement of discriminability. Exhaustive experiments on five authoritative UAV-specific benchmarks validate the competitiveness and efficiency of MRCF against top-ranked trackers. Furthermore, we apply our proposed tracker to monocular UAV self-localization under air-ground robot coordination. Evaluations indicate the practicability of the presented method in UAV localization applications.","tags":["Unmanned aerial vehicle (UAV)","Model-free object tracking","Multi-regularized correlation filter","Vision-based UAV self-localization"],"title":"Multi-Regularized Correlation Filter for UAV Tracking and Self-Localization","type":"publication"},{"authors":["Changhong Fu","Ziang Cao","Yiming Li","Junjie Ye","Chen Feng"],"categories":null,"content":" The workflow of SiamAPN. It is composed of four subnetworks and two stages, i.e., feature extraction network, feature fusion network, anchor proposal network, and classification\u0026amp;regression network. Stage-1 includes feature extraction network and anchor proposal network (APN). Stage-2 contains feature fusion network and classification\u0026amp;regression network. \n","date":1620950400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620950400,"objectID":"f33f3154720a3c4a64b5aa1335e2e34f","permalink":"https://vision4robotics.github.io/publication/2021_tgrs_siamapn_ext/","publishdate":"2021-05-14T00:00:00Z","relpermalink":"/publication/2021_tgrs_siamapn_ext/","section":"publication","summary":"Object tracking approaches based on siamese network have demonstrated their huge potential in remote sensing field recently. Nevertheless, due to the limited computing resource of aerial platforms and special challenges in aerial tracking, most existing siamese-based methods can hardly meet the real-time and state-of-the-art performance at the same time. Consequently, a novel siamese-based method is proposed in this work for onboard real-time aerial tracking, i.e., SiamAPN. The proposed method is a no-prior two-stage method, i.e., stage-1 for proposing adaptive anchors to enhance the ability of object perception, stage-2 for fine-tuning the proposed anchors to obtain accurate results. Distinct from pre-defined fixed-sized anchors, our adaptive anchors are adapt automatically to accommodate the tracking object. Besides, the internal information of adaptive anchors is utilized to feedback SiamAPN for enhancing the object perception. Attributing to the feature fusion network, different semantic information is integrated, enriching the information flow. In the end, the regression and multi-classification operation refine the proposed anchors meticulously. Comprehensive evaluations on three well-known benchmarks have proven the superior performance of our approach. Moreover, to verify the practicability of the proposed method, SiamAPN is implemented in an onboard system. Real-world flight tests are conducted on aerial tracking specific scenarios, e.g., low resolution, fast motion, and long-term tracking, the results demonstrate the efficiency and accuracy of our approach, with a processing speed of over 30 frame/s. In addition, the image sequences in the real-world flight tests are collected and annotated as a new benchmark, i.e., UAVTrack112.","tags":["Real-time aerial tracking","Efficient siamese structure","Anchor proposal network","No-prior adaptive anchors","Onboard embedded processing","Real-world flight tests"],"title":"Onboard Real-Time Aerial Tracking with Efficient Siamese Anchor Proposal Network","type":"publication"},{"authors":["Changhong Fu","Bowen Li","Fangqiang Ding","Fuling Lin","Geng Lu"],"categories":null,"content":" General tracking structure of DCF-based methods onboard the UAV platform, which can be divided into the training stage, model update, and detection stage. \n","date":1617321600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617321600,"objectID":"ebb1040cd9d24405fedb8632d04ff414","permalink":"https://vision4robotics.github.io/publication/2021_grsm_cfmg/","publishdate":"2021-04-02T00:00:00Z","relpermalink":"/publication/2021_grsm_cfmg/","section":"publication","summary":"Aerial tracking, which has exhibited its omnipresent dedication and splendid performance, is one of the most active applications in the remote sensing field. Especially, unmanned aerial vehicle (UAV)-based remote sensing system, equipped with a visual tracking approach, has been widely used in aviation, navigation, agriculture, transportation, and public security, etc. As is mentioned above, the UAV-based aerial tracking platform has been gradually developed from research to practical application stage, reaching one of the main aerial remote sensing technologies in the future. However, due to the real-world onerous situations, e.g., harsh external challenges, the vibration of the UAV’s mechanical structure (especially under strong wind conditions), the maneuvering flight in complex environment, and the limited computation resources onboard, accuracy, robustness, and high efficiency are all crucial for the onboard tracking methods. Recently, the discriminative correlation filter (DCF)-based trackers have stood out for their high computational efficiency and appealing robustness on a single CPU, and have flourished in the UAV visual tracking community. In this work, the basic framework of the DCF-based trackers is firstly generalized, based on which, 23 state-of-the-art DCF-based trackers are orderly summarized according to their innovations for solving various issues. Besides, exhaustive and quantitative experiments have been extended on various prevailing UAV tracking benchmarks, i.e., UAV123, UAV123@10fps, UAV20L, UAVDT, DTB70, and VisDrone2019-SOT, which contain 371,903 frames in total. The experiments show the performance, verify the feasibility, and demonstrate the current challenges of DCF-based trackers onboard UAV tracking. Besides, this work also implements the brilliant DCF-based trackers on a typical CPU-based onboard PC to achieve real flight UAV tracking tests to further validate their real-time capabilities and robustness under challenging scenes. A concise summary of future research trends in the area of DCF-based methods for UAV tracking is further provided. Finally, comprehensive conclusions on the directions for future research are presented.","tags":["Real-time remote sensing","Unmanned aerial vehicle","Aerial object tracking","Discriminative correlation filter","Review and experimental evaluation"],"title":"Correlation Filters for Unmanned Aerial Vehicle-Based Aerial Tracking: A Review and Experimental Evaluation","type":"publication"},{"authors":["Bowen Li","Yiming Li","Junjie Ye","Changhong Fu","Hang Zhao"],"categories":null,"content":"  \n","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"8f69f97ecf740fb99d1a9fb280cd8166","permalink":"https://vision4robotics.github.io/publication/2021_arxiv_pvt/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/2021_arxiv_pvt/","section":"publication","summary":"As a crucial robotic perception capability, visual tracking has been intensively studied recently. In the real-world scenarios, the onboard processing time of the image streams inevitably leads to a discrepancy between the tracking results and the real-world states. However, existing visual tracking benchmarks commonly run the trackers offline and ignore such latency in the evaluation. In this work, we aim to deal with a more realistic problem of latency-aware tracking. The state-ofthe-art trackers are evaluated in the aerial scenarios with new metrics jointly assessing the tracking accuracy and efficiency. Moreover, a new predictive visual tracking baseline is developed to compensate for the latency stemming from the onboard computation. Our latency-aware benchmark can provide a more realistic evaluation of the trackers for the robotic applications. Besides, exhaustive experiments have proven the effectiveness of the proposed predictive visual tracking baseline approach. Our code is on https://github.com/vision4robotics/LAE-PVT-master.","tags":["Unmanned aerial vehicle","Visual object tracking","Latency-aware tracking"],"title":"Predictive Visual Tracking: A New Benchmark and Baseline Approach","type":"publication"},{"authors":["Bowen Li","Changhong Fu","Fangqiang Ding","Junjie Ye","Fuling Lin"],"categories":null,"content":" Overall framework of the proposed ADTrack. ADTrack includes 3 stages: pretreatment, training, and detection, which are marked out by boxes in different colors. Dual filters, i.e., context filter and target-focused filter, training and detection follow routes in different colors. It can be seen that the final response shaded noises in context response, which indicates the validity of proposed dual filter.\n\n","date":1614499200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614499200,"objectID":"1433737504ce4cad2e1d5e6586c8e05f","permalink":"https://vision4robotics.github.io/publication/2021_icra_adtrack/","publishdate":"2021-02-28T08:00:00Z","relpermalink":"/publication/2021_icra_adtrack/","section":"publication","summary":"Prior correlation filter (CF)-based tracking methods for unmanned aerial vehicles (UAVs) have virtually focused on tracking in the daytime. However, when the night falls, the trackers will encounter more harsh scenes, which can easily lead to tracking failure. In this regard, this work proposes a novel tracker with anti-dark function (ADTrack). The proposed method integrates an effcient and effective low-light image enhancer into a CF-based tracker. Besides, a target-aware mask is simultaneously generated by virtue of image illumination variation. The target-aware mask can be applied to jointly train a target-focused filter that assists the context filter for robust tracking. Specifically, ADTrack adopts dual regression, where the context filter and the target-focused filter restrict each other for dual filter learning. Exhaustive experiments are conducted on typical dark sceneries benchmark, consisting of 37 typical night sequences from authoritative benchmarks, i.e., UAVDark, and our newly constructed benchmark UAVDark70. The results have shown that ADTrack favorably outperforms other state-of-the-art trackers and achieves a real-time speed of 34 frames/s on a single CPU, thus greatly extending robust UAV tracking to night scenes.","tags":["Visual tracking","Unmanned aerial vehicles","Correlation filter"],"title":"ADTrack: Target-Aware Dual Filter Learning for Real-Time Anti-Dark UAV Tracking","type":"publication"},{"authors":["Guangze Zheng","Changhong Fu","Junjie Ye","Fuling Lin","Fangqiang Ding"],"categories":null,"content":" Tracking procedure of the proposed MSCF tracker. Dashed boxes denote the variables to be solved in the main regression. As MTF in the red box is generated from search region in frame k, it is applied to adjust the altitude value of the cruciform pedestal.\n\n","date":1614499200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614499200,"objectID":"2b51ed03d91fc1ed1d4cc28c2abf7736","permalink":"https://vision4robotics.github.io/publication/2021_icra_mscf_tracker/","publishdate":"2021-02-28T08:00:00Z","relpermalink":"/publication/2021_icra_mscf_tracker/","section":"publication","summary":"Unmanned aerial vehicle (UAV) based visual tracking has been confronted with numerous challenges, e.g., object motion and occlusion. These challenges generally bring about target appearance mutations and cause tracking failure. However, most prevalent discriminative correlation filter (DCF) based trackers are insensitive to target mutations due to a predefined label, which concentrates on merely the centre of the target. Meanwhile, appearance mutations incited by occlusion or similar objects commonly lead to inevitable learning of erroneous information. To cope with appearance mutations, this paper proposes a novel DCF-based method to enhance the sensitivity and resistance to mutations with an adaptive hybrid label, i.e., MSCF. The ideal label is optimized jointly with the correlation filter and remains consistent with the previous label. Meanwhile, a novel measurement of mutations called mutation threat factor (MTF) is applied to correct the label dynamically. Through the revision of label into hybrid shape, MSCF can demonstrate preferable adaptability during appearance mutations. Considerable experiments are conducted on widely used UAV benchmarks. Results manifest the performance of MSCF tracker surpassing other 26 state-ofthe-art DCF-based and deep-based trackers. With a real-time speed of ~ 38 frames/s, the proposed approach is sufficient for UAV tracking commissions.","tags":["Visual tracking","Unmanned aerial vehicles","Correlation filter"],"title":"Mutation Sensitive Correlation Filter for Real-Time UAV Tracking with Adaptive Hybrid Label","type":"publication"},{"authors":["Ran Duan","Changhong Fu","Kostas Alexis","Erdal Kayacan"],"categories":null,"content":"\nThe overview of SiamAPN tracker. It composes of four subnetworks, i.e., feature extraction network, feature fusion network, anchor proposal network (APN), and muti-classification®ression network.\n\n","date":1614499200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614499200,"objectID":"6b17c9d26a327a2e52573f21e997da82","permalink":"https://vision4robotics.github.io/publication/2021_icra_orcf/","publishdate":"2021-02-28T08:00:00Z","relpermalink":"/publication/2021_icra_orcf/","section":"publication","summary":"In this paper, we develop an online learning-based visual tracking framework that can optimize the target model and estimate the scale variation for object tracking. We propose a recommender-based tracker, which is capable of selecting the representative convolutional neural network (CNN) layers and feature maps autonomously. A sub-network is extracted from the pre-trained CNN to optimize the convolutional feature computing. In addition, the proposed recommender computes the weights of these layers and feature maps. A discriminative target percept of each recommended layer is reconstructed by the weighted sum of the recommended feature maps. Then the target model of the correlation filter is updated by the weighted sum of the target percepts. To deal with scale changes, we propose a spatio-temporal-based min-channel method to estimate the target size variation over time. Experimental results on 50 benchmark datasets and video data from rescue drone demonstrate that the proposed tracker is quite competitive with the state-of-the-art CNN-based trackers in terms of accuracy, scale adaptation, and robustness for UAV related application.","tags":["Visual tracking","Unmanned aerial vehicles","Correlation filter"],"title":"Online Recommendation-based Convolutional Features for Scale-Aware Visual Tracking","type":"publication"},{"authors":["Changhong Fu","Ziang Cao","Yiming Li","Junjie Ye","Chen Feng"],"categories":null,"content":"\nThe overview of SiamAPN tracker. It composes of four subnetworks, i.e., feature extraction network, feature fusion network, anchor proposal network (APN), and muti-classification®ression network.\n\n","date":1614499200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614499200,"objectID":"9e61feeea128ae22e4769a11f6af92fb","permalink":"https://vision4robotics.github.io/publication/2021_icra_siamapn/","publishdate":"2021-02-28T08:00:00Z","relpermalink":"/publication/2021_icra_siamapn/","section":"publication","summary":"In the domain of visual tracking, most deep learning-based trackers highlight the accuracy but casting aside efficiency, thereby impeding their real-world deployment on mobile platforms like the unmanned aerial vehicle (UAV). In this work, a novel two-stage siamese network-based method is proposed for aerial tracking, \\textit{i.e.}, stage-1 for high-quality anchor proposal generation, stage-2 for refining the anchor proposal. Different from anchor-based methods with numerous pre-defined fixed-sized anchors, our no-prior method can 1) make tracker robust and general to different objects with various sizes, especially to small, occluded, and fast-moving objects, under complex scenarios in light of the adaptive anchor generation, 2) make calculation feasible due to the substantial decrease of anchor numbers. In addition, compared to anchor-free methods, our framework has better performance owing to refinement at stage-2. Comprehensive experiments on three benchmarks have proven the state-of-the-art performance of our approach, with a speed of ~ 200 frames/s.","tags":["Visual tracking","Unmanned aerial vehicles","Anchor proposal network"],"title":"Siamese Anchor Proposal Network for High-Speed Aerial Tracking","type":"publication"},{"authors":["Bowen Li","Changhong Fu","Fangqiang Ding","Junjie Ye","Fuling Lin"],"categories":null,"content":" Pipeline of ADTrack. \n","date":1611446400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611446400,"objectID":"a1378502c275eba58375066b33f246bc","permalink":"https://vision4robotics.github.io/publication/2021_arxiv_adtrack/","publishdate":"2021-01-24T00:00:00Z","relpermalink":"/publication/2021_arxiv_adtrack/","section":"publication","summary":"Visual object tracking, which is representing a major interest in image processing field, has facilitated numerous real world applications. Among them, equipping unmanned aerial vehicle (UAV) with real time robust visual trackers for all day aerial maneuver, is currently attracting incremental attention and has remarkably broadened the scope of applications of object tracking. However, prior tracking methods have merely focused on robust tracking in the well-illuminated scenes, while ignoring trackers' capabilities to be deployed in the dark. In darkness, the conditions can be more complex and harsh, easily posing inferior robust tracking or even tracking failure. To this end, this work proposed a novel discriminative correlation filter based tracker with illumination adaptive and anti dark capability, namely ADTrack. ADTrack firstly exploits image illuminance information to enable adaptability of the model to the given light condition. Then, by virtue of an efficient and effective image enhancer, ADTrack carries out image pretreatment, where a target aware mask is generated. Benefiting from the mask, ADTrack aims to solve a dual regression problem where dual filters, i.e., the context filter and target focused filter, are trained with mutual constraint. Thus ADTrack is able to maintain continuously favorable performance in all-day conditions. Besides, this work also constructed one UAV nighttime tracking benchmark UAVDark135, comprising of more than 125k manually annotated frames, which is also very first UAV nighttime tracking benchmark. Exhaustive experiments are extended on authoritative daytime benchmarks, i.e., UAV123 10fps, DTB70, and the newly built dark benchmark UAVDark135, which have validated the superiority of ADTrack in both bright and dark conditions on a single CPU.","tags":["Unmanned aerial vehicle","Visual object tracking","Discriminative correlation filter","Dark tracking benchmark","Image illumination based mask","Dual regression model"],"title":"All-Day Object Tracking for Unmanned Aerial Vehicle","type":"publication"},{"authors":["Changhong Fu","Fangqiang Ding","Yiming Li","Jin Jin","Chen Feng"],"categories":null,"content":" The object tracking workflow of DR2Track. \n","date":1607990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607990400,"objectID":"cc005773b1cbcd8344c2bd2cf8744382","permalink":"https://vision4robotics.github.io/publication/2021_eaai_dr2track-tracker/","publishdate":"2020-12-15T00:00:00Z","relpermalink":"/publication/2021_eaai_dr2track-tracker/","section":"publication","summary":"With high efficiency and efficacy, the trackers based on the discriminative correlation filter have experienced rapid development in the field of unmanned aerial vehicle (UAV) over the past decade. In literature, these trackers aim at solving a regression problem in which the circulated samples are mapped into a Gaussian label for online filter training. However, the fixed target label for regression makes trackers lose adaptivity in uncertain tracking scenarios. One of the typical failure cases is that the distractors, e.g., background clutter, camouflage, and similar object, are prone to confuse these trackers. In this work, an efficient approach to instantly monitor the local maximums of the response map for discovering distractors automatically is proposed. In addition, the regression target is accordingly learned, i.e., the location possessing local maximum indicates latent distractor and thus should be repressed by reducing its target response value in filter training. Qualitative and quantitative experiments performed on three challenging well-known benchmarks demonstrate that the presented method not only outperforms the state-of-the-art handcrafted feature-based trackers but also exhibits comparable performance compared to deep learning-based approaches. Specifically, the presented tracker has phenomenal practicability in real-time UAV applications with an average speed of ∼50 frames per second on an affordable CPU.","tags":["Unmanned aerial vehicles","Visual object tracking","Discriminative correlation filter","Learning dynamic regression","Local maximums repression"],"title":"Learning Dynamic Regression with Automatic Distractor Repression for Real-Time UAV Tracking","type":"publication"},{"authors":null,"categories":null,"content":" UAVDark135 is the very first UAV dark tracking benchmark dedicated to providing a comprehensive evaluation of tracking performance at night.\nUAVDark135 consists of 135 sequences, most of which were shot by a standard UAV at night, including more than 125k manually annotated frames. The benchmark covers a wide range of scenes, e.g., road, ocean, street, highway, and lakeside, including a large number of objects, such as person, car, building, athlete, truck, and bike.\nThe benchmark is available here (password: axci).\nUAVDark135 Tracking Benchmark A. Platform and Statistics Standing as the first UAV dark tracking benchmark, the UAVDark135 contains totally 135 sequences captured by a standard UAV2 at night. The benchmark includes various tracking scenes, e.g., crossings, t-junctions, road, highway, and consists of different kinds of tracked objects like people, boat, bus, car, truck, athletes, house, etc. To extent the covered scenes, the benchmark also contains some sequences from YouTube, which were shot on the sea. The total frames, mean frames, maximum frames, and minimum frames of the benchmark are 125466, 929, 4571, and 216 respectively, making it suitable for large-scale evaluation. The videos are captured at a frame-rate of 30 frames/s (FPS), with the resolution of 1920×1080.\nB. Annotation The frames in UAVDark135 are all manually annotated, where a sequence is completely processed by the same annotator to ensure consistency. Since in some dark scenes the object is nearly invisible, annotation process is much more strenuous. After the first round, 5 professional annotators carefully checked the results and made revision for several rounds to reduce errors as much as possible in nearly 2 months.\nSince the boundary contour of the object is not obvious in the dark, the result boxes of the first annotation fluctuates in continuous image frames. However, the actual motion process of the object should be smooth. In these considerations, we record the original annotation every 5 frames for the sequence with extremely severe vibration, and the results of the remaining frames are obtained by linear interpolation, which is closer to the position and scale variation of the real object.\nC. Attributes For more details, please refer to our paper.\n","date":1604102400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604102400,"objectID":"2d77a9e9fdf8af223bcf640d5b739add","permalink":"https://vision4robotics.github.io/project/uavdark135/","publishdate":"2020-10-31T00:00:00Z","relpermalink":"/project/uavdark135/","section":"project","summary":"A pioneering UAV dark tracking benchmark consists of 135 videos with a variety of objects.","tags":["UAV dark tracking","Tracking benchmark"],"title":"UAVDark135","type":"project"},{"authors":["Changhong Fu","Junjie Ye","Juntao Xu","Yujie He","Fuling Lin"],"categories":null,"content":" Tracking procedure of the proposed IBRI tracker in the k-th frame. Historical interval responses are incorporated into the filter training phase after denoising by a novel disruptor-aware scheme based on response bucketing. \n","date":1602028800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602028800,"objectID":"06a2c27d3e614c9491702be79f0c1bcf","permalink":"https://vision4robotics.github.io/publication/2020_tgrs_ibri-tracker/","publishdate":"2020-10-07T00:00:00Z","relpermalink":"/publication/2020_tgrs_ibri-tracker/","section":"publication","summary":"Aerial object tracking approaches based on discriminative correlation filter (DCF) have attracted wide attention in the tracking community due to their impressive progress recently. Many studies introduce temporal regularization into the DCF-based framework to achieve a more robust appearance model and further enhance the tracking performance. However, existing temporal regularization approaches usually utilize the information of two consecutive frames, which are not robust enough due to limited information. Although some methods attempt to incorporate abundant training samples and generally improve the tracking performance, these improvements are at the expense of significantly increased computing consumption. Besides, most existing methods introduce historical information directly without denoising, which means background noises are also introduced into the filter training and may degrade the tracking accuracy. To tackle the drawbacks mentioned above, this work proposes a novel aerial object tracking approach to exploit disruptor-aware interval-based response inconsistency, i.e., IBRI tracker. The proposed method is able to incorporate historical interval information by utilizing responses in the filter training process, thereby obtaining a robust tracking performance while maintaining the real-time speed. Moreover, to reduce the disruptions caused by similar object, partial occlusion, and other challenging scenes, a novel disruptor-aware scheme based on response bucketing is introduced to detect the disruptor and enforce a spatial penalty for the disruptive area around the tracked object. Exhausted experiments on multiple well-known challenging aerial tracking benchmarks demonstrate the accuracy and robustness of the proposed IBRI tracker against other 35 state-of-the-art trackers. With a real-time speed of ~32 frames per second on a single CPU, the proposed approach can be applied for typical aerial platforms to achieve aerial visual object tracking efficiently.","tags":["Aerial object tracking","Discriminative correlation filter (DCF)","Temporal regularization","Historical frame information","Interval-based response inconsistency","Disruptor-aware bucketing"],"title":"Disruptor-Aware Interval-Based Response Inconsistency for Correlation Filters in Real-Time Aerial Tracking","type":"publication"},{"authors":["Fuling Lin","Changhong Fu","Yujie He","Fuyu Guo","Qian Tang"],"categories":null,"content":" A flowchart of the proposed TB-BiCF tracker. \n","date":1599436800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599436800,"objectID":"63a546ae784c6d754ee9ab3741c2dc20","permalink":"https://vision4robotics.github.io/publication/2020_tcsvt_tb-bicf-tracker/","publishdate":"2020-09-07T00:00:00Z","relpermalink":"/publication/2020_tcsvt_tb-bicf-tracker/","section":"publication","summary":"In the field of UAV object tracking, correlation filter based approaches have received lots of attention due to their computational efficiency. The methods learn filters by the ridge regression and generate response maps to distinguish the specified target from the background. An ideal filter can predict the object's position in a new frame, and in turn, can backtrack the object in the past frames. However, the neglect of tracking reversibility in most methods limits the potential of using inter-frame information to improve performance. In this work, a novel bidirectional incongruity-aware correlation filter is presented based on the nature of tracking reversibility. The proposed method incorporates the response-based bidirectional incongruity, which represents the gap between the filters' discriminative difference in the forward and backward tracking perspective caused by object appearance changes. It enables the filter not only to inherit the discriminability from previous filters but also to enhance the generalization capability to unpredictable appearance variations in upcoming frames. Moreover, a temporary block-based strategy is introduced to empower the filter accommodate more drastic object appearance changes and make more effective use of inter-frame information. Comprehensive experiments are conducted on three challenging UAV tracking benchmarks, including UAV123@10fps, DTB70, and UAVDT. Experimental results indicate that the proposed method has superior performance compared with the other 34 state-of-the-art trackers. Our approach permits real-time performance at ∼46.8 FPS on a single CPU and is suitable for UAV online tracking applications.","tags":["Aerial video analysis","Unmanned aerial vehicle","Visual object tracking","Discriminative correlation filter","Temporary block-based bidirectional incongruity"],"title":"Learning Temporary Block-Based Bidirectional Incongruity-Aware Correlation Filters for Efficient UAV Object Tracking","type":"publication"},{"authors":["Yiming Li","Changhong Fu","Fangqiang Ding","Ziyuan Huang","Jia Pan"],"categories":null,"content":" Overall structure of AMCF. \n","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"465de267b6953cf02f6e1d667dfc86a1","permalink":"https://vision4robotics.github.io/publication/2020_iros_amcf-tracker/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/2020_iros_amcf-tracker/","section":"publication","summary":"The outstanding computational efficiency of discriminative correlation filter (DCF) fades away with various complicated improvements. Previous appearances are also gradually forgotten due to the exponential decay of historical views in traditional appearance updating scheme of DCF framework, reducing the model's robustness. In this work, a novel tracker based on DCF framework is proposed to augment memory of previously appeared views while running at a real-time speed. Several historical views and the current view are simultaneously introduced in training to allow tracker to adapt to new appearances as well as memorize previous ones. A novel rapid compressive context learning is proposed to increase the discriminative ability of the filter efficiently. Substantial experiments on UAVDT and UAV123 datasets have validated that the proposed tracker performs competitively against other 26 top DCF and deep-based trackers with over 40 FPS on CPU.","tags":["Visual tracking","Unmanned aerial vehicles","Augmented memory"],"title":"Augmented Memory for Correlation Filters in Real-Time UAV Tracking","type":"publication"},{"authors":["Fangqiang Ding","Changhong Fu","Yiming Li","Jin Jin","Chen Feng"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"4c9cb98cabea629fb7a37ba71c5310a6","permalink":"https://vision4robotics.github.io/publication/2020_iros_jsar-tracker/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/2020_iros_jsar-tracker/","section":"publication","summary":"Current unmanned aerial vehicle (UAV) visual tracking algorithms are primarily limited with respect to\":\" (i) the kind of size variation they can deal with, (ii) the implementation speed which hardly meet the real-time requirement. In this work, a real-time UAV tracking algorithm with powerful size estimation ability is proposed. Specifically, the overall tracking task is allocated to two 2D filters\":\" (i) translation filter for location prediction in the space domain, (ii) size filter for scale and aspect ratio optimization in the size domain. Besides, an efficient two-stage re-detection strategy is introduced for long-term UAV tracking tasks. Large-scale experiments on four UAV benchmarks demonstrate the superiority of the presented method which has computation feasibility on a low-cost CPU.","tags":["Visual tracking","Unmanned aerial vehicles","Joint scale and aspect ratio optimization"],"title":"Automatic Failure Recovery and Re-Initialization for Online UAV Tracking with Joint Scale and Aspect Ratio Optimization","type":"publication"},{"authors":["Changhong Fu","Fangqiang Ding","Yiming Li","Jin Jin","Chen Feng"],"categories":null,"content":" Overall work-flow of DR^2Track. \n","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"8c2ab231206a62eb37d14ffed9bf43ff","permalink":"https://vision4robotics.github.io/publication/2020_iros_dr2track-tracker/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/2020_iros_dr2track-tracker/","section":"publication","summary":"Visual tracking has yielded promising applications with unmanned aerial vehicle (UAV). In literature, the advanced discriminative correlation filter (DCF) type trackers generally distinguish the foreground from the background with a learned regressor which regresses the implicit circulated samples into a fixed target label. However, the predefined and unchanged regression target results in low robustness and adaptivity to uncertain aerial tracking scenarios. In this work, we exploit the local extreme points of the response map generated in the detection phase to automatically locate current distractors. By repressing the response of distractors in the regressor learning, we can dynamically and adaptively alter our regression target to leverage the tracking robustness as well as adaptivity. Substantial experiments conducted on three challenging UAV benchmarks demonstrate both the excellent performance and extraordinary speed (∼50fps on a cheap CPU) of our tracker.","tags":["Visual tracking","Unmanned aerial vehicles","Distractor repressed dynamic regression"],"title":"DR^2Track: Towards Real-Time Visual Tracking for UAV via Distractor Repressed Dynamic Regression","type":"publication"},{"authors":["Changhong Fu","Xiaoxiao Yang","Fan Li","Changjing Liu","Peng Lu"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"5850528e163470bcdde66d49766e4b65","permalink":"https://vision4robotics.github.io/publication/2020_iros_cpcf-tracker/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/2020_iros_cpcf-tracker/","section":"publication","summary":"Correlation filter (CF) has proven its superb efficiency in visual tracking for unmanned aerial vehicle (UAV) applications. To enhance the temporal smoothness of the filter, many CF-based approaches introduce temporal regularization terms to penalize the variation of coefficients in an element-wise manner. However, this element-wise smoothness is stiff to the filter coefficients and can lead to poor adaptiveness in case of various challenges, e.g., fast motion and viewpoint changes, which frequently occur in the UAV tracking process. To tackle this issue, this work introduces a novel tracker with consistency pursed correlation filter, i.e., CPCF tracker. It is able to achieve flexible temporal smoothness by evaluating the similarity between two consecutive response maps with a correlation operation. By correlation operations, the consistency constraint allows for flexible variations in the response map without losing temporal smoothness. Besides, a dynamic label function is introduced to further increase adaptiveness in the training process. Considerable experiments on three challenging UAV tracking benchmarks verify that the presented tracker has surpassed the other 25 state-of-the-art trackers with satisfactory speed (~25 FPS) for real-time applications on a single CPU.","tags":["Visual tracking","Unmanned aerial vehicles","Consistency pursued"],"title":"Learning Consistency Pursued Correlation Filters for Real-Time UAV Tracking","type":"publication"},{"authors":["Yujie He","Changhong Fu","Fuling Lin","Yiming Li","Peng Lu"],"categories":null,"content":" The main workflow of the TACF tracker. \n","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"577e5083b7eff43d607721263bb9babd","permalink":"https://vision4robotics.github.io/publication/2020_iros_tacf-tracker/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/2020_iros_tacf-tracker/","section":"publication","summary":"Object tracking has been broadly applied in unmanned aerial vehicle (UAV) tasks in recent years. However, existing algorithms still face difficulties such as partial occlusion, clutter background, and other challenging visual factors. Inspired by the cutting-edge attention mechanisms, a new visual tracking framework leveraging multi-level visual attention to make full use of the information during tracking. Three primary attention, i.e., contextual attention, dimensional attention, and spatiotemporal attention, are integrated into the training and detection stages of correlation filter-based tracking pipeline. Therefore, the proposed tracker is equipped with robust discriminative power against challenging factors while maintains high operational efficiency in UAV scenarios. Quantitative and qualitative experiments on two well-known benchmark with 173 challenging UAV video sequences demonstrate the effectiveness of the proposed framework. The proposed tracking algorithm compares favorably against state-of-the-art methods, yielding 4.8% relative gain in UAVDT and 8.2% relative gain in UAV123@10fps against the baseline tracker while operating at the speed of ∼28 frames per second","tags":["Visual tracking","Unmanned aerial vehicles","Multi-level visual attention"],"title":"Towards Robust Visual Tracking for Unmanned Aerial Vehicle with Tri-Attentional Correlation Filters","type":"publication"},{"authors":["Changhong Fu","Juntao Xu","Fuling Lin","Fuyu Guo","Tingcong Liu","Zhijun Zhang"],"categories":null,"content":" Comparison between the tracking pipeline of the baseline SRDCF tracker and the proposed DRCF tracker. \n","date":1587772800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587772800,"objectID":"5af1766e7fd3898b385f4b0d358898af","permalink":"https://vision4robotics.github.io/publication/2020_tgrs_drcf-tracker/","publishdate":"2020-04-25T00:00:00Z","relpermalink":"/publication/2020_tgrs_drcf-tracker/","section":"publication","summary":"Spatial regularization has proven itself to be an effective method in terms of alleviating the boundary effect and boosting the performance of a discriminative correlation filter (DCF) in aerial visual object tracking. However, existing spatial regularization methods usually treat the regularizer as a supplementary term apart from the main regression and neglect to regularize the filter involved in the correlation operation. To address the aforementioned issue, this work introduces a novel object saliency-aware dual regularized correlation filter, i.e., DRCF. Specifically, the proposed DRCF tracker suggests a dual regularization strategy to directly regularize the filter involved with the correlation operation inside the core of the filter generating ridge regression. This allows the DRCF tracker to suppress the boundary effect and consequently enhance the performance of the tracker. Furthermore, an efficient method based on a saliency detection algorithm is employed to generate the dual regularizers dynamically and provide the regularizers with online adjusting ability. This enables the generated dynamic regularizers to automatically discern the object from the background and actively regularize the filter to accentuate the object during its unpredictable appearance changes. By the merits of the dual regularization strategy and the saliency-aware dynamical regularizers, the proposed DRCF tracker performs favorably in terms of suppressing the boundary effect, penalizing the irrelevant background noise coefficients and boosting the overall performance of the tracker. Exhaustive evaluations on 193 challenging video sequences from multiple well-known challenging aerial object tracking benchmarks validate the accuracy and robustness of the proposed DRCF tracker against 27 other state-of-the-art methods. Meanwhile, the proposed tracker can perform real-time aerial tracking applications on a single CPU with a sufficient speed of 38.4 frames per second.","tags":["Visual tracking","Unmanned aerial vehicles","Keyfilter"],"title":"Object Saliency-Aware Dual Regularized Correlation Filter for Real-Time Aerial Tracking","type":"publication"},{"authors":["Yiming Li","Changhong Fu","Ziyuan Huang","Yinqiang Zhang","Jia Pan"],"categories":null,"content":" Main work-flow of the KAOT tracker. \n","date":1586217600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586217600,"objectID":"40b96a8cb3779f78b5457015094a6642","permalink":"https://vision4robotics.github.io/publication/2020_tmm_kaot-tracker/","publishdate":"2020-04-07T00:00:00Z","relpermalink":"/publication/2020_tmm_kaot-tracker/","section":"publication","summary":"Correlation filter-based tracking has been widely applied in unmanned aerial vehicle (UAV) with high efficiency. However, it has two imperfections, i.e., boundary effect and filter corruption. Several methods enlarging the search area can mitigate boundary effect, yet introducing undesired background distraction. Existing frame-by-frame context learning strategies for repressing background distraction nevertheless lower the tracking speed. Inspired by keyframe-based simultaneous localization and mapping, keyfilter is proposed in visual tracking for the first time, in order to handle the above issues efficiently and effectively. Keyfilters generated by periodically selected keyframes learn the context intermittently and are used to restrain the learning of filters, so that 1) context awareness can be transmitted to all the filters via keyfilter restriction, and 2) filter corruption can be repressed. Compared to the state-of-the-art results, our tracker performs better on two challenging benchmarks, with enough speed for UAV real-time applications.","tags":["Visual tracking","Unmanned aerial vehicles","Keyfilter"],"title":"Intermittent Contextual Learning for Keyfilter-Aware UAV Object Tracking Using Deep Convolutional Feature","type":"publication"},{"authors":["Yiming Li","Changhong Fu","Fangqiang Ding","Ziyuan Huang","Geng Lu"],"categories":null,"content":" Central idea of AutoTrack.\n\n","date":1582502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582502400,"objectID":"f96897038cb4fd1e59d7ddbd39f3388f","permalink":"https://vision4robotics.github.io/publication/2020_cvpr_autotrack/","publishdate":"2020-02-24T00:00:00Z","relpermalink":"/publication/2020_cvpr_autotrack/","section":"publication","summary":"Most existing trackers based on discriminative correlation filters (DCF) try to introduce predefined regularization term to improve the learning of target objects, e.g., by sup-pressing background learning or by restricting change rate of correlation filters. However, predefined parameters intro-duce much effort in tuning them and they still fail to adapt to new situations that the designer did not think of. In this work, a novel approach is proposed to online automatically and adaptively learn spatio-temporal regularization term. Spatially local response map variation is introduced as spatial regularization to make DCF focus on the learning of trust-worthy parts of the object, and global response map variation determines the updating rate of the filter. Extensive experiments on four UAV benchmarks have proven the superiority of our method compared to the state-of-the-art CPU- and GPU-based trackers, with a speed of ∼60 frames per second running on a single CPU. Our tracker is additionally proposed to be applied in UAV localization. Considerable tests in the indoor practical scenarios have proven the effectiveness and versatility of our localization method. The code is available at .","tags":["Correlation filter","Real-time object tracking","Unmanned aerial vehicles","Localization by tracking"],"title":"AutoTrack: Towards High-Performance Visual Tracking for UAV with Automatic Spatio-Temporal Regularization","type":"publication"},{"authors":["Fuling Lin","Changhong Fu","Yujie He","Fuyu Guo","Qian Tang"],"categories":null,"content":"\n Comparison between discriminative correlation filter (DCF) and the proposed BiCF tracker\n\n","date":1579694400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579694400,"objectID":"6bb33494dc7d24845b77b9fe07c72ab1","permalink":"https://vision4robotics.github.io/publication/2020_icra_bicf-tracker/","publishdate":"2020-01-22T12:00:00Z","relpermalink":"/publication/2020_icra_bicf-tracker/","section":"publication","summary":"Correlation filters (CFs) have shown excellent performance in unmanned aerial vehicle (UAV) tracking scenarios due to their high computational efficiency. During the UAV tracking process, viewpoint variations are usually accompanied by changes in the object and background appearance, which poses a unique challenge to CF-based trackers. Since the appearance is gradually changing over time, an ideal tracker can not only forward predict the object position but also backtrack to locate its position in the previous frame. There exist response-based errors in the reversibility of the tracking process containing the information on the changes in appearance. However, some existing methods do not consider the forward and backward errors based on while using only the current training sample to learn the filter. For other ones, the applicants of considerable historical training samples impose a computational burden on the UAV. In this work, a novel bidirectional incongruity-aware correlation filter (BiCF) is proposed. By integrating the response-based bidirectional incongruity error into the CF, BiCF can efficiently learn the changes in appearance and suppress the inconsistent error. Extensive experiments on 243 challenging sequences from three UAV datasets (UAV123, UAVDT, and DTB70) are conducted to demonstrate that BiCF favorably outperforms other 25 state-of-the-art trackers and achieves a real-time speed of 45.4 FPS on a single CPU, which can be applied in UAV efficiently.","tags":["Visual tracking","Unmanned aerial vehicles","Bidirectional incongruity learning"],"title":"BiCF: Learning Bidirectional Incongruity-Aware Correlation Filter for Efficient UAV Object Tracking","type":"publication"},{"authors":["Fan Li","Changhong Fu","Fuling Lin","Yiming Li","Peng Lu"],"categories":null,"content":"\nComparison between our TSD tracker with the baseline BACF tracker\n\n","date":1579680000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579680000,"objectID":"6bba38c858ff464c41cde65aad3840f8","permalink":"https://vision4robotics.github.io/publication/2020_icra_tsd-tracker/","publishdate":"2020-01-22T08:00:00Z","relpermalink":"/publication/2020_icra_tsd-tracker/","section":"publication","summary":"Correlation filter (CF) has recently exhibited promising performance in visual object tracking for unmanned aerial vehicle (UAV). Such online learning method heavily depends on the quality of the training-set, yet complicated aerial scenarios like occlusion or out of view can reduce its reliability. In this work, a novel time slot-based distillation approach is proposed to efficiently and effectively optimize the training-set’s quality on the fly. A cooperative energy minimization function is established to score the historical samples adaptively. To accel-erate the scoring process, frames with high confident tracking results are employed as the keyframes to divide the tracking process into multiple time slots. After the establishment of a new slot, the weighted fusion of the previous samples generates one key-sample, in order to reduce the number of samples to be scored. Besides, when the current time slot exceeds the maximum frame number, which can be scored, the sample with the lowest score will be discarded. Consequently, the training-set can be efficiently and reliably distilled. Comprehensive tests on two well-known UAV benchmarks prove the effectiveness of our method with real-time speed on a single CPU.","tags":["Visual tracking","Unmanned aerial vehicles","Training-Set distillation"],"title":"Training-Set Distillation for Real-Time UAV Object Tracking","type":"publication"},{"authors":["Yiming Li","Changhong Fu","Ziyuan Huang","Yinqiang Zhang","Jia Pan"],"categories":null,"content":" Comparison between response maps of our tracker and baseline.\n\n","date":1579651200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579651200,"objectID":"3bcd6c1a928b6f63c32bf6c64f38f25b","permalink":"https://vision4robotics.github.io/publication/2020_icra_kaot-tracker/","publishdate":"2020-01-22T00:00:00Z","relpermalink":"/publication/2020_icra_kaot-tracker/","section":"publication","summary":"Correlation filter-based tracking has been widely applied in unmanned aerial vehicle (UAV) with high efficiency. However, it has two imperfections, i.e., boundary effect and filter corruption. Several methods enlarging the search area can mitigate boundary effect, yet introducing undesired background distraction. Existing frame-by-frame context learning strategies for repressing background distraction nevertheless lower the tracking speed. Inspired by keyframe-based simultaneous localization and mapping, keyfilter is proposed in visual tracking for the first time, in order to handle the above issues efficiently and effectively. Keyfilters generated by periodically selected keyframes learn the context intermittently and are used to restrain the learning of filters, so that 1) context awareness can be transmitted to all the filters via keyfilter restriction, and 2) filter corruption can be repressed. Compared to the state-of-the-art results, our tracker performs better on two challenging benchmarks, with enough speed for UAV real-time applications.","tags":["Visual tracking","Unmanned aerial vehicles","Keyfilter"],"title":"Keyfilter-Aware Real-Time UAV Object Tracking","type":"publication"},{"authors":["Changhong Fu","Yujie He","Fuling Lin","Weijiang Xiong"],"categories":null,"content":"\nMain structure of the proposed MKCT-Tracker\n\n","date":1575504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575504000,"objectID":"aa813cef24765cfa64a8db4422313184","permalink":"https://vision4robotics.github.io/publication/2020_ncaa_mkct-tracker/","publishdate":"2020-01-06T00:00:00Z","relpermalink":"/publication/2020_ncaa_mkct-tracker/","section":"publication","summary":"In recent years, the correlation filter (CF)-based method has significantly advanced in the tracking for unmanned aerial vehicles (UAV). As the core component of most trackers, CF is a discriminative classifier to distinguish the object from the surrounding environment. However, the poor representation of the object and lack of contextual information have restricted the tracker to gain better performance. In this work, a robust framework with multi-kernelized correlators is proposed to improve robustness and accuracy simultaneously. Both convolutional features extracted from the neural network and hand-crafted features are employed to enhance expressions for object appearances. Then, the adaptive context analysis strategy helps filters to effectively learn the surrounding information by introducing context patches with the GMSD index. In the training stage, multiple dynamic filters with time-attenuated factors are introduced to avoid tracking failure caused by dramatic appearance changes. The response maps corresponding to different features are finally fused before the novel resolution enhancement operation to increase distinguishing capability. As a result, the optimization problem is reformulated, and a closed-form solution for the proposed framework can be obtained in the kernel space. Extensive experiments on 100 challenging UAV tracking sequences demonstrate the proposed tracker outperforms other 23 state-of-the-art trackers and can effectively handle unexpected appearance variations under the complex and constantly changing working conditions.","tags":["Visual tracking","Unmanned aerial vehicles","Multi-kernelized correlators","Adaptive context analysis","Dynamic weighted filters"],"title":"Robust Multi-Kernelized Correlators for UAV Tracking with Adaptive Context Analysis and Dynamic Weighted Filters","type":"publication"},{"authors":["Changhong Fu","Weijiang Xiong","Fuling Lin","Yufeng Yue"],"categories":null,"content":"\nFig. 1 Main workflow of the proposed SASR tracker.\n\n","date":1569801600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569801600,"objectID":"d2d81b87c37cfff0a97221e87a865c7a","permalink":"https://vision4robotics.github.io/publication/2019_signal_processing_sasr-tracker/","publishdate":"2019-09-30T00:00:00Z","relpermalink":"/publication/2019_signal_processing_sasr-tracker/","section":"publication","summary":"The great advance of visual object tracking has provided unmanned aerial vehicle (UAV) with intriguing capability for various practical applications. With promising performance and efficiency, discriminative correlation filter-based trackers have drawn great attention and undergone remarkable progress. However, background interference and boundary effect remain two thorny problems. In this paper, a surrounding-aware tracker with selective spatial regularization (SASR) is presented. SASR tracker extracts surrounding samples according to the size and shape of the object in order to utilize context and maintain the integrality of the object. Additionally, a selective spatial regularizer is introduced to address boundary effect. Central coefficients in the filter are evenly regularized to preserve valid information from the object. While the others are penalized according to their spatial location. Under the framework of SASR tracker, surrounding information and selective spatial regularization prove to be complementary to each other, which actually did not draw much attention before. They managed to improve not only the robustness against various distractions in the surrounding but also the flexibility to catch up with frequent appearance change of the object. Qualitative evaluation and quantitative experiments on challenging UAV tracking sequences have shown that SASR tracker has performed favorably against 23 state-of-the-art trackers.","tags":["Unmanned aerial vehicle (UAV)","Visual object tracking","Discriminative correlation filter","Surrounding information","Selective spatial regularization"],"title":"Surrounding-Aware Correlation Filter for UAV Tracking with Selective Spatial Regularization","type":"publication"},{"authors":["Ziyuan Huang","Changhong Fu","Yiming Li","Fuling Lin","Peng Lu"],"categories":null,"content":"\nFig. 1 Comparison between background-aware correlation filter (BACF) and the proposed ARCF tracker.\nFig. 2 Main structure of the proposed ARCF tracker.\n\n","date":1563753600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563753600,"objectID":"f83555e1f1f71a6d7b37896ad8adbdd5","permalink":"https://vision4robotics.github.io/publication/2019_iccv_arcf-tracker/","publishdate":"2019-07-22T00:00:00Z","relpermalink":"/publication/2019_iccv_arcf-tracker/","section":"publication","summary":"Traditional framework of discriminative correlation filters (DCF) is often subject to undesired boundary effects. Several approaches to enlarge search regions have been already proposed in the past years to make up for this shortcoming. However, with excessive background information, more background noises are also introduced and the discriminative filter is prone to learn from the ambiance rather than the object. This situation, along with appearance changes of objects caused by full/partial occlusion, illumination variation, and other reasons has made it more likely to have aberrances in the detection process, which could substantially degrade the credibility of its result. Therefore, in this work, a novel approach to repress the aberrances happening during the detection process is proposed, i.e., aberrance repressed correlation filter (ARCF). By enforcing restriction to the rate of alteration in response maps generated in the detection phase, the ARCF tracker can evidently suppress aberrances and is thus more robust and accurate to track objects. Considerable experiments are conducted on different UAV datasets to perform object tracking from an aerial view, i.e., UAV123, UAVDT, and DTB70, with 243 challenging image sequences containing over 90K frames to verify the performance of the ARCF tracker and it has proven itself to have outperformed other 20 state-of-the-art trackers based on DCF and deep-based frameworks with sufficient speed for real-time applications.","tags":["Correlation filter","Real-time object tracking","Unmanned aerial vehicles","Multi-Frame Consensus Veriﬁcation"],"title":"Learning Aberrance Repressed Correlation Filters for Real-Time UAV Tracking","type":"publication"},{"authors":["Changhong Fu","Ziyuan Huang","Yiming Li","Ran Duan","Peng Lu"],"categories":null,"content":"\nFig. 1 Main structure of the proposed tracking approach.\n\n","date":1561334400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561334400,"objectID":"261d6e3cb4942a75c4083e904f782650","permalink":"https://vision4robotics.github.io/publication/2019_iros_bevt-tracker/","publishdate":"2019-06-24T00:00:00Z","relpermalink":"/publication/2019_iros_bevt-tracker/","section":"publication","summary":"Due to implicitly introduced periodic shifting of limited searching area, visual object tracking using correlation filters often has to confront undesired boundary effect. As boundary effect severely degrade the quality of object model, it has made it a challenging task for unmanned aerial vehicles (UAV) to perform robust and accurate object following. Traditional hand-crafted features are also not precise and robust enough to describe the object in the viewing point of UAV. In this work, a novel tracker with online enhanced background learning is specifically proposed to tackle boundary effects. Real background samples are densely extracted to learn as well as update correlation filters. Spatial penalization is introduced to offset the noise introduced by exceedingly more background information so that a more accurate appearance model can be established. Meanwhile, convolutional features are extracted to provide a more comprehensive representation of the object. In order to mitigate changes of objects' appearances, multi-frame technique is applied to learn an ideal response map and verify the generated one in each frame. Exhaustive experiments were conducted on 100 challenging UAV image sequences and the proposed tracker has achieved state-of-the-art performance.","tags":["Correlation filter","Onject tracking","Unmanned aerial vehicles","Multi-Frame Consensus Veriﬁcation"],"title":"Boundary Effect-Aware Visual Tracking for UAV with Online Enhanced Background Learning and Multi-Frame Consensus Veriﬁcation","type":"publication"},{"authors":["Changhong Fu","Yinqiang Zhang","Ziyuan Huang","Ran Duan","Zongwu Xie"],"categories":null,"content":"\nFig. 1 Main structure of the proposed tracking approach.\n\n","date":1560211200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560211200,"objectID":"d52b309a8d46898dba500a69ea2a9fdd","permalink":"https://vision4robotics.github.io/publication/2019_ieee_access_pbbat-tracker/","publishdate":"2019-06-11T00:00:00Z","relpermalink":"/publication/2019_ieee_access_pbbat-tracker/","section":"publication","summary":"In recent years, visual tracking is a challenging task in UAV applications. The standard correlation filter (CF) has been extensively applied for UAV object tracking. However, the CF-based tracker severely suffers from boundary effects and cannot effectively cope with object occlusion, which results in suboptimal performance. Besides, it is still a tough task to obtain an appearance model precisely with hand-crafted features. In this paper, a novel part-based tracker is proposed for the UAV. With successive cropping operations, the tracking object is separated into several parts. More specially, background-aware correlation filters with different cropping matrices are applied. To estimate the translation and scale variation of the tracking object, a structure comparison, and a Bayesian inference approach are proposed, which jointly achieve a coarse-to-fine strategy. Moreover, an adaptive mechanism is used to update the local appearance model of each part with a Gaussian process regression method. To construct a better appearance model, features extracted from the convolutional neural network are utilized instead of hand-crafted features. Through extensive experiments, the proposed tracker reaches competitive performance on 123 challenging UAV image sequences and outperforms other 20 popular state-of-the-art visual trackers in terms of overall performance and different challenging attributes.","tags":["Visual tracking","Unmanned aerial vehicle (UAV)"],"title":"Part-Based Background-Aware Tracking for UAV with Convolutional Features","type":"publication"},{"authors":["Guang Chen","Shu Liu","Kejia Ren","Zhongnan Qu","Changhong Fu","Gereon Hinz","Alois Knoll"],"categories":null,"content":"![SSIM-WMIL_workflow](featured.jpg) Fig. 1 The closed-loop control structure for the long-term navigation of the quadrotor UAV in real-time application.  -- ","date":1560124800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560124800,"objectID":"67e10887ad282922fd5dee0e7a0e3c87","permalink":"https://vision4robotics.github.io/publication/2019_iet_intell_transp_syst/","publishdate":"2019-06-10T00:00:00Z","relpermalink":"/publication/2019_iet_intell_transp_syst/","section":"publication","summary":"Advanced communication technology of IoT era enables a heterogeneous connectivity where mobile devices broadcast information to everything. Previous short-range on-board sensor perception system attached to moblie applications such as robots and vehicles could be transferred to long-range mobilesensing perception system, which can be used as part of a more extensive intelligent system surveilling real-time state of the environment. However, the mobile sensing perception brings new challenges for how to efficiently analyze and intelligently interpret the deluge of IoT data in mission-critical services. In this article, we model the challenges as latency, packet loss and measurement noise which severely deteriorate the reliability and quality of IoT data. We integrate the artificial intelligence into IoT to tackle these challenges. We propose a novel architecture that leverages recurrent neural networks (RNN) and Kalman filtering to anticipate motions and interactions between objects. The basic idea is to learn environment dynamics by recurrent networks. To improve the robustness of IoT communication, we use the idea of Kalman filtering and deploy a prediction and correction step. In this way, the architecture learns to develop a biased belief between prediction and measurement in the different situation. We demonstrate our approach with synthetic and real-world datasets with noise that mimics the challenges of IoT communications. Our method brings a new level of IoT intelligence. It is also lightweight compared to other state-of-theart convolutional recurrent architecture and is ideally suitable for the resource-limited mobile applications","tags":["Recurrent neural network","Internet of things","Kalman filtering","Convolutional LSTM","Factor graph"],"title":"Deep Anticipation: Lightweight Intelligent Mobile Sensing for Unmanned Vehicles in IoT by Recurrent Architecture","type":"publication"},{"authors":["Andriy Sarabakha","Changhong Fu","Erdal Kayacan"],"categories":null,"content":"![SSIM-WMIL_workflow](featured.jpg) Fig. 1 The closed-loop control structure for the long-term navigation of the quadrotor UAV in real-time application.  -- ","date":1557446400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557446400,"objectID":"8ce82b86a51519a1e30d70a72724343f","permalink":"https://vision4robotics.github.io/publication/2019_appl_soft_comput/","publishdate":"2019-05-10T00:00:00Z","relpermalink":"/publication/2019_appl_soft_comput/","section":"publication","summary":"Although a considerable amount of effort has been put in to show that fuzzy logic controllers have exceptional capabilities of dealing with uncertainty, there are still noteworthy concerns, e.g., the design of fuzzy logic controllers is an arduous task due to the lack of closed-form input–output relationships which is a limitation to interpretability of these controllers. The role of design parameters in fuzzy logic controllers, such as position, shape, and height of membership functions, is not straightforward. Motivated by the fact that the availability of an interpretable relationship from input to output will simplify the design procedure of fuzzy logic controllers, the main aims in this work are derive fuzzy mappings for both type-1 and interval type-2 fuzzy logic controllers, analyse them, and eventually benefit from such a nonlinear mapping to design fuzzy logic controllers. Thereafter, simulation and real-time experimental results support the presented theoretical findings.","tags":["Type-1 fuzzy logic controllers","Interval type-2 fuzzy logic controllers","Fuzzy mapping","Aerial robotics","Unmanned aerial vehicles"],"title":"Intuit Before Tuning: Type-1 and Type-2 Fuzzy Logic Controllers","type":"publication"},{"authors":["Changhong Fu","Fuling Lin","Yiming Li","Guang Chen"],"categories":null,"content":"\nFig. 1 Main structure of the proposed tracking approach.\n\n","date":1551830400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551830400,"objectID":"9c89d8499279bdf9ef669630a3a66cf0","permalink":"https://vision4robotics.github.io/publication/2019_remote_sens_omfl-tracker/","publishdate":"2019-03-06T00:00:00Z","relpermalink":"/publication/2019_remote_sens_omfl-tracker/","section":"publication","summary":"In this paper, a novel online learning-based tracker is presented for the unmanned aerial vehicle (UAV) in different types of tracking applications.","tags":["Visual tracking","Unmanned aerial vehicle (UAV)","Background-aware correlation filter","Online multi-feature learning","Peak-to-sidelobe ratio (PSR)","Response map fusion"],"title":"Correlation Filter-Based Visual Tracking for UAV with Online Multi-Feature Learning","type":"publication"},{"authors":["Changhong Fu","Ran Duan","Erdal Kayacan"],"categories":null,"content":"\nFig. 1 Main structure of the proposed tracking approach.\n\n","date":1546041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546041600,"objectID":"cd794bb37b2cab0f1083e488a11e406a","permalink":"https://vision4robotics.github.io/publication/2019_inf_sci_ssim-wmil/","publishdate":"2018-12-29T00:00:00Z","relpermalink":"/publication/2019_inf_sci_ssim-wmil/","section":"publication","summary":"This paper presents an online adaptive tracker, which employs a novel weighted multiple instance learning (WMIL) approach.In the proposed tracker, both positive and negative sample importances are integrated into an online learning mechanism for improving tracking performance in challenging environments. The sample importance is computed based on a new measure, i.e., structural similarity (SSIM), instead of using the Euclidean distance. Moreover, a novel bag probability function, which adopts both positive and negative weighted instance probabilities, is designed. Furthermore, a novel efficient weak classifier selection solution is developed for the proposed tracker. Qualitative and quantitative experiments on 30 challenging image sequences show that the novel tracking algorithm, i.e., SSIM-WMIL tracker, performs favorably against the MIL and WMIL counterparts as well as other 13 recently-proposed state-of-the-art trackers in terms of accuracy, robustness and efficiency. In addition, the negative sample importance can be used to enhance the multiple instance learning, and the SSIM-based approach is capable of improving the multiple instance learning performance for object tracking when compared to the Euclidean distance-based method.","tags":["Visual tracking","Multiple instance learning","Structural similarity","Negative sample importance","Bag probability function","Weak classifier selection"],"title":"Visual Tracking With Online Structural Similarity-Based Weighted Multiple Instance Learning","type":"publication"},{"authors":["Changhong Fu","Yinqiang Zhang","Ran Duan","Zongwu Xie"],"categories":null,"content":"\nFig. 1 Main structure of the proposed tracking approach.\n\n","date":1539820800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539820800,"objectID":"0930f45b6aefa5864f0fe417409c6351","permalink":"https://vision4robotics.github.io/publication/2018_robio_spbacf-tracker/","publishdate":"2018-10-18T00:00:00Z","relpermalink":"/publication/2018_robio_spbacf-tracker/","section":"publication","summary":"Robust visual tracking for the unmanned aerial vehicle (UAV) is a challenging task in different types of civilian UAV applications. Although the classical correlation filter (CF) has been widely applied for UAV object tracking, the background of the object is not learned in the classical CF. In addition, the classical CF cannot estimate the object scale changes, and it is not able to cope with object occlusion effectively. Part-based tracking approach is often used for the visual tracker to solve the occlusion issue. However, its real-time performance for the UAV cannot be achieved due to the high cost of object appearance updating. In this paper, a novel robust visual tracker is presented for the UAV. The object is initially divided into multiple parts, and different background-aware correlation filters are applied for these divided object parts, respectively. An efficient coarse-to-fine strategy with structure comparison and Bayesian inference approach is proposed to locate object and estimate the object scale changes. In addition, an adaptive threshold is presented to update each local appearance model with a Gaussian process regression method. Qualitative and quantitative tests show that the presented visual tracking algorithm reaches real-time performance (i.e., more than twenty frames per second) on an i7 processor with 640×360 image resolution, and performs favorably against the most popular state-of-the-art visual trackers in terms of robustness and accuracy. To the best of our knowledge, it is the first time that this novel scalable part-based visual tracker is presented, and applied for the UAV tracking applications.","tags":["Correlation filter","Onject tracking","Unmanned aerial vehicles","Real-time systems"],"title":"Robust Scalable Part-Based Visual Tracking for UAV with Background-Aware Correlation Filter","type":"publication"},{"authors":["Changhong Fu","Andriy Sarabakha","Erdal Kayacan","Christian Wagner","Robert John","Jon Garibaldi"],"categories":null,"content":"\nFig. 1 The closed-loop control structure for the long-term navigation of the quadrotor UAV in real-time application.\n\n","date":1519776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519776000,"objectID":"d6e3c6233bd65d3fee4a94c25879af84","permalink":"https://vision4robotics.github.io/publication/2018_tmech/","publishdate":"2018-02-28T00:00:00Z","relpermalink":"/publication/2018_tmech/","section":"publication","summary":"Input uncertainty, e.g., noise on the on-board camera and inertial measurement unit, in vision-based control of unmanned aerial vehicles (UAVs) is an inevitable problem. In order to handle input uncertainties as well as further analyze the interaction between the input and the antecedent fuzzy sets (FSs) of nonsingleton fuzzy logic controllers (NSFLCs), an input uncertainty sensitivity enhanced NSFLC has been developed in robot operating system using the C++ programming language. Based on recent advances in nonsingleton inference, the centroid of the intersection of the input and antecedent FSs (Cen-NSFLC) is utilized to calculate the firing strength of each rule instead of the maximum of the intersection used in traditional NSFLC (Tra-NSFLC). An 8-shaped trajectory, consisting of straight and curved lines, is used for the real-time validation of the proposed controllers for a trajectory following problem. An accurate monocular keyframe-based visual-inertial simultaneous localization and mapping (SLAM) approach is used to estimate the position of the quadrotor UAV in GPS-denied unknown environments. The performance of the Cen-NSFLC is compared with a conventional proportional-integral derivative (PID) controller, a singleton FLC and a Tra-NSFLC. All controllers are evaluated for different flight speeds, thus introducing different levels of uncertainty into the control problem. Visual-inertial SLAM-based real-time quadrotor UAV flight tests demonstrate that not only does the Cen-NSFLC achieve the best control performance among the four controllers, but it also shows better control performance when compared to their singleton counterparts. Considering the bias in the use of model-based controllers, e.g., PID, for the control of UAVs, this paper advocates an alternative method, namely Cen-NSFLCs, in uncertain working environments.","tags":["Fuzzy logic controller (FLC)","input uncertainty sensitivity enhanced nonsingleton FLC (NSFLC)","Monocular visual-inertial simultaneous localization and mapping (SLAM)","Unmanned aerial vehicle (UAV)"],"title":"Input Uncertainty Sensitivity Enhanced Nonsingleton Fuzzy Logic Controllers for Long-Term Navigation of Quadrotor UAVs","type":"publication"}]