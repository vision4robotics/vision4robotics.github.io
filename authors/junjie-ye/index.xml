<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Vision4robotics</title>
    <link>https://vision4robotics.github.io/authors/junjie-ye/</link>
    <description>Recent content on Vision4robotics</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>Vision4robotics &amp;copy; 2019 - 2020</copyright>
    <lastBuildDate>Fri, 14 May 2021 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="https://vision4robotics.github.io/authors/junjie-ye/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Onboard Real-Time Aerial Tracking with Efficient Siamese Anchor Proposal Network</title>
      <link>https://vision4robotics.github.io/publication/2021_tgrs_siamapn_ext/</link>
      <pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://vision4robotics.github.io/publication/2021_tgrs_siamapn_ext/</guid>
      <description>&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;featured.jpg&#34; alt=&#34;SiamAPN_workflow&#34; /&gt;
&lt;small&gt;The workflow of SiamAPN. It is composed of four subnetworks and two stages, i.e., feature extraction network, feature fusion network, anchor proposal network, and classification&amp;amp;regression network. Stage-1 includes feature extraction network and anchor proposal network (APN). Stage-2 contains feature fusion network and classification&amp;amp;regression network.&lt;/small&gt;
&lt;/center&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Predictive Visual Tracking: A New Benchmark and Baseline Approach</title>
      <link>https://vision4robotics.github.io/publication/2021_arxiv_pvt/</link>
      <pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://vision4robotics.github.io/publication/2021_arxiv_pvt/</guid>
      <description>&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;featured.png&#34; alt=&#34;ADTrack_workflow&#34; /&gt;
&lt;small&gt;&lt;/small&gt;
&lt;/center&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ADTrack: Target-Aware Dual Filter Learning for Real-Time Anti-Dark UAV Tracking</title>
      <link>https://vision4robotics.github.io/publication/2021_icra_adtrack/</link>
      <pubDate>Sun, 28 Feb 2021 08:00:00 +0000</pubDate>
      
      <guid>https://vision4robotics.github.io/publication/2021_icra_adtrack/</guid>
      <description>&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;featured.png&#34; alt=&#34;MKCT_workflow&#34; /&gt;
&lt;small&gt;Overall framework of the proposed ADTrack. ADTrack includes 3 stages: pretreatment, training, and detection, which are marked out by boxes in different colors. Dual filters, i.e., context filter and target-focused filter, training and detection follow routes in different colors. It can be seen that the final response shaded noises in context response, which indicates the validity of proposed dual filter.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;/center&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mutation Sensitive Correlation Filter for Real-Time UAV Tracking with Adaptive Hybrid Label</title>
      <link>https://vision4robotics.github.io/publication/2021_icra_mscf_tracker/</link>
      <pubDate>Sun, 28 Feb 2021 08:00:00 +0000</pubDate>
      
      <guid>https://vision4robotics.github.io/publication/2021_icra_mscf_tracker/</guid>
      <description>&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;featured.png&#34; alt=&#34;MKCT_workflow&#34; /&gt;
&lt;small&gt;Tracking procedure of the proposed MSCF tracker. Dashed boxes denote the variables to be solved in the main regression. As MTF in the red
box is generated from search region in frame k, it is applied to adjust the altitude value of the cruciform pedestal.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;/center&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Siamese Anchor Proposal Network for High-Speed Aerial Tracking</title>
      <link>https://vision4robotics.github.io/publication/2021_icra_siamapn/</link>
      <pubDate>Sun, 28 Feb 2021 08:00:00 +0000</pubDate>
      
      <guid>https://vision4robotics.github.io/publication/2021_icra_siamapn/</guid>
      <description>&lt;p&gt;&lt;center&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;featured.png&#34; alt=&#34;MKCT_workflow&#34; /&gt;
&lt;small&gt;The overview of SiamAPN tracker. It composes of four subnetworks, i.e., feature extraction network, feature fusion network, anchor proposal network (APN), and muti-classificationÂ®ression network.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;/center&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>All-Day Object Tracking for Unmanned Aerial Vehicle</title>
      <link>https://vision4robotics.github.io/publication/2021_arxiv_adtrack/</link>
      <pubDate>Sun, 24 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://vision4robotics.github.io/publication/2021_arxiv_adtrack/</guid>
      <description>&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;featured.jpg&#34; alt=&#34;ADTrack_workflow&#34; /&gt;
&lt;small&gt;Pipeline of ADTrack.&lt;/small&gt;
&lt;/center&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Disruptor-Aware Interval-Based Response Inconsistency for Correlation Filters in Real-Time Aerial Tracking</title>
      <link>https://vision4robotics.github.io/publication/2020_tgrs_ibri-tracker/</link>
      <pubDate>Wed, 07 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://vision4robotics.github.io/publication/2020_tgrs_ibri-tracker/</guid>
      <description>&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;featured.jpg&#34; alt=&#34;IBRI_workflow&#34; /&gt;
&lt;small&gt;Tracking procedure of the proposed IBRI tracker in the k-th frame. Historical interval responses are incorporated into the filter training phase after denoising by a novel disruptor-aware scheme based on response bucketing.&lt;/small&gt;
&lt;/center&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://vision4robotics.github.io/authors/junjie-ye/</link>
      <pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://vision4robotics.github.io/authors/junjie-ye/</guid>
      <description>&lt;p&gt;Junjie Ye received his B.Eng. degree in mechanical engineering from Tongji University, Shanghai, China. He is currently pursuing M.Sc. degree in mechanical engineering at Tongji University, Shanghai, China. His research interests include visual object tracking, deep learning, and robotics.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
